[{"content":" 场景与需求：\nPOD中的容器在启动后才能接管业务流量，如果启动失败要能够重启；\n如果容器在运行过程中，出现OOM错误或网络不通等，要能够使得该容器不再提供服务，且尝试自愈（重启）。\n针对上面的需求，Kubernetes提供三种探针来解决。\nstartupProbe探针: 探测容器是否已经启动。用在有些POD启动需要很长时间的场景，避免每隔一段时间使用其他探针探测一次。 如果提供了启动探针，则所有其他探针都会被禁用，直到此探针成功为止，即容器启动时优先使用startupProbe探测容器是否已经启动。 如果启动探测失败，kubelet 将kill POD中的容器，而容器依其重启策略（RestartPolicy）进行重启。 如果探测容器启动成功，将会启动容器中的其他两个探针（如果启动了的话），即readinessProbe和livenessProbe会接管进行下一步的探测。 如若没有提供startupProbe探针，默认容器启动成功。 readinessProbe探针：探测容器是否准备好对外提供服务。 容器启动后，如果探测容器readiness失败，kubelet将从与 该Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。也就是 svc 负载均衡会将这个 Pod 删掉，不提供服务。 readiness探针中的initialDelaySeconds之前的时间里，探针状态是failure，过了这个时间点，如果探测成功，则状态为success。 如果没有提供readiness探针，则默认容器一直是就绪的，是可以对外提供服务的。 livenessProbe探针：探测容器提供的服务是否活着。 容器启动后，如果探测容器提供的服务readiness失败， kubelet 会杀死容器， 并且容器将根据其重启策略（RestartPolicy）决定是否重启。 如果没有提供liveness探针，则默认容器提供的服务是活着的，不会主动重启。 使用这三种探针来确认容器中业务高可用的实践，容器如果挂了不再提供服务，并一定程度自愈（三板斧之一 ：重启）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 spec: containers: #...省略 startupProbe: failureThreshold: 30 #探测失败重试阈值为30次 httpGet: path: /health port: 9080 scheme: HTTP periodSeconds: 5 #每5秒探测一次 successThreshold: 1 # 探测成功阈值，1次成功就让liveness探针接管 readinessProbe: failureThreshold: 3 #探测失败重试阈值为3次 httpGet: path: /health port: 9080 scheme: HTTP periodSeconds: 5 #每5秒探测一次 livenessProbe: failureThreshold: 3 #探测失败重试阈值为3次 httpGet: path: /health port: 9080 scheme: HTTP periodSeconds: 10 #每10秒探测一次 successThreshold: 1 # 探测成功阈值 #...省略 这里首先设置启动探针的阈值为30次，每5秒探测一次，应用程序将有最大(150秒)完成启动，一旦启动成功，readiness和liveness探针即会接管后续的探测。在后续的探测中，readiness是每5s探测一次，如果探测到3次失败则认定已经失败，service Endpoints将会摘除这个POD IP使其不再对外提供服务；同时，liveness探针也会每10s中探测一次，如果探测到3次失败则认定已经失败，根据容器的重启策略，决定是否重启容器，如果不指定POD的重启策略（RestartPolicy），则默认为Always策略。也即意味着，如果这个容器出现故障，Kubelet会自动将这个容器重启。\n全文完。\n","description":"从业务层面来讲，应用需要高可用，确保容器启动失败时能自愈，业务异常能自愈。","id":0,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"16-使用探针保障容器高可用","uri":"https://laomeinote.com/posts/use-probes-within-the-containers/"},{"content":" 场景：应用基于Kubernetes部署，应用开发者受限登陆集群节点下载文件。当Java（或者其他）应用出现OOM异常需要生成heap dump二进制文件。\nKubernetes部署负载时，如果不指定POD的重启策略（RestartPolicy），则默认为Always策略。也即意味着，如果这个容器主进程出现故障，Kubelet会自动将这个容器重启。\n遗憾的是，容器故障时，由于Kubelet会很快自动重启容器，所以我们没法人工进入做java的heap dump这个动作。那是不是就没法获取到应用的heap dump信息呢？当然不是，需要注意的是，容器重启但是POD并没有重启，也就是说POD被调度的节点没有变化，也就意味着，我们可以将容器生成的文件直接存放到节点中，集群管理员可以协助去下载heap dump文件。\n1. 集群管理员获取heap dump文件 按照上面的思路，对于有整个集群权限的管理员而且，可以获取到节点中的任何数据，在容器退出前导出heap dump即可：\nDeployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: your-app spec: replicas: 1 template: metadata: labels: test: heapdump spec: containers: - name: a-jvm-container image: openjdk:11.0.1-jdk-slim-sid lifecycle: preStop: exec: command: [\u0026#34;java\u0026#34;, \u0026#34;-XX:+HeapDumpOnOutOfMemoryError\u0026#34;, \u0026#34;-XX:HeapDumpPath=/dumps/oom.bin\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;yourapp.jar\u0026#34;] volumeMounts: - name: heap-dumps mountPath: /dumps volumes: - name: heap-dumps emptyDir: {} 这里，设置lifecycle的preStop阶段执行heap dump动作，并将其写入容器的/dumps目录，主机上有一个目录会被kubelet自动挂载到容器的dumps目录下面，这样的话，即使容器重启了，POD没有重启，该目录依然存在，集群管理员依然可以到主机上下载该文件。\n但这显然不是一个好的运维方式，如果应用开发者能够自助获取到文件就好了。其实，应用可以启一个Sidecar容器来做采集就好了，没有权限登陆节点，那我们把应用采集到OBS对象存储里就好了呀。\n2. 应用开发者获取heap dump 顺着上面的思路，可以准备如下：\n准备一个镜像，里面包含脚本能够上传文件到OBS存储。 业务部署的Deployment里加上一个Sidecar，使用上面的镜像，采集应用的文件到OBS存储。 这里，我们使用的是华为云OBS存储，基于python sdk写一个s3存储客户端，编写Dockerfile制作镜像。\n2.1 步骤1：制作镜像 dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 设置Base镜像为alpine镜像,或者其他镜像 ARG baseImage=\u0026#34;kweecr03.xxx.xxx.com:80/public/alpine:x86-3.11.5\u0026#34; FROM $baseImage #安装inotify-tools和python3 RUN set -eux \u0026amp;\u0026amp; sed -i \u0026#34;s/dl-cdn.alpinelinux.org/mirrors.tools.xxx.com/g\u0026#34; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk update \\ \u0026amp;\u0026amp; apk add --no-cache bash \\ \u0026amp;\u0026amp; apk add --update --no-cache python3 \\ \u0026amp;\u0026amp; ln -sf python3 /usr/bin/python \\ \u0026amp;\u0026amp; apk add --no-cache inotify-tools #加入OBS客户端到/usr/local ADD s3_client.py /usr/local #设置后台主进程为bash ENTRYPOINT [\u0026#34;/bin/bash\u0026#34;] 在这个镜像中装了inotify和python，其中python用于跑OBS客户端脚本，inotify用于检测jump dump文件变化。将OBS客户端到easy_s3_2.1.py复制到与dockerfile所在目录，build镜像：\n1 docker build -f dockerfile -t myalpine:v1.1 . 2.2 步骤2：应用定义sidecar容器 Deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: your-app spec: replicas: 1 template: metadata: labels: test: heapdump spec: containers: - name: a-jvm-container image: openjdk:11.0.1-jdk-slim-sid command: [\u0026#34;java\u0026#34;, \u0026#34;-XX:+HeapDumpOnOutOfMemoryError\u0026#34;, \u0026#34;-XX:HeapDumpPath=/dumps/oom.bin\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;yourapp.jar\u0026#34;] volumeMounts: - name: heap-dumps mountPath: /dumps - name: sidecar-container image: myalpine:v1.1 #使用刚才制作的镜像 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - | inotifywait -m /dumps -e close_write | while read path action file; python s3_client.py -s s3-hc-dgg.xxx.xxx.com put /lmbucket/oom.bin -f /dumps/oom.bin ; done; volumeMounts: - name: heap-dumps mountPath: /dumps volumes: - name: heap-dumps emptyDir: {} 在上面的这段Deployment的定义中，比管理员获取heap dump文件 方案多了一个sidecar容器，其他内容完全一致，如下：\n1 2 3 4 5 6 - name: sidecar-container image: myalpine:v1.1 #使用刚才制作的镜像 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - | inotifywait -m /dumps -e close_write | while read path action file; python s3_client.py -s s3-hc-dgg.xxx.xxx.com put /lmbucket/oom.bin -f /dumps/oom.bin ; done; 其中inotifywait这个sidecar容器中的应用，主要是检测/dumps目录，检测内容包括read / path / action 和file, 检测方法是close_write ，即当dumps/目录下的文件关闭了不再写入，就将文件通过s3客户端上传到sc桶s3-hc-dgg.xxx.xxx.com中的/lmbucket目录下并命名为oom.bin。\n这样，应用开发者即可自助到s3中下载heap dump文件了。\n当然，这种方法适用于下载任何自己应用生成的文件。\n另外，这里使用s3_client.py的时候是将obs桶访问的AK/SK写死在程序里了，建议使用secret对象写，能在一定程度上加密。\n参考：\nhttps://danlebrero.com/2018/11/20/how-to-do-java-jvm-heapdump-in-kubernetes/\nhttps://github.com/aws-samples/kubernetes-for-java-developers/issues/12\n全文完。\n","description":"Kubernetes集群管理员在提供集群给开发者使用，出于安全考虑一般节点不会开放给开发者直接使用，导致开发者的heap dump数据无法直接获取到，本文描述这种场景的解决方案。","id":1,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"15-在K8s中导出Heap dump信息","uri":"https://laomeinote.com/posts/how-to-export-heap-dump-outside-of-k8s/"},{"content":" 需求背景：一些自研插件通过sidecar容器提供服务，容器需要获取到当前所在节点的IP与hostname作为环境变量传递给应用。\nKubernetes通过环境变量可满足该需求。通过环境变量获取的内容如下：\nstatus.podIP - POD的IP spec.serviceAccountName - Pod 服务帐号名称, 版本要求 v1.4.0-alpha.3 spec.nodeName - 节点名称, 版本要求 v1.4.0-alpha.3 status.hostIP - 节点 IP, 版本要求 v1.7.0-alpha.1 此外，Kubernetes通过downward API卷可以给容器提供POD信息、以及容器限额信息，详细可参考官网- Downward API 的能力\n实践 - 向容器中注入主机IP与主机Hostname deploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind: Deployment apiVersion: apps/v1 metadata: name: nginx-deploy spec: replicas: 1 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - env: - name: my_env value: \u0026#39;beta\u0026#39; - name: MY_NODE_IP valueFrom: fieldRef: #通过fieldRef引入到env中 fieldPath: status.hostIP #注入hostIP信息 name: container-1 image: \u0026#39;kweecr03.xxx.xxx.xxx:80/public/nginx:x86-1.20.1\u0026#39; resources: limits: cpu: 500m memory: 200Mi requests: cpu: 500m memory: 200Mi 测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ kubectl apply -f deploy.yaml deployment.apps/nginx-deploy created $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-deploy 1/1 1 1 6s laomei@myworkspace:~/kubernetes/exec/exec15$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-deploy-dfdf4f84d-v9w8r 1/1 Running 0 10s $ kubectl exec -it nginx-deploy-dfdf4f84d-v9w8r env |grep my_env #查看容器的环境变量my_env kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. my_env=beta $ kubectl exec -it nginx-deploy-dfdf4f84d-v9w8r env |grep MY_NODE_IP #查看容器的环境变量MY_NODE_IP kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. MY_NODE_IP=10.247.154.19 $ kubectl exec -it nginx-deploy-dfdf4f84d-v9w8r env |grep MY_NODE_NAME #查看容器的环境变量MY_NODE_NAME kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. MY_NODE_NAME=10.247.154.19 从这里可以看出其实其节点名和节点IP是一致的，之所以这样是因为K8s安装时节点命名直接引用了IP地址，如下：\n1 2 3 4 $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.247.154.19 Ready \u0026lt;none\u0026gt; 23d v1.19.10-r0-CCE21.11.1.B006-21.11.1.B006 10.247.154.61 Ready \u0026lt;none\u0026gt; 23d v1.19.10-r0-CCE21.11.1.B006-21.11.1.B006 但是节点本身在/etc/hostname中配置的名字是对的，那如何将这个配置文件传递给容器呢？\n实践2 - 通过hostpath卷引入容器中 接上面的实践1，我们可以通过hostpath将主机的/etc/hostname作为volume挂载到容器中的某个目录，容器如果需要读取，在应用中读取该volume即可。\ndeploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 kind: Deployment apiVersion: apps/v1 metadata: name: nginx-deploy spec: replicas: 1 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - env: - name: my_env value: \u0026#39;beta\u0026#39; - name: MY_NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName name: container-1 image: \u0026#39;kweecr03.xxx.xxx.xxx:80/public/nginx:x86-1.20.1\u0026#39; resources: limits: # limits与requests建议取值保持一致，避免扩缩容过程中出现震荡 cpu: 500m memory: 200Mi requests: cpu: 500m memory: 200Mi volumeMounts: - mountPath: /etc/hostname_eks name: hostname #引入下面的名字 volumes: - hostPath: path: /etc/hostname #主机目录 name: hostname #命名为hostame 部署后查询：\n1 2 3 $ kubectl exec -it nginx-deploy-6677765846-bbrh5 cat /etc/hostname_eks kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. kweekshcgp-nsdk4 其本质是引入hostpath文件，但最终是要被容器中的应用调用的。\n","description":"一些自研插件通过sidecar容器提供服务，容器需要获取到当前所在节点的IP与hostname作为环境变量或者downward API传递给应用。","id":2,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"14-向容器中注入主机信息","uri":"https://laomeinote.com/posts/how-to-inject-host-info-to-container/"},{"content":"通常容器的生命周期是不固定的、甚至是比较短的，但是它生成的数据可能是有价值的需要长期保存的，即持久化存储。在Kubernetes中持久化存储主要通过PV与PVC来实现。\nPV：PersistentVolume（持久化卷）。 PVC：PersistentVolumeClaim（持久化卷声明）。 PVC与PV的关系类似于POD与节点的关系，PVC消耗的是PV资源，POD消耗的是节点资源。一般来讲，是资源管理员或者SRE运维团队提前创建好PV存储资源，然后开发团队通过PVC来声明使用PV资源（是有点麻烦了）。\n除了上面的两个基本资源对象以外，Kubernetes还引入了一个StorageClass对象，这样就可以对存储进行分类，比如是快存储还是慢存储，是块存储、文件存储还是对象存储，是AWS的还是其他厂家的存储；同时通过StorageClass对象，Kubernetes就可以自动创建PV了，不需要管理员提前创建好PV再给开发成员通过PVC声明使用。\n也即是说持久化存储最终是通过PV来实现的，但是细化到具体存储资源，其实是多样化的，可以是Kubernetes主机本身的节点上挂载的磁盘，也可以是外部各类存储（块存储、文件存储、对象存储）。\n来看看几种场景：\n场景1 \u0026ndash; 容器生成可读写的数据，但无需持久化存储场景：Kubernetes提供的方案是emptyDir，POD或Deployment的声明中不需要指定具体节点路径，在POD创建的那一刻起，kubernetes自动在POD调度的节点上创建一个目录给POD使用，当POD注销的时候该目录也会被kubernetes清除掉，这显然没有达到持久化存储的目的，因为emptyDir的生命周期与POD一样长，POD很可能多次重启，那么相应的emptyDir里的数据也会被多次清除。\n注：emptyDir的生命周期与POD一致，但是POD里的容器与emptyDir不一致，因为POD里可能有多个容器。\n场景2 \u0026ndash; 容器生成可读写的数据，需要持久化保存，但是需要最佳的读写速度：虽然外挂存储也可满足要求，但是性能肯定比不上节点本身的磁盘，这种场景下，Kubernetes提供的是hostpath资源对象，POD或Deployment的声明中指明节点的具体路径为POD存放数据的地方，它的生命周期跟POD本身无关，即使POD挂了重启了hostpath数据也不会丢的。\n显然，hostpath其实也有局限，POD重启后可能会被调度到其他节点上去，之前存在当前节点上的数据，无法被重启后的POD读取到。\n场景3 \u0026ndash; 容器生成可读写的数据，需要持久化保存与最佳的读写速度，同时POD重启后依然能正常读写： 针对上面的场景2，Kubernetes提供的解决方案是local PV，其实原理也很简单，就是hostpath + nodeaffinity，就是告诉POD在重启后亲和到跟local PV的节点上去。\n场景4 \u0026ndash; 无需资源管理员提前创建PV，应用方（开发成员)可直接使用持久化存储的场景： 这种场景下使用StorageClass即可。\n下面针对这几种场景做下测试：\n场景1 - emptyDir 创建一个pod，包含一个nginx容器与一个busybox容器。在nginx中的emptyDir中写入一个文件，可被busybox容器查看，在主机上也可到目录下查看，随着POD的注销，目录及其数据也会被删除。\nemptydir-pod.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: v1 kind: Pod metadata: name: pod-emptydir namespace: default spec: containers: - name: myapp-pod image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /cache name: cache-volume - name: busybox-pod image: kweecr03.xxx.xxx.com:80/repo-mxh/busybox:v1.0 imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 3600\u0026#34;] volumeMounts: - mountPath: /test/cache name: cache-volume imagePullSecrets: - name: myregcred volumes: - name: cache-volume emptyDir: {} 说明：之所以加了secret对象，是因为上面的镜像在我的私仓里，需要secret访问才行。\n这个pod中创建了一个名为cache-volume的emptyDir，这个volume被挂载到两个容器中。apply生成pod：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 创建pod $ kubectl apply -f emptydir.yaml pod/pod-emptydir created $ kubectl get po NAME READY STATUS RESTARTS AGE pod-emptydir 2/2 Running 0 9s #进入到nginx的pod中在挂载的emptyDir目录下创建123.txt文件 $ kubectl exec -it pod-emptydir -c myapp-pod -- sh # ls bin boot cache dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var # cd cache # touch 123.txt # ls 123.txt # exit #进入到busybox的pod中，可查看到nginx容器创建的文件 $ kubectl exec -it pod-emptydir -c busybox-pod -- sh / # ls bin dev etc home proc root sys test tmp usr var / # cd /test/cache/ /test/cache # ls 123.txt /test/cache # exit #获取容器ID $ kubectl describe pod pod-emptydir | grep \u0026#34;Container ID\u0026#34; Container ID: docker://208dab7448336f9da697b173d2b08cb99e4a98cef76e913f91d9c69ce962903b Container ID: docker://986a9124b282b7e31be681c6695e2441b469b226c87441a97c68d5da5019658b 通过获取到的容器ID，进入节点上查找Kubernetes创建的目录。\n1 2 3 4 5 6 7 8 9 10 $ docker inspect 208dab7448336f9da697b173d2b08cb99e4a98cef76e913f91d9c69ce962903b | grep volume \u0026#34;/mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~empty-dir/cache-volume:/cache\u0026#34;, \u0026#34;/mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~secret/default-token-47g7m:/var/run/secrets/kubernetes.io/serviceaccount:ro\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~empty-dir/cache-volume\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~secret/default-token-47g7m\u0026#34;, #进入节点上Kubernetes创建给pod的emptyDir的目录中 $ cd /mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~empty-dir/cache-volume/ $ ls 123.txt 将pod删除，再次查看文件。\n1 2 $ kubectl delete po pod-emptydir pod \u0026#34;pod-emptydir\u0026#34; deleted 查看节点的目录与文件信息：\n1 2 $ ll /mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~empty-dir/cache-volume/ ls: cannot access \u0026#39;/mnt/paas/kubernetes/kubelet/pods/1759becf-5396-4e4c-828f-d256e43475e5/volumes/kubernetes.io~empty-dir/cache-volume/\u0026#39; 可见，随着POD的消亡，Kubernetes通过emptyDir在主机上创建的数据也被删除掉了。\n场景2 - hostpath 对于hostpath，其本质是PV与PVC，PV有几个属性需要关注：\nCapacity（存储能力）：一般来说，一个 PV 对象都要指定一个存储能力，通过 PV 的 capacity 属性来设置的，目前只支持存储空间的设置，就是我们这里的 storage=10Gi，不过未来可能会加入 IOPS、吞吐量等指标的配置。\nAccessModes（访问模式）：用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\nReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 RECLAIM POLICY（回收策略）：是指PV删除后的数据是清除，还是保留等，主要有以下三种：\nRetain（保留）：保留数据，需要管理员手工清理数据 Recycle（回收）：清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）：与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务 STATUS（状态）：指的是PV的生命周期，可能会处于4种不同的阶段：\nAvailable（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PVC 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 OK，有了上面的概念后，来创建一个PV、PVC与POD。\npod-pvc-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion: v1 kind: PersistentVolume metadata: name: pv-hostpath labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/data/k8s/test/hostpath\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-hostpath spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 3Gi --- apiVersion: v1 kind: Pod metadata: name: pv-hostpath-pod spec: volumes: - name: pv-hostpath persistentVolumeClaim: claimName: pvc-hostpath containers: - name: task-pv-container image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 ports: - containerPort: 80 volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: pv-hostpath 在上面的定义中，PV是首先声明的，然后再声明PVC，可以看到PVC的定义里并没有指定需要哪个名字的PV。其实在Kubernetes创建PVC后，会自动查看满足声明的PV，比如 storageClassName、accessModes 以及容量这些是否满足要求，如果满足要求就会将 PV 和 PVC 绑定在一起。\n在POD的定义中，使用具体PVC，名字就是前面定义的PVC的名字。然后apply生成资源测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 $ kubectl apply -f pod-pvc-pv.yaml persistentvolume/pv-hostpath created persistentvolumeclaim/pvc-hostpath created pod/pv-hostpath-pod created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-hostpath 10Gi RWO Retain Bound default/pvc-hostpath manual 33s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-hostpath Bound pv-hostpath 10Gi RWO manual 36s $ kubectl get pod NAME READY STATUS RESTARTS AGE pv-hostpath-pod 1/1 Running 0 38s $ kubectl describe po pv-hostpath-po ... ##省略 Containers: task-pv-container: Container ID: docker://eead5bc53e345e7ec9e0c8a99d5441ddcf597edec4a6d434bdc70304ce0fe186 Image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 Image ID: docker-pullable://kweecr03.xxx.xxx.com:80/public/nginx@sha256:56cbb3c9ada0858d69d19415039ba2aa1e9b357ba9aa9c88c73c30307aae17b0 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 13 Apr 2022 20:15:08 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /usr/share/nginx/html from pv-hostpath (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-47g7m (ro) ... ##省略 Volumes: pv-hostpath: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: pvc-hostpath ReadOnly: false ... ##省略 $ kubectl exec -it pv-hostpath-pod -- sh # ls -l /usr/share/nginx/html total 4 -rw------- 1 root root 10 Apr 2 10:53 test.txt 可见PV是10Gi大小，虽然声明了3Gi容量，但是比它大的PV也可满足要求，PVC与PV完成了Bound绑定。节点的volume被mount的目录为/usr/share/nginx/html。这个目录下有一个test.txt文件，是提前在节点创建目录写下的。查看节点文件：\n1 2 3 # ll /data/k8s/test/hostpath total 4 -rw------- 1 root root 10 Apr 2 10:53 test.txt 节点上目录与文件内容与容器挂载内容是一致的，符合预期。另外有一点需要额外注意：hostpath需要提前在节点上创建好对应的目录，否则会创建失败。\n场景3 - local PV 上面提到，local PV其实类似于 Hostpath + nodeAffinity，使得POD即使重启了也会调度到与hostpath一样的节点上。但是还需要注意的是PVC要等到POD创建时才去关联PV，而不能在PVC创建时立刻关联PV，避免POD因为关联到已经绑定到节点上去了导致POD创建失败的问题，这里就需要用到StorageClass的延迟绑定特性。\npod-localPV.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 apiVersion: v1 kind: PersistentVolume metadata: name: pv-local spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: #local字段来指定为local-pv path: /data/k8s/localpv # 集群node1节点上的目录，需要提前创建好该目录 nodeAffinity: # 加入亲和性设置 required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 10.247.154.19 # 集群node1节点的名字 --- # 如果pod被调度到其他非节点1的节点上去了，POD引用的PVC却在节点1上，因为PVC已经与节点1的PV绑定了，所以这会导致POD因为PVC的原因调度失败 # 所以可引用Kubernetes的StorageClass指定延迟绑定WaitForFirstConsumer，等待POD被调度后才对PVC绑定PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc-local spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage --- apiVersion: v1 kind: Pod metadata: name: pv-local-pod spec: volumes: - name: example-pv-local persistentVolumeClaim: claimName: pvc-local containers: - name: example-pv-local image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 ports: - containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html name: example-pv-local 这段定义中分别定义了4个k8s资源对象，pv，storageclass，pvc和pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ kubectl apply -f pod-localpv.yaml persistentvolume/pv-local created storageclass.storage.k8s.io/local-storage created persistentvolumeclaim/pvc-local created pod/pv-local-pod created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-hostpath 10Gi RWO Retain Bound default/pvc-hostpath manual 12h pv-local 5Gi RWO Delete Bound default/pvc-local local-storage 23s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-hostpath Bound pv-hostpath 10Gi RWO manual 12h pvc-local Bound pv-local 5Gi RWO local-storage 27s $ kubectl get pod NAME READY STATUS RESTARTS AGE pv-hostpath-pod 1/1 Running 0 12h pv-local-pod 1/1 Running 0 43s 从上面的打印中可以看出local-pv跟hostpath的类型还是有区别的，在pod的定义中引入pvc即可使用底层存储，其中引入storageclass指定volumeBindingMode: WaitForFirstConsumer，等待POD被调度后才对PVC绑定PV动作。\n将POD删除之后，重启拉取POD，由于设置了NodeAffinity亲和性，下次还是会被部署到同一个节点上来，这样就能使用这个节点上的PV存储资源了。\n需要注意的是，上面手动创建 PV 的方式，即静态的 PV 管理方式，在删除 PV 时需要按如下流程执行操作：\n1）删除使用这个 PV 的 Pod 2）从宿主机移除本地磁盘 3）删除 PVC 4）删除 PV 如果不按照这个流程的话，这个 PV 的删除就会失败。\n场景4 - storageclass 上面手工创建PV是静态PV管理，需要管理员提前创建好PV，比较复杂，而采用storageclass可以实现动态管理PV。storageclass本身是个存储类型，上面的几个场景中已经有用到。\n基于StorageClass自动创建PV，需要管理员部署PV配置器（provisioner），然后定义对应的StorageClass，这样开发者在创建PVC的时候只需要选择要创建存储的类型，PVC会把StorageClass传递给PV provisioner，由provisioner自动创建PV。\n如华为云CCE就提供csi-disk、csi-nas、csi-obs等StorageClass，在声明PVC时加上StorageClassName，就可以自动创建PV，并自动创建底层的存储资源。查询管理员已经创建好的storageclass：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE csi-disk everest-csi-provisioner Delete Immediate true 129d csi-disk-topology everest-csi-provisioner Delete WaitForFirstConsumer true 129d csi-nas everest-csi-provisioner Delete Immediate true 129d csi-obs everest-csi-provisioner Delete Immediate false 129d csi-sfsturbo everest-csi-provisioner Delete Immediate true 129d efs-performance flexvolume-xxx.com/fuxiefs Delete Immediate true 129d efs-standard flexvolume-xxx.com/fuxiefs Delete Immediate true 129d local-storage kubernetes.io/no-provisioner Delete WaitForFirstConsumer false 6h53m nfs-rw flexvolume-xxx.com/fuxinfs Delete Immediate true 129d obs-standard flexvolume-xxx.com/fuxiobs Delete Immediate false 129d obs-standard-ia flexvolume-xxx.com/fuxiobs Delete Immediate false 129d sas flexvolume-xxx.com/fuxivol Delete Immediate true 129d sata flexvolume-xxx.com/fuxivol Delete Immediate true 129d ssd flexvolume-xxx.com/fuxivol Delete Immediate true 129d 其中csi是引用的华为云CCE的存储插件，可以基于这些storageclass直接创建PV。\nvolume.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-sfs-auto-example spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: csi-nas # StorageClass类型引入上面csi中的nas类型存储 创建这个PVC，即可自动创建与之对应的PV，并且PVC与PV会进行自动绑定。\n1 2 3 4 5 6 7 8 9 10 11 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-hostpath Bound pv-hostpath 10Gi RWO manual 19h pvc-local Bound pv-local 5Gi RWO local-storage 6h57m pvc-sfs-auto-example Bound pvc-c4911ba4-e55a-41e7-a246-a93a32b95a9e 5Gi RWX csi-nas 12s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-hostpath 10Gi RWO Retain Bound default/pvc-hostpath manual 19h pv-local 5Gi RWO Delete Bound default/pvc-local local-storage 6h58m pvc-c4911ba4-e55a-41e7-a246-a93a32b95a9e 5Gi RWX Delete Bound default/pvc-sfs-auto-example csi-nas 15s deploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 name: container-0 volumeMounts: - mountPath: /tmp # 挂载路径 name: pvc-sfs-example restartPolicy: Always volumes: - name: pvc-sfs-example persistentVolumeClaim: claimName: pvc-sfs-auto-example # PVC的名称，使用上述的nas存储 这里定义了两个nginx pod副本，容器使用的volumes中的PVC即为上述创建的PVC，直接使用上述的nas存储。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $ kubectl apply -f deploy.yaml deployment.apps/nginx-deployment created $ kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-5ffdb86f59-7v46b 0/1 Pending 0 24s nginx-deployment-5ffdb86f59-tlt9m 0/1 Pending 0 24s $ kubectl describe po nginx-deployment-5ffdb86f59-7v46b ...省略 Containers: container-0: Container ID: docker://578300edb44cbdbabc7faa0fb770e3b4bf83b536deb5dc27e5b4879818dbe9eb Image: kweecr03.xxx.xxx.com:80/public/nginx:x86-1.20.1 Image ID: docker-pullable://kweecr03.xxx.xxx.com:80/public/nginx@sha256:56cbb3c9ada0858d69d19415039ba2aa1e9b357ba9aa9c88c73c30307aae17b0 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Running Started: Thu, 14 Apr 2022 16:06:30 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /tmp from pvc-sfs-example (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-47g7m (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: pvc-sfs-example: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: pvc-sfs-auto-example ReadOnly: false default-token-47g7m: Type: Secret (a volume populated by a Secret) SecretName: default-token-47g7m Optional: false ...省略 两个POD实例通过使用StorageClass使用自动创建了PV，且两个POD共享使用该nas存储。\n全文完。\n","description":"通常容器的生命周期是不固定的、甚至是比较短的，但是它生成的数据可能是有价值的需要长期保存的，即持久化存储。在Kubernetes中持久化存储主要通过PV与PVC来实现。","id":3,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"13-容器数据持久化存储","uri":"https://laomeinote.com/posts/persistent-storage-for-services-in-k8s/"},{"content":"1. 背景 大规模集群上每个节点都有大量镜像需要定期维护，如果通过Kubelet GC机制垃圾回收镜像的话，可能会导致原本还在用的镜像被误删除。所以需要能指定镜像删除，同时不必登陆到每个节点上手动删除镜像。\n2. 原理 核心思路是，在节点中的POD中获取特权，通过Docker on Docker方案，链接到主机docker socket上：/var/run/docker.sock，然后可通过Docker客户端操作主机，如：docker rmi命令强制删除主机镜像。如果使用docker rmi则要求POD使用的镜像中安装有docker，稍微麻烦。\n其实docker客户端多种多样，其本质都是操作HTTP Restful API，还可以使用python、go等docker客户端，不过最简单是使用curl命令行调用HTTP Restful API了。\n1 curl --unix-socket /var/run/docker.sock -X DELETE http://localhost/[docker Version]/images/[imageID] 详细描述参考：https://docs.docker.com/engine/api/sdk/\n3. 方案 1 -Daemonset 通过Daemonset来实现，在每个节点上都生成一个POD，在POD中调用上面的curl命令，删除主机镜像 缺点：需要手动清除Daemonset daemonset.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind: DaemonSet apiVersion: apps/v1 metadata: name: nginx-ds labels: k8s-app: nginx spec: selector: matchLabels: k8s-app: nginx template: metadata: labels: k8s-app: nginx spec: containers: - name: nginx env: - name: DELETE_IMAGE_NAME value: \u0026#34;kweecr03.xxx.xxx.xxx:80/public/euleros:x86-V200R007C00SPC521B060\u0026#34; image: kweecr03.xxx.xxx.xxx:80/public/nginx:x86-1.20.1 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;curl --unix-socket /var/run/docker.sock -X DELETE http://localhost/v1.39/images/$(DELETE_IMAGE_NAME)?force=true\u0026#39;] securityContext: privileged: true volumeMounts: - mountPath: /var/run/docker.sock name: docker-sock-volume volumes: - name: docker-sock-volume hostPath: path: /var/run/docker.sock 3.2 方案2 - Job 相比Daemonset，Job可以在完成后自动清除，不会遗留垃圾数据，但是需要手动设置反亲和性，指定节点之间的反亲和即可。\njob.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion: batch/v1 kind: Job metadata: name: myjob spec: completions: 6 # Job结束需要运行6个Pod， parallelism: 6 # 并行6个Pod backoffLimit: 2 # 最多重试2次 ttlSecondsAfterFinished: 100 #Job运行后100s会删除 template: spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: job-name operator: In values: - myjob topologyKey: kubernetes.io/hostname containers: - name: imagejob env: - name: DELETE_IMAGE_NAME value: \u0026#34;kweecr03.xxx.xxx.xxx:80/public/alpine:x86-3.11.5\u0026#34; image: kweecr03.xxx.xxx.xxx:80/public/nginx:x86-1.20.1 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;curl --unix-socket /var/run/docker.sock -X DELETE http://localhost/v1.39/images/$(DELETE_IMAGE_NAME)?force=true\u0026#39;] securityContext: privileged: true volumeMounts: - mountPath: /var/run/docker.sock name: docker-sock-volume volumes: - name: docker-sock-volume hostPath: path: /var/run/docker.sock restartPolicy: Never 关键点：\n设置反亲和性：节点之间硬亲和requiredDuringSchedulingIgnoredDuringExecution 设置Job自动清理时间ttlSecondsAfterFinished,比如设置值为100，表示成功执行job起，100s后Job被清理。 验证：\n给集群扩容到6节点，并给每个节点拉取镜像kweecr03.xxx.xxx.xxx:80/public/alpine:x86-3.11.5 1 docker pull kweecr03.xxx.xxx.xxx:80/public/alpine:x86-3.11.5 生成job任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ➜ kubectl apply -f job.yaml job.batch/myjob created ➜ kubectl get job NAME COMPLETIONS DURATION AGE myjob 6/6 5s 5s ➜ kubectl get po -o wide #发现POD是分布在每个节点上 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myjob-9hd6b 0/1 Completed 0 67s 12.11.0.117 10.247.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myjob-cqg2b 0/1 Completed 0 67s 12.11.0.109 10.247.154.144 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myjob-hcd7r 0/1 Completed 0 67s 12.11.0.139 10.247.154.180 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myjob-p9c8n 0/1 Completed 0 67s 12.11.0.93 10.247.154.122 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myjob-xpqmn 0/1 Completed 0 67s 12.11.0.63 10.247.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myjob-zb97w 0/1 Completed 0 67s 12.11.0.76 10.247.154.19 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ➜ kubectl logs myjob-9hd6b #查看POD日志，日志显示删除了镜像 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed [{\u0026#34;Untagged\u0026#34;:\u0026#34;kweecr03.xxx.xxx.xxx:80/public/alpine:x86-3.11.5\u0026#34;},{\u0026#34;Untagged\u0026#34;:\u0026#34;kweecr03.xxx.xxx.xxx:80/public/alpine@sha256:cb8a924afdf0229ef7515d9e5b3024e23b3eb03ddbba287f4a19c6ac90b8d221\u0026#34;},{\u0026#34;Deleted\u0026#34;:\u0026#34;sha256:a187dde48cd289ac374ad8539930628314bc581a481cdb41409c9289419ddb72\u0026#34;}] 100 283 100 283 0 0 35375 0 --:--:-- --:--:-- --:--:-- 35375 登陆到每个节点中查看镜像是否存在 1 ➜ docker images |grep alpine 发现每个节点确实已经没有该镜像。\n100s后观察POD已经被自动清除。 全文完。\n","description":"大规模集群上每个节点都有大量镜像需要定期维护，如果通过Kubelet GC机制垃圾回收镜像的话，可能会导致原本还在用的镜像被误删除。所以需要能指定镜像删除，同时不必登陆到每个节点上手动删除镜像。","id":4,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"12-快速删除集群所有节点上镜像","uri":"https://laomeinote.com/posts/delete-images-in-all-node-rapidly/"},{"content":"相对于HPA横向弹性伸缩POD数量，VPA垂直弹性会在现有POD基础上对POD的CPU与内存进行弹性，它根据容器资源使用率自动设置 CPU 和 内存 的requests，从而允许在节点上进行适当的调度，以便为每个 Pod 提供适当的资源。VPA既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。\n说明：VPA与HPA不能同时工作，二者只能选其一，且VPA目前还未大规模推广商用，仅供测试。\n依赖条件：\nmetrics-server：跟HPA一样，需要有metrics-server识别CPU、内存指标数据。 vertical-pod-autoscaler：是Kubernetes的CRD对象，可在https://github.com/kubernetes/autoscaler.git获取安装包到集群中安装使用。 metrics-server与vertical-pod-autoscaler也是通过POD的方式运行在kube-system命名空间下，查看如下结果则表示安装成功。\n1 2 3 4 5 $ kubectl get po -nkube-system |grep -E \u0026#34;metrics|vpa\u0026#34; metrics-server-5458746495-pfv4d 1/1 Running 5 63d vpa-admission-controller-657857bfb7-bjgrm 1/1 Running 0 94m vpa-recommender-77dccb87b8-4rg69 1/1 Running 0 94m vpa-updater-5f574b5d57-vblz9 1/1 Running 0 93m 下面部署一个负载，指定POD的CPU与内存大小，并通过压测触发VPA弹性策略，增加其CPU与内存，压测结束后CPU与内存缩小。\n1. 部署Nginx负载 nginx-deploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx spec: replicas: 2 selector: matchLabels: app: nginx_vpa template: metadata: labels: app: nginx_vpa spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx resources: requests: cpu: 100m memory: 250Mi 这里指定容器的request资源为CPU 100m，内存 250Mi。\n1 2 3 4 5 6 7 $ kubectl apply -f nginx_vpa.yaml deployment.apps/nginx created $ kubectl get po -l app=nginx_vpa NAME READY STATUS RESTARTS AGE nginx-79c77f8bc7-fn574 1/1 Running 0 18s nginx-79c77f8bc7-g8jzx 1/1 Running 0 18s apply资源文件后生成两个nginx的POD。\n2. 部署Nginx service服务 为了测试服务，给POD加上前端serice服务作为业务入口Endpoint。\nnginx-vpa-svc.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Service metadata: name: nginx spec: #type: NodePort ports: - port: 80 targetPort: 80 selector: app: nginx_vpa 这里我们可以设置service类型为ClusterIP或者NodePort，前者是在集群内部互通，后者可通过节点在外部访问，默认不指定服务类型的时候，使用的类型为ClusterIP，为测试方便可选择默认ClusterIP并在集群的节点上直接压测。\napply资源对象文件：\n1 2 $ kubectl apply -f nginx-svc.yaml service/nginx created 查看生成的服务：\n1 2 3 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.1.41.177 \u0026lt;none\u0026gt; 80/TCP 4s 可在集群节点上直接测试curl -I 10.1.41.177:80。\n1 2 3 4 5 6 7 8 9 10 11 [lm@node2 ~]$ curl -I 10.1.41.177:80 curl -I 10.1.41.177:80 HTTP/1.1 200 OK Server: nginx/1.21.5 Date: Thu, 31 Mar 2022 08:13:55 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 28 Dec 2021 15:28:38 GMT Connection: keep-alive ETag: \u0026#34;61cb2d26-267\u0026#34; Accept-Ranges: bytes 3. 部署VPA策略 nginx-vpa.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: autoscaling.k8s.io/v1beta2 kind: VerticalPodAutoscaler metadata: name: nginx-vpa spec: targetRef: apiVersion: \u0026#34;apps/v1\u0026#34; kind: Deployment name: nginx updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: \u0026#34;nginx\u0026#34; minAllowed: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;100Mi\u0026#34; maxAllowed: cpu: \u0026#34;2000m\u0026#34; memory: \u0026#34;2048Mi\u0026#34; 这里我们指定的策略为CPU最小弹性至250m，内存到100Mi，CPU最大2000m，内存最大2048Mi。另外还有一个重要参数：updateMode: \u0026quot;Auto\u0026quot;。这里指将触发VPA弹性动作。如果该值为OFF表示只推荐，但是不会真的触发弹性值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ kubectl apply -f nginx-vpa-demo.yaml verticalpodautoscaler.autoscaling.k8s.io/nginx-vpa created $ kubectl get vpa NAME MODE CPU MEM PROVIDED AGE nginx-vpa Off 250m 262144k True 8s $ kubectl describe vpa nginx-vpa |tail -n 20 Conditions: Last Transition Time: 2022-03-31T08:18:08Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: nginx Lower Bound: Cpu: 250m Memory: 262144k Target: Cpu: 250m Memory: 262144k Uncapped Target: Cpu: 25m Memory: 262144k Upper Bound: Cpu: 429m Memory: 448631544 Events: \u0026lt;none\u0026gt; 从策略里看有几个关键点：\nLower Bound：下限值\nTarget: 推荐值\nUpper Bound:上限值\nUncapped Target: 如果没有为VPA提供最小或最大边界，则表示目标利用率 。\n上述结果表明，推荐的 Pod 的 CPU 请求为 250m，推荐的内存请求为 262144k 字节。\n4. 对Nginx服务进行压测触发VPA弹性 由于我们的Endpoint为clusterIP类型的service，所需需在集群内部访问，可在节点上测试Nginx服务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@node2 ~]# ab -c 100 -n 10000000 http://10.1.41.177:80/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 10.1.41.177 (be patient) Completed 1000000 requests Completed 2000000 requests Completed 3000000 requests Completed 4000000 requests Completed 5000000 requests Completed 6000000 requests Completed 7000000 requests Completed 8000000 requests Completed 9000000 requests apr_pollset_poll: The timeout specified has expired (70007) Total of 9999808 requests completed 在压测过程中查看vpa详情，只关注Container Recommendations部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ kubectl describe vpa nginx-vpa |tail -n 20 Conditions: Last Transition Time: 2022-03-31T08:18:08Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: nginx Lower Bound: Cpu: 250m Memory: 262144k Target: Cpu: 250m Memory: 262144k Uncapped Target: Cpu: 25m Memory: 262144k Upper Bound: Cpu: 2 Memory: 545717993 Events: \u0026lt;none\u0026gt; 这里Target为Cpu: 250m ，Memory: 262144k，继续查看event事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ kubectl get event LAST SEEN TYPE REASON OBJECT MESSAGE 2m42s Warning FailedGetScale horizontalpodautoscaler/hpa-example deployments/scale.apps \u0026#34;hpa-example\u0026#34; not found 11m Normal EvictedByVPA pod/nginx-79c77f8bc7-fn574 Pod was evicted by VPA Updater to apply resource recommendation. 11m Normal Killing pod/nginx-79c77f8bc7-fn574 Stopping container nginx 12m Normal Scheduled pod/nginx-79c77f8bc7-g68rc Successfully assigned default/nginx-79c77f8bc7-g68rc to master 12m Normal Pulled pod/nginx-79c77f8bc7-g68rc Container image \u0026#34;nginx\u0026#34; already present on machine 12m Normal Created pod/nginx-79c77f8bc7-g68rc Created container nginx 12m Normal Started pod/nginx-79c77f8bc7-g68rc Started container nginx 12m Normal EvictedByVPA pod/nginx-79c77f8bc7-g8jzx Pod was evicted by VPA Updater to apply resource recommendation. 12m Normal Killing pod/nginx-79c77f8bc7-g8jzx Stopping container nginx 11m Normal Scheduled pod/nginx-79c77f8bc7-vtknc Successfully assigned default/nginx-79c77f8bc7-vtknc to node2 11m Normal Pulled pod/nginx-79c77f8bc7-vtknc Container image \u0026#34;nginx\u0026#34; already present on machine 11m Normal Created pod/nginx-79c77f8bc7-vtknc Created container nginx 11m Normal Started pod/nginx-79c77f8bc7-vtknc Started container nginx 12m Normal SuccessfulCreate replicaset/nginx-79c77f8bc7 Created pod: nginx-79c77f8bc7-g68rc 11m Normal SuccessfulCreate replicaset/nginx-79c77f8bc7 Created pod: nginx-79c77f8bc7-vtknc 从事件里可以看出，vpa执行了EvictedByVPA，自动停掉了老的POD，pod/nginx-79c77f8bc7-fn574与pod/nginx-79c77f8bc7-g8jzx，然后使用 VPA推荐的资源启动了新的nginx : pod/nginx-79c77f8bc7-g68rc与pod/nginx-79c77f8bc7-vtknc 。分别查看这两个POD的资源requests情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $ kubectl describe pod nginx-79c77f8bc7-g68rc Name: nginx-79c77f8bc7-g68rc Namespace: default Priority: 0 Node: master/192.168.0.114 Start Time: Thu, 31 Mar 2022 16:38:40 +0800 Labels: app=nginx_vpa pod-template-hash=79c77f8bc7 Annotations: vpaObservedContainers: nginx vpaUpdates: Pod resources updated by nginx-vpa: container 0: cpu request, memory request Status: Running IP: 10.244.0.126 IPs: IP: 10.244.0.126 Controlled By: ReplicaSet/nginx-79c77f8bc7 Containers: nginx: Container ID: docker://d45c8eeada45b8bc8e932cd28d7e72c6ac412c90568689f4090c97f207a99914 Image: nginx Image ID: docker-pullable://nginx@sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Running Started: Thu, 31 Mar 2022 16:38:41 +0800 Ready: True Restart Count: 0 Requests: cpu: 250m memory: 262144k Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-g4ktr (ro) 可见其申请了新的CPU与内存值。不过压测半小时过去了，也没见VPA将POD的requests缩回去。。。可能等的时间不够，或者VPA本身不成熟吧。\n5.总结 VPA垂直弹性可对POD进行CPU与内存requests修改，kill掉老POD，生成新POD。\nVPA与HPA不能同时使用，从上面的event事件中可看出VPA测试过程中确实影响了HPA弹性。\nVPA压测很久之后POD还没有垂直缩回去，功能应该还不成熟。\n全文完。\n","description":"相对于HPA横向弹性伸缩POD数量，VPA垂直弹性既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。","id":5,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"11-通过VPA实现资源灵活调度","uri":"https://laomeinote.com/posts/how-to-make-vpa-policy-in-kubernetes/"},{"content":"一般ConfigMap用于保存通用配置信息，属于明文信息，不能保存密钥，如果要保存密钥等加密信息，Kubernetes提供Secret对象进行Base64转码。\nSecret 主要使用的有以下三种类型：\nOpaque：base64 编码格式的 Secret，用来存储密码、密钥等；但数据也可以通过base64 –decode解码得到原始数据，所以加密性很弱。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token：用于被 ServiceAccount ServiceAccount 创建时 Kubernetes 会默认创建一个对应的 Secret 对象。Pod 如果使用了 ServiceAccount，对应的 Secret 会自动挂载到 Pod 目录 /run/secrets/kubernetes.io/serviceaccount 中。 如下在集群中查询secret信息：\n1 2 3 4 5 $ kubectl get secret NAME TYPE DATA AGE default-token-g4ktr kubernetes.io/service-account-token 3 108d mxhregcred kubernetes.io/dockerconfigjson 1 91d mysecret Opaque 2 11s 其中第一个default-token-g4ktr是创建service account时默认创建的secret，内容为加密的token信息，第二个mxhregcred是创建docker 私仓的密钥时生成的，也是加密的token信息，第三个是创建的Opaque类型的secret。下面以Opaque类型secret为例，演示向容器中注入secret方法。\n1. 创建Secret对象 将需要加密的对象采用base64进行加密，下面分别对admin和admin321使用base64算法加密。\n1 2 3 4 $ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;admin321\u0026#34; | base64 YWRtaW4zMjE= 创建Opaque类型的secret对象，编辑secret-demo.yaml，内容如下：\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: YWRtaW4zMjE= 创建Secret对象：\n1 2 $ kubectl apply -f secret-demo.yaml secret/mysecret created 查看生成的Secret对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 $ kubectl describe secret mysecret Name: mysecret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 8 bytes username: 5 bytes $ kubectl get secret mysecret -o yaml apiVersion: v1 data: password: YWRtaW4zMjE= username: YWRtaW4= kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;password\u0026#34;:\u0026#34;YWRtaW4zMjE=\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;YWRtaW4=\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;Secret\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;mysecret\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;Opaque\u0026#34;} creationTimestamp: \u0026#34;2022-03-31T02:42:27Z\u0026#34; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:annotations: .: {} f:kubectl.kubernetes.io/last-applied-configuration: {} f:type: {} manager: kubectl-client-side-apply operation: Update time: \u0026#34;2022-03-31T02:42:27Z\u0026#34; name: mysecret namespace: default resourceVersion: \u0026#34;38254944\u0026#34; selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 04bbf4f7-a4b9-4031-a137-408fd69e11cc type: Opaque 2. 使用Secret对象 创建好 Secret对象后，有两种方式来使用：\n以环境变量的形式 以Volume的形式挂载 2.1 以环境变量形式使用Secret对象 在containes下声明: env -\u0026gt; name valueFrom -\u0026gt; secretRef 层级来引用secret密钥，如果需要引用多个密钥则引用多个 name valueFrom 。\neg: secret-demo1.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Pod metadata: name: secret1-pod spec: containers: - name: secret1 image: busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: - name: USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: PASSWORD valueFrom: secretKeyRef: name: mysecret key: password 生成pod后查看其log日志：\n1 2 3 4 5 $ kubectl logs secret1-pod ...... USERNAME=admin PASSWORD=admin321 ...... 可见密钥已经注入到容器中，可正常被识别。\n2.2 以Volume的形式挂载使用Secret对象 与ConfigMap类似，挂载一个volumes，将其指定为secret，然后在container中引入使用volumeMounts进行挂载，这样的话相当于，在容器上挂了一个secret类型的磁盘，磁盘里内容即为secret内容。\neg：secret-demo2.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: secret2-pod spec: containers: - name: secret2 image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls /etc/secrets\u0026#34;] volumeMounts: - name: secrets mountPath: /etc/secrets volumes: - name: secrets secret: secretName: mysecret 生成pod后查看其log日志：\n1 2 3 $ kubectl logs secret2-pod password username 可见volume方式也成功注入到容器中。\n全文完。\n","description":"一般ConfigMap用于保存通用配置信息，属于明文信息，不能保存密钥，如果要保存密钥等加密信息，Kubernetes提供Secret对象进行Base64转码。","id":6,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"10-向容器中注入加密信息","uri":"https://laomeinote.com/posts/inject-secret-info-to-container/"},{"content":"应用的配置在Kubernetes中可通过ConfigMap资源对象来实现，避免被直接写死到应用程序中，比如应用连接一个redis服务，下一次想更换另一个，如果写死的话，就得重新修改代码，重新制作新镜像。而利用ConfigMap就可以很方便地向容器中注入配置信息。类似于Windows/Linux中的环境变量的配置。\n不过需要注意的是，ConfigMap一般是保存非安全的配置信息，比如服务连接地址，环境变量参数，服务端口等等，如果是涉及到密钥等敏感信息，则不适合用ConfigMap，因为ConfigMap是明文保存的，如果保存密钥等信息就会引起安全问题了。\n1. 实践1：通过ConfigMap向容器中注入数据库端口值 最直观的方式是在命令行中直接注入参数值，通过--from-literal参数传递配置信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ kubectl create configmap cm-demo1 --from-literal=db.host=localhost --from-literal=db.port=3306 #命令行直接创建configmap configmap/cm-demo1 created $ kubectl get cm/cm-demo1 -o yaml #查看configmap具体信息 apiVersion: v1 data: db.host: localhost db.port: \u0026#34;3306\u0026#34; kind: ConfigMap metadata: creationTimestamp: \u0026#34;2022-03-28T07:36:40Z\u0026#34; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:db.host: {} f:db.port: {} manager: kubectl-create operation: Update time: \u0026#34;2022-03-28T07:36:40Z\u0026#34; name: cm-demo1 namespace: default resourceVersion: \u0026#34;37278796\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/cm-demo1 uid: 53212514-4737-4bc1-b841-48deb80c9597 通过kubectl create configmap创建configmap配置信息，指定了key与value值，如果需要指定多个参数，则指定多个--from-litera.通过上面的命令，创建了一个名为cm-demo1的configmap，对应有两个变量。\ndb.host: localhost db.port: 3306 上面创建好的configmap如何被其他API对象引用呢？可通过env -\u0026gt; name valueFrom -\u0026gt; configMapKeyRef -\u0026gt; name key 引用。如：\ntestcm1_pod.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Pod metadata: name: testcm1-pod spec: containers: - name: testcm1 image: busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo DB_HOST: $DB_HOST, DB_PORT: $DB_PORT\u0026#34; ] env: - name: DB_HOST valueFrom: configMapKeyRef: name: cm-demo1 key: db.host - name: DB_PORT valueFrom: configMapKeyRef: name: cm-demo1 key: db.port 这里定义一个POD，使用busybox镜像，启动容器后只需命令echo DB_HOST: $DB_HOST, DB_PORT: $DB_PORT,显然，busybox镜像默认并没有DB_HOST与DB_PORT的变量，而是通过在POD下文中通过env定义的。\n在env中定义了两个变量：\nname为DB_HOST的变量：通过valueFrom-configMapKeyRef定位到configmap，分别设置name与key，则引用名为cm-demo1的configmap对象例的db.host变量。 name为DB_PORT的变量：通过valueFrom-configMapKeyRef定位到configmap，分别设置name与key，则引用名为cm-demo1的configmap对象例的db.port变量。 测试查看结果：\n1 2 3 4 5 $ kubectl apply -f pod_cm.yaml pod/testcm1-pod created $ kubectl logs testcm1-pod DB_HOST: localhost, DB_PORT: 3306 在创建pod对象后，testcm1-pod能够识别打印出提前定义的变量DB_HOST与DB_PORT。\n2. 实践2：通过在数据卷中使用configmap向容器中注入多个变量 上面通过命令行，写了两个变量，已经有点麻烦了，如果涉及到多个变量的话，写命令行就不合适了，最好需要将配置写入文件，应用来读取该文件。一般采取步骤如下：\n创建好配置文件，如Mysql的配置文件mysql.cnf。 1 2 3 4 5 6 7 8 [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid !includedir /etc/my.cnf.d 根据该配置文件，创建成configmap配置，以供Kubernetes对象使用。 1 2 $ kubectl create cm mysql-cm --from-file=mysql.cnf configmap/mysql-cm created 创建完成后，可查看创建好的cm。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ kubectl get cm/mysql-cm -o yaml apiVersion: v1 data: mysql.cnf: | [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid !includedir /etc/my.cnf.d kind: ConfigMap metadata: creationTimestamp: \u0026#34;2022-03-28T08:20:12Z\u0026#34; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:mysql.cnf: {} manager: kubectl-create operation: Update time: \u0026#34;2022-03-28T08:20:12Z\u0026#34; name: mysql-cm namespace: default resourceVersion: \u0026#34;37289515\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/mysql-cm uid: 8f220505-0a44-4363-8bfc-9067e86dd034 Kubernetes资源对象引用configmap 定义POD，pod_vol_cm.yaml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: testcm2-pod spec: volumes: - name: config-volume configMap: name: mysql-cm containers: - name: testcm2 image: busybox # command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/msyql.cnf\u0026#34; ] #为了更显性化测试，将command修改为sleep 5分钟，然后进去POD中查看注入文件 command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;sleep 3000\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config POD中定义了volumes，引用configmap，即：名为mysql-cm的configMap也是一个volumes数据卷，最终在container中被挂载到/etc/config目录下。在容器中执行cat命令，最终打印出的将是mysql.cnf文件内容。\n生成POD并查看注入的配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl apply -f pod_vol_cm.yaml #生成POD pod/testcm2-pod created $ kubectl exec -it testcm2-pod sh #进入POD执行命令 kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. / # cat /etc/config/mysql.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid !includedir /etc/my.cnf.d / # 在POD中可查看到configmap注入的配置信息。\n简单理解下：有一个数据卷名为mysql-cm，这个名字其实也就是configmap的名字，这个数据卷被挂载到容器里的某个路径/etc/config，这个路径下的文件也即是configmap的配置文件。\n3. 总结ConfigMap的使用 configmap可以用来存储Kubernetes资源对象所需的配置文件，其内容将写入到ETCD存储中。\n3.1 创建ConfigMap 通用创建ConfigMap的方式有以下几种：\n通过直接在命令行中指定configmap参数创建，即--from-literal； 通过指定文件创建，即将一个配置文件创建为一个ConfigMap，--from-file=\u0026lt;文件\u0026gt;； 事先写好标准的configmap的yaml文件，然后kubectl create -f 创建。 3.1.1 通过\u0026ndash;from-literal创建 1 $ kubectl create configmap test-config1 --from-literal=db.host=localhost --from-literal=db.port=\u0026#39;3306\u0026#39; 直接在命令行中写好所有的变量，通过configmap注入到容器中。\n3.1.2 通过\u0026ndash;from-file 1 2 3 $ echo -n 172.18.8.200 \u0026gt; ./db.host $ echo -n 3306 \u0026gt; ./db.port $ kubectl create cm test-config2 --from-file=./db.host --from-file=./db.port 将环境变量写入文件，通过from-file导入。\n3.1.1.3 通过YAML配置 1 2 3 4 5 6 7 apiVersion: v1 kind: ConfigMap metadata: name: test-config4 data: db.host: 172.18.8.200 db.port: \u0026#34;3306\u0026#34; 直接写好configmap的配置，通过yaml文件创建API资源对象。\n3.2 使用ConfigMap 使用ConfigMap的方式主要有两种：\n第一种是通过环境变量的方式，直接传递给pod； 第二种是作为volume的方式挂载到pod内。 3.2.1 通过环境变量使用 使用valueFrom、configMapKeyRef、name、key指定要用的key，如上面的实践1。当然也可以通过envFrom、configMapRef、name使得configmap中的所有key/value对都自动变成环境变量。如：\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: busybox args: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 3000\u0026#34; ] envFrom: - configMapRef: name: test-config4 3.2.2 作为volume数据卷被挂载使用 通过将ConfigMap定义为数据卷，挂载到容器的某路径中。如上面的实践2。当然我们也可以在 ConfigMap 值被映射的数据卷里去控制路径，如下 Pod 定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: testcm2-pod spec: volumes: - name: config-volume configMap: name: mysql-cm items: - key: mysql.conf path: path/to/msyql.conf containers: - name: testcm2 image: busybox command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/path/to/msyql.cnf\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config 参考链接：https://blog.51cto.com/wzlinux/2331050\n全文完。\n","description":"应用的配置在Kubernetes中可通过ConfigMap资源对象来实现，避免被直接写死到应用程序中。","id":7,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"09-往容器中注入配置信息","uri":"https://laomeinote.com/posts/add-configuration-info-with-configmap/"},{"content":"除了基于 CPU 和内存来进行自动扩缩容之外，有些业务需要基于自定义指标进行HPA弹性伸缩，通用做法是结合prometheus能力。\n如上图示，基于自定义指标进行HPA弹性的核心原理为：\n业务能够暴露指标，注意是prometheus格式的指标，即能被prometheus识别，一般是暴露到prometheus的/metrics下。 Kubernetes并不能识别Prometheus格式指标，所以这时需要通过Prometheus Adapter将其进行转换，转换为Kubernetes metrics API. Kubernetes的HPA控制器通过Kubernetes metrics API识别到业务指标变化，根据HPA策略来发起POD的弹性伸缩。 1. 准备工作 集群完成安装prometheus插件\nprometheus插件已经安装完成prometheus-adapter\n业务定义暴露的指标\n2. 验证步骤 1）基于自定义指标进行HPA弹性伸缩，首先需要业务侧暴露自己的指标；\n2）在prometheus中查阅到业务自定义指标，基于指标定义HPA策略\n3）模拟增加业务流量触发HPA策略\n2.1 业务自定义指标 准备一个简单的go web 应用，能够记录请求次数并经过 /metrics 端点输出 Prometheus 格式的指标 http_requests_total。\nmain.go代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package main import ( \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { metrics := prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_requests_total\u0026#34;, Help: \u0026#34;Number of total http requests\u0026#34;, }, []string{\u0026#34;status\u0026#34;}, ) prometheus.MustRegister(metrics) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { path := r.URL.Path statusCode := 200 switch path { case \u0026#34;/metrics\u0026#34;: promhttp.Handler().ServeHTTP(w, r) default: w.WriteHeader(statusCode) w.Write([]byte(\u0026#34;Hello World!\\n\u0026#34;)) } metrics.WithLabelValues(strconv.Itoa(statusCode)).Inc() }) http.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } 将应用打包成镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 FROM golang:1.17-alpine as builder WORKDIR /workspace COPY go.mod . COPY go.sum . RUN go env -w GO111MODULE=on RUN go env -w GOPROXY=http://xxx.xxx.xxx.com/goproxy/ RUN go env -w GONOSUMDB=* RUN go mod download COPY . . RUN go build -o /out/httpserver . FROM alpine:3.12 COPY --from=builder /out/httpserver /app/httpserver EXPOSE 3000 ENTRYPOINT [\u0026#34;/app/httpserver\u0026#34;] 通过Dockerfile将业务打包成镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 ➜ docker build . -t go_hpa_test_:v1.0 Sending build context to Docker daemon 330.5MB Step 1/14 : FROM golang:1.17-alpine as builder ---\u0026gt; d8bf44a3f6b4 Step 2/14 : WORKDIR /workspace ---\u0026gt; Using cache ---\u0026gt; aa4724347b00 Step 3/14 : COPY go.mod . ---\u0026gt; Using cache ---\u0026gt; fba2a0df8d07 Step 4/14 : COPY go.sum . ---\u0026gt; Using cache ---\u0026gt; 97f38bc17bba Step 5/14 : RUN go env -w GO111MODULE=on ---\u0026gt; Running in be2663fc36da Removing intermediate container be2663fc36da ---\u0026gt; 95280728aa87 Step 6/14 : RUN go env -w GOPROXY=http://xxx.xxx.xxx.com/goproxy/ ---\u0026gt; Running in 4e5b01768775 Removing intermediate container 4e5b01768775 ---\u0026gt; 10d61346ee92 Step 7/14 : RUN go env -w GONOSUMDB=* ---\u0026gt; Running in f35b7aa9c7e4 Removing intermediate container f35b7aa9c7e4 ---\u0026gt; 278f85c1206e Step 8/14 : RUN go mod download ---\u0026gt; Running in 4dd31a66aa4b Removing intermediate container 4dd31a66aa4b ---\u0026gt; 973554f40b46 Step 9/14 : COPY . . ---\u0026gt; 369a6973ab07 Step 10/14 : RUN go build -o /out/httpserver . ---\u0026gt; Running in 9f77356a0c55 Removing intermediate container 9f77356a0c55 ---\u0026gt; a367baf79a62 Step 11/14 : FROM alpine:3.12 ---\u0026gt; b0925e081921 Step 12/14 : COPY --from=builder /out/httpserver /app/httpserver ---\u0026gt; 38c8af1e9c72 Step 13/14 : EXPOSE 3000 ---\u0026gt; Running in 11986e605c45 Removing intermediate container 11986e605c45 ---\u0026gt; 0a48b326db28 Step 14/14 : ENTRYPOINT [\u0026#34;/app/httpserver\u0026#34;] ---\u0026gt; Running in cde4621aa699 Removing intermediate container cde4621aa699 ---\u0026gt; 84fc14c9f6c1 Successfully built 84fc14c9f6c1 Successfully tagged hpa_test_img:v1.0 需要注意的是，在内网无法拉取golang:1.17-alpine和alpine:3.12镜像，可以先在可联网的设备上下载好镜像在load加载进来。\n上面的Dockerfiler会生成go_hpa_test:v1.0镜像，可以将镜像push到ECR镜像私有仓，方便后续从Kubernetes中创建Deployment，在各个节点生成POD。\n1 2 3 ➜ docker login -u m00602320 xxx.xxx.xxx.com:80 #输入账号密码 ➜ docker tag go_hpa_test:v1.0 xxx.xxx.xxx.com:80/repo-mxh/go_hpa_test:v1.0 ➜ docker push xxx.xxx.xxx.com:80/repo-mxh/go_hpa_test:v1.0 创建Deployment，sample-httpserver-deployment.yaml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: Deployment metadata: labels: app: sample-httpserver name: sample-httpserver namespace: default spec: replicas: 1 selector: matchLabels: app: sample-httpserver strategy: {} template: metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: /metrics prometheus.io/port: \u0026#34;3000\u0026#34; labels: app: sample-httpserver spec: imagePullSecrets: - name: myregcred containers: - image: xxx.xxx.xxx.com:80/repo-mxh/go_hpa_test:v1.0 name: httpserver-n-metrics ports: - containerPort: 3000 resources: requests: memory: \u0026#39;300Mi\u0026#39; apply生成负载：\n1 2 3 4 5 6 7 8 9 10 ➜ kubectl apply -f sample-httpserver-deployment.yaml deployment.apps/sample-httpserver created ➜ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE sample-httpserver 1/1 1 1 8s ➜ kubectl get po NAME READY STATUS RESTARTS AGE sample-httpserver-6784dcf77c-56th4 1/1 Running 0 14s sample-service.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Service metadata: name: http-sample-svc labels: app: http-sample-svc spec: ports: - port: 8080 targetPort: 3000 # pod port 与上面3000保持一致 protocol: TCP name: http type: NodePort selector: app: sample-httpserver #这里与上面的pod的labels要一致 apply生成service：\n1 2 3 4 5 6 ➜ kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR http-sample-svc NodePort 12.12.126.83 \u0026lt;none\u0026gt; 8080:30826/TCP 9s app=sample-httpserver ➜ curl http://12.12.126.83:8080 #在集群节点上访问上面的应用服务 Hello World! 因为服务的类型设置为NodePort了，所以我们在集群外部也可以通过NodeIP:NodePort来访问：\n通过/metrics来查看业务指标：\n2.2 通过Prometheus查看业务指标 prometheus安装完成后，可修改对应服务为NodePort类型，然后通过访问Node节点的方式访问Prometheus服务。\n在prometheus中可见服务指标已经上报，如下图。\n也可用PromQL语句聚合查看指标：\n2.3 修改Prometheus Adapter自定义HPA指标 Prometheus Adapter的配置文件写在configMap中，查看配置信息并添加自定义指标：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ➜ kubectl get cm -nmonitoring NAME DATA AGE adapter-config 1 112m grafana-conf 1 112m grafana-dashboard-k8s-cluster-rsrc-use 1 112m grafana-dashboard-k8s-cluster-usage 1 112m grafana-dashboard-k8s-node-rsrc-use 1 112m grafana-dashboard-k8s-resources-cluster 1 112m grafana-dashboard-k8s-resources-namespace 1 112m grafana-dashboard-k8s-resources-pod 1 112m grafana-dashboard-k8s-resources-workload 1 112m grafana-dashboard-k8s-resources-workloads-namespace 1 112m grafana-dashboard-nodes 1 112m grafana-dashboard-persistentvolumesusage 1 112m grafana-dashboard-pods 1 112m grafana-dashboard-statefulset 1 112m grafana-dashboards 1 112m grafana-datasources 1 112m prometheus 1 112m prometheus-server-record-rules 1 112m ➜ kubectl edit cm adapter-config -nmonitoring configmap/adapter-config edited 修改adapter-config，在rule中增加如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 - seriesQuery: \u0026#39;{__name__=~\u0026#34;^http_requests.*_total$\u0026#34;,kubernetes_namespace!=\u0026#34;\u0026#34;,kubernetes_pod!=\u0026#34;\u0026#34;}\u0026#39; seriesFilters: [] resources: overrides: kubernetes_namespace: resource: namespace kubernetes_pod: resource: pod name: matches: \u0026#34;(.*)_total\u0026#34; as: \u0026#34;${1}_qps\u0026#34; metricsQuery: sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[30s])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) 修改Adapter配置后需要重启Prometheus Adapter，删除POD后即可重新拉起一个新的POD。\n1 2 3 4 5 6 7 8 9 10 11 ➜ kubectl delete po custom-metrics-apiserver-6ffdb9d866-ht8p2 -nmonitoring ➜ kubectl get po -nmonitoring NAME READY STATUS RESTARTS AGE cceaddon-prometheus-kube-state-metrics-566fd599d5-f258v 1/1 Running 0 6h46m cceaddon-prometheus-node-exporter-btn4d 1/1 Running 0 6h46m cceaddon-prometheus-node-exporter-dq4kb 1/1 Running 0 6h46m cceaddon-prometheus-operator-5c8d5fdbb7-7bvp8 1/1 Running 0 6h46m custom-metrics-apiserver-6ffdb9d866-bgpdm 1/1 Running 0 32s grafana-65ccf49786-mqf2f 1/1 Running 0 6h46m prometheus-0 2/2 Running 0 6h46m 重启后可以获取到自定义指标。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ➜ kubectl get --raw \u0026#39;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_qps\u0026#39; |jq . { \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_qps\u0026#34; }, \u0026#34;items\u0026#34;: [ { \u0026#34;describedObject\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sample-httpserver-6784dcf77c-56th4\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34; }, \u0026#34;metricName\u0026#34;: \u0026#34;http_requests_qps\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2022-02-18T09:25:53Z\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;66m\u0026#34;, \u0026#34;selector\u0026#34;: null } ] } 注意：这里的 value: 66m，值的后缀“m” 标识 milli-requests per seconds，因此这里的 66m 的意思是 0.1/s 每秒0.066 个请求。\n定义HPA策略，sample-httpserver-hpa.yaml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: sample-httpserver spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: sample-httpserver minReplicas: 1 maxReplicas: 10 behavior: scaleDown: stabilizationWindowSeconds: 5 policies: - type: Percent value: 100 periodSeconds: 5 scaleUp: stabilizationWindowSeconds: 0 policies: - type: Percent value: 100 periodSeconds: 5 metrics: - type: Pods pods: metric: name: http_requests_qps target: type: AverageValue averageValue: 50000m 在这里，定义http_requests_qps的目标值为50000m，表示平均每个POD的RPS为50，假如300的RPS则对应副本数为300/50 = 6.\napply生产HPA对象：\n1 2 3 4 5 6 ➜ kubectl apply -f sample-httpserver-hpa.yaml ➜ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 1 50d sample-httpserver Deployment/sample-httpserver 66m/500m 1 10 1 1m 2.4 压测触发HPA弹性 这里使用vegeta工具来做压测：\n1 2 3 4 # 安装vegeta ➜ wget https://github.com/tsenart/vegeta/releases/download/v12.8.4/vegeta_12.8.4_linux_amd64.tar.gz ➜ tar xzvf vegeta_12.8.4_linux_amd64.tar.gz ➜ mv vegeta /usr/local/bin/ 分别使用 240、120、40 的 RPS 发起请求：\n1 2 3 4 5 6 # 240,将XXX换成svc IP ➜ echo \u0026#34;GET http://xxxx:30826\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 240 | vegeta report # 120,将XXX换成svc IP ➜ echo \u0026#34;GET http://xxxx:30826\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 120 | vegeta report # 40,将XXX换成svc IP ➜ echo \u0026#34;GET http://xxxx:30826\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 40 | vegeta report 对应在prometheus使用PromQL语句sum(rate(http_requests_total[30s])) by (pod)上可观测到业务指标变化：\nOK，正式测试，使用RPS为1000测试1分钟看看。\n1 2 3 4 5 6 7 8 9 ➜ echo \u0026#34;GET http://xxx:30826\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 1000 | vegeta report Requests [total, rate, throughput] 60000, 1000.02, 1000.01 Duration [total, attack, wait] 59.999s, 59.999s, 331.936µs Latencies [min, mean, 50, 90, 95, 99, max] 225.81µs, 453.246µs, 381.693µs, 456.696µs, 520.12µs, 2.592ms, 16.406ms Bytes In [total, mean] 720000, 12.00 Bytes Out [total, mean] 0, 0.00 Success [ratio] 100.00% Status Codes [code:count] 200:60000 Error Set: 查看HPA伸缩情况与POD的实际变化情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 ➜ kubectl get hpa sample-httpserver -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE sample-httpserver Deployment/sample-httpserver 66m/50 1 10 1 5h33m sample-httpserver Deployment/sample-httpserver 244933m/50 1 10 1 5h36m sample-httpserver Deployment/sample-httpserver 1000133m/50 1 10 2 5h36m sample-httpserver Deployment/sample-httpserver 694800m/50 1 10 4 5h37m sample-httpserver Deployment/sample-httpserver 567033m/50 1 10 8 5h37m sample-httpserver Deployment/sample-httpserver 127582m/50 1 10 10 5h37m sample-httpserver Deployment/sample-httpserver 66m/50 1 10 10 5h37m sample-httpserver Deployment/sample-httpserver 66m/50 1 10 3 5h38m sample-httpserver Deployment/sample-httpserver 66m/50 1 10 1 5h38m ➜ kubectl get po -l app=sample-httpserver -w NAME READY STATUS RESTARTS AGE sample-httpserver-6784dcf77c-56th4 1/1 Running 0 5h57m sample-httpserver-6784dcf77c-df4cd 0/1 Pending 0 0s sample-httpserver-6784dcf77c-df4cd 0/1 Pending 0 0s sample-httpserver-6784dcf77c-df4cd 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-df4cd 0/1 ContainerCreating 0 1s sample-httpserver-6784dcf77c-df4cd 1/1 Running 0 2s sample-httpserver-6784dcf77c-g8bhk 0/1 Pending 0 0s sample-httpserver-6784dcf77c-g8bhk 0/1 Pending 0 0s sample-httpserver-6784dcf77c-w9blj 0/1 Pending 0 0s sample-httpserver-6784dcf77c-w9blj 0/1 Pending 0 0s sample-httpserver-6784dcf77c-g8bhk 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-w9blj 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-w9blj 0/1 ContainerCreating 0 2s sample-httpserver-6784dcf77c-w9blj 1/1 Running 0 2s sample-httpserver-6784dcf77c-g8bhk 0/1 ContainerCreating 0 2s sample-httpserver-6784dcf77c-g8bhk 0/1 ContainerCreating 0 3s sample-httpserver-6784dcf77c-g8bhk 1/1 Running 0 3s sample-httpserver-6784dcf77c-2hrdp 0/1 Pending 0 0s sample-httpserver-6784dcf77c-2hrdp 0/1 Pending 0 0s sample-httpserver-6784dcf77c-pmr6f 0/1 Pending 0 0s sample-httpserver-6784dcf77c-6rklq 0/1 Pending 0 0s sample-httpserver-6784dcf77c-pmr6f 0/1 Pending 0 0s sample-httpserver-6784dcf77c-6rklq 0/1 Pending 0 0s sample-httpserver-6784dcf77c-2hrdp 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-pmr6f 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-6rklq 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-h5wpj 0/1 Pending 0 0s sample-httpserver-6784dcf77c-h5wpj 0/1 Pending 0 0s sample-httpserver-6784dcf77c-h5wpj 0/1 ContainerCreating 0 0s sample-httpserver-6784dcf77c-2hrdp 0/1 ContainerCreating 0 2s sample-httpserver-6784dcf77c-6rklq 0/1 ContainerCreating 0 2s sample-httpserver-6784dcf77c-2hrdp 1/1 Running 0 3s sample-httpserver-6784dcf77c-pmr6f 0/1 ContainerCreating 0 3s sample-httpserver-6784dcf77c-h5wpj 0/1 ContainerCreating 0 3s sample-httpserver-6784dcf77c-pmr6f 1/1 Running 0 4s sample-httpserver-6784dcf77c-6rklq 1/1 Running 0 4s sample-httpserver-6784dcf77c-h5wpj 1/1 Running 0 4s sample-httpserver-6784dcf77c-zb2sx 0/1 Pending 0 0s sample-httpserver-6784dcf77c-zb2sx 0/1 Pending 0 0s sample-httpserver-6784dcf77c-gkh2f 0/1 Pending 0 0s sample-httpserver-6784dcf77c-gkh2f 0/1 Pending 0 0s sample-httpserver-6784dcf77c-pmr6f 1/1 Terminating 0 45s sample-httpserver-6784dcf77c-h5wpj 1/1 Terminating 0 45s sample-httpserver-6784dcf77c-gkh2f 0/1 Terminating 0 30s sample-httpserver-6784dcf77c-g8bhk 1/1 Terminating 0 60s sample-httpserver-6784dcf77c-2hrdp 1/1 Terminating 0 45s sample-httpserver-6784dcf77c-zb2sx 0/1 Terminating 0 30s sample-httpserver-6784dcf77c-6rklq 1/1 Terminating 0 45s sample-httpserver-6784dcf77c-zb2sx 0/1 Terminating 0 30s sample-httpserver-6784dcf77c-gkh2f 0/1 Terminating 0 30s sample-httpserver-6784dcf77c-pmr6f 0/1 Terminating 0 46s sample-httpserver-6784dcf77c-pmr6f 0/1 Terminating 0 47s sample-httpserver-6784dcf77c-pmr6f 0/1 Terminating 0 47s sample-httpserver-6784dcf77c-2hrdp 0/1 Terminating 0 47s sample-httpserver-6784dcf77c-6rklq 0/1 Terminating 0 47s sample-httpserver-6784dcf77c-h5wpj 0/1 Terminating 0 47s sample-httpserver-6784dcf77c-g8bhk 0/1 Terminating 0 62s sample-httpserver-6784dcf77c-2hrdp 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-2hrdp 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-6rklq 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-6rklq 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-h5wpj 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-h5wpj 0/1 Terminating 0 58s sample-httpserver-6784dcf77c-g8bhk 0/1 Terminating 0 73s sample-httpserver-6784dcf77c-g8bhk 0/1 Terminating 0 73s sample-httpserver-6784dcf77c-df4cd 1/1 Terminating 0 90s sample-httpserver-6784dcf77c-w9blj 1/1 Terminating 0 75s sample-httpserver-6784dcf77c-w9blj 0/1 Terminating 0 76s sample-httpserver-6784dcf77c-df4cd 0/1 Terminating 0 91s sample-httpserver-6784dcf77c-df4cd 0/1 Terminating 0 92s sample-httpserver-6784dcf77c-df4cd 0/1 Terminating 0 92s sample-httpserver-6784dcf77c-w9blj 0/1 Terminating 0 77s sample-httpserver-6784dcf77c-w9blj 0/1 Terminating 0 77s 参考文章：\nhttps://www.shangmayuan.com/a/da1fc7f47f684c71938fd3f2.html\nhttps://www.qikqiak.com/k8strain/monitor/adapter/#_3\n全文完。\n","description":"除了基于 CPU 和内存来进行自动扩缩容之外，有些业务需要基于自定义指标进行HPA弹性伸缩，通用做法是结合prometheus能力。","id":8,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"08-基于自定义指标实现HPA弹性","uri":"https://laomeinote.com/posts/how-to-make-hpa-with-customized-metrics-in-k8s/"},{"content":"Kubernetes中弹性伸缩最主要的就是使用HPA（Horizontal Pod Autoscaling）和CA（Cluster AutoScaling）两种弹性伸缩策略，HPA负责工作负载弹性伸缩，也就是应用层面的弹性伸缩，CA负责节点弹性伸缩，也就是资源层面的弹性伸缩。\n通常情况下，两者需要配合使用，因为HPA需要集群有足够的资源才能扩容成功，当集群资源不够时需要CA扩容节点，使得集群有足够资源；而当HPA缩容后集群会有大量空余资源，这时需要CA缩容节点释放资源，才不至于造成浪费。\n本文先演示HPA弹性伸缩，基于CPU和内存使用率实现POD实例的弹性。\n1. 准备工作 本次实验基于EKS集群，需要集群安装metrics-server插件，该插件能够收集包括Pod、Node、容器、Service等主要Kubernetes核心资源的度量数据,并通过标准的 Kubernetes API 把数据暴露出来。\n2. 工作原理 在负载中指定多个POD副本，metrics-server可收集到这些POD使用的CPU与内存度量数据； 创建一个HorizontalPodAutoscaler对象，在对象的定义中指定CPU与内存策略，比如超过CPU 50%时扩容POD； 模拟CPU和内存增加场景，使得POD加大CPU和内存使用量，触发HPA策略生效。 **扩缩容决策算法：**HPA controller根据当前指标和期望指标来计算缩放比例，计算公式如下：\ndesiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]\n参数1：currentMetricValue 表示当前的值\n参数2：desiredMetricValue 表示期望的值\n参数3：currentReplicas 表示当前的POD副本数\n参数4：desiredReplicas 表示希望扩容后的POD副本数\n比如：有一个负载，创建了一个POD，它当前CPU利用率是20%，我们期望它的利用率不要超过30%，即desiredMetricValue = 30%；当它业务激增到60%的时候，currentMetricValue = 60%；\n那么这时，期望的POD数 = 1 * (60/30) = 2，HPA控制器就会启动扩容增加一个POD实例；这个POD启动后它能够分担业务流量，两个POD的CPU降低，不再扩容；如果这时两个POD的流量继续增加一倍，业务压力导致CPU继续飙升一倍，则期望的POD数继续翻倍为4；业务压力下降时，POD随之缩容。\n在实际过程中，可能会遇到实例数值反复伸缩，导致集群震荡。为了保证稳定性，HPA controller从以下几个方面进行优化：：\n冷却时间：在1.11版本以及之前的版本，社区引入了horizontal-pod-autoscaler-downscale-stabilization-window和horizontal-pod-autoScaler-upscale-stabilization-window这两个启动参数代表缩容冷却时间和扩容冷却时间，这样保证在冷却时间内，跳过扩缩容。1.14版本之后引入延迟队列，保存一段时间内每一次检测的决策建议，然后根据当前所有有效的决策建议来进行决策，从而保证期望的副本数尽量小的发生变更，保证稳定性。如，设置downscaleWindow的值为1m表示1分钟内不缩容,upscaleWindow的值为1m表示1分钟内不扩容。\n忍受度：可以看成一个缓冲区，当实例变化范围在忍受范围之内的话，保持原有的实例数不变。\n比如：CPU缓冲区为30%-50%，表示当前CPU利用率在30%~50%之间时不会扩容缩容，只有低于30%时才会缩容，高于50%时才会扩容。\n3. 测试验证 分别创建CPU和内存策略演示。\n3.1 创建工作负载 基于Nginx镜像创建无状态工作负载，副本数为1。负载01-nginx-demo.yaml内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kind: Deployment apiVersion: apps/v1 metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: hpa-nginx template: metadata: labels: app: hpa-nginx spec: containers: - name: container-1 image: \u0026#39;xxx.xxx.xxx.com:80/public/nginx:x86-1.20.1\u0026#39; resources: limits: # limits与requests建议取值保持一致，避免扩缩容过程中出现震荡 cpu: 500m memory: 200Mi requests: cpu: 500m memory: 200Mi 基于yaml资源文件创建负载deployment。\n1 2 3 4 5 6 7 8 ➜ kubectl apply -f deployment.yaml deployment.apps/nginx configured ➜ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 3m5s ➜ kubectl get po -l app=hpa-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5dbc9b8ddc-7wbc7 1/1 Running 0 2m6s 12.11.0.52 10.247.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 这里可见一个名为nginx-5dbc9b8ddc-7wbc7的POD已经在running了，其获取到一个集群内部IP12.11.0.52。接下来创建一个HPA策略，并逐步增加该POD的CPU负载触发HPA弹性。\n3.2 创建CPU相关HPA策略 创建HPA策略，如下所示。\nhpa-demo.yaml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: hpa-example annotations: extendedhpa.metrics: \u0026#39;[{\u0026#34;type\u0026#34;:\u0026#34;Resource\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;cpu\u0026#34;,\u0026#34;targetType\u0026#34;:\u0026#34;Utilization\u0026#34;,\u0026#34;targetRange\u0026#34;:{\u0026#34;low\u0026#34;:\u0026#34;40\u0026#34;,\u0026#34;high\u0026#34;:\u0026#34;55\u0026#34;}}]\u0026#39; extendedhpa.option: \u0026#39;{\u0026#34;downscaleWindow\u0026#34;:\u0026#34;1m\u0026#34;,\u0026#34;upscaleWindow\u0026#34;:\u0026#34;1m\u0026#34;}\u0026#39; name: hpa-example namespace: default spec: scaleTargetRef: kind: Deployment name: nginx apiVersion: apps/v1 minReplicas: 1 maxReplicas: 100 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 该策略关联了名为nginx的Deployment，期望CPU使用率为50%： targetAverageUtilization 为50。指明了最大副本数maxReplicas为100，最小副本数为minReplicas为1。\n另外有两条注解annotations，一条是CPU的阈值范围，最低40，最高55，表示CPU使用率在40%到55%之间时，不会扩缩容，防止小幅度波动造成影响。另一条是扩缩容时间窗，表示策略成功触发后，在缩容/扩容冷却时间内，不会再次触发缩容/扩容，以防止短期波动造成影响。\n如果这条HPA策略生效了，那什么时候扩容缩容呢？根据上面的公式：desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]\n可知：\n1）只有当CPU利用率超过55%的时候，会扩容；只有当CPU利用率低于40%的时候会缩容；\n2）如果POD当前CPU利用率为70%时，根据上面的公式计算期望的POD数 =ceil[ 1 x（70/50) ] = ceil[1.4]，向下取整为1，不扩容，当CPU超过75%时，期望的POD数 = ceil [1 x (75/50)] = 1.5，向上取整为2，扩容一个POD。当POD的CPU利用率低于40%会考虑缩容，如CPU利用率为20%，根据上面的公式计算期望的POD数 = ceil[ 1 x (20/50) ] = ceil[0.4] \u0026lt; 1，会缩容一个POD。\n3）扩容缩容后的1分钟内不会再次扩容缩容，防止POD数频繁变化，引起震荡。\n创建HPA策略。\n1 2 3 4 5 6 7 8 ➜ kubectl apply -f hpa-demo.yaml horizontalpodautoscaler.autoscaling/hpa-example configured ➜ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/hpa-example \u0026lt;unknown\u0026gt;/50% 1 100 0 5s ➜ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/hpa-example 0%/50% 1 100 1 44s HPA策略创建成功后，短暂时刻内获取不到指标，显示的TARGETS为unknown，片刻后数字变为0。 也可通过kubectl top命令检查Node与POD指标是否能正常获取：\n1 2 3 4 5 6 7 8 9 ➜ kubectl top node #检查Node节点指标 NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% 10.247.154.106 1383m 35% 4644Mi 80% 10.247.154.147 1240m 31% 4232Mi 73% 10.247.154.161 1184m 30% 3275Mi 56% 10.247.154.39 1348m 34% 3268Mi 56% ➜ kubectl top pod -l app=hpa-example #检查POD指标 NAME CPU(cores) MEMORY(bytes) nginx-5dbc9b8ddc-7wbc7 0m 3Mi 可见当前的POD - nginx-5dbc9b8ddc-7wbc7的CPU利用率为0个M，接下来进入POD内部，模拟增加CPU负荷来触发HPA。\n3.3 增加POD的CPU负载触发HPA弹性 下面命令会创建 CPU 负荷，方法是通过压缩随机数据并将结果发送到 /dev/null：\n1 cat /dev/urandom | gzip -9 \u0026gt; /dev/null 如果想要更大的负荷，或者系统有多个核，那么只需要对数据进行压缩和解压就行了，像这样：\n1 cat /dev/urandom | gzip -9 | gzip -d | gzip -9 | gzip -d \u0026gt; /dev/null 可按下 CTRL+C 来终止上面的进程。\n我们进入单个POD nginx-5dbc9b8ddc-7wbc里，执行上面的命令，模拟CPU负载增加，触发HPA控制器扩容POD。同时观察hpa扩容情况，很快扩容出一个POD nginx-5dbc9b8ddc-qbmk9了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ➜ kubectl get hpa hpa-example --watch #观察hpa控制器扩缩容动作 NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/nginx 0%/50% 1 100 1 49d hpa-example Deployment/nginx 99%/50% 1 100 1 49d hpa-example Deployment/nginx 100%/50% 1 100 1 49d hpa-example Deployment/nginx 100%/50% 1 100 2 49d hpa-example Deployment/nginx 50%/50% 1 100 2 49d hpa-example Deployment/nginx 49%/50% 1 100 2 49d hpa-example Deployment/nginx 50%/50% 1 100 2 49d ➜ kubectl get po -l app=hpa-nginx #获取负载pod情况 NAME READY STATUS RESTARTS AGE nginx-5dbc9b8ddc-7wbc7 1/1 Running 0 3h13m nginx-5dbc9b8ddc-qbmk9 1/1 Running 0 95s ➜ kubectl top po -l app=hpa-nginx #获取负载pod的指标 NAME CPU(cores) MEMORY(bytes) nginx-5dbc9b8ddc-7wbc7 501m 5Mi nginx-5dbc9b8ddc-qbmk9 0m 3Mi 由于这里只是进入到第一个POD里模拟CPU负载增加，所以只有第一个POD的CPU增加，第二个并没有增加，如果这时终止CPU负载增加命令后，1分钟后会看到第二个POD会被缩容掉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 ➜ kubectl get hpa hpa-example --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/nginx 0%/50% 1 100 1 49d hpa-example Deployment/nginx 99%/50% 1 100 1 49d hpa-example Deployment/nginx 100%/50% 1 100 1 49d hpa-example Deployment/nginx 100%/50% 1 100 2 49d hpa-example Deployment/nginx 50%/50% 1 100 2 49d hpa-example Deployment/nginx 49%/50% 1 100 2 49d hpa-example Deployment/nginx 50%/50% 1 100 2 49d hpa-example Deployment/nginx 50%/50% 1 100 2 49d hpa-example Deployment/nginx 0%/50% 1 100 2 49d hpa-example Deployment/nginx 0%/50% 1 100 2 49d hpa-example Deployment/nginx 0%/50% 1 100 1 49d 3.4 增加POD的内存负载触发HPA弹性 内存指标跟CPU负载类似，在yaml中添加mem字段即可，hpa-demo.yaml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: hpa-example annotations: extendedhpa.metrics: \u0026#39;[{\u0026#34;type\u0026#34;:\u0026#34;Resource\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;cpu\u0026#34;,\u0026#34;targetType\u0026#34;:\u0026#34;Utilization\u0026#34;,\u0026#34;targetRange\u0026#34;:{\u0026#34;low\u0026#34;:\u0026#34;40\u0026#34;,\u0026#34;high\u0026#34;:\u0026#34;55\u0026#34;}},{\u0026#34;type\u0026#34;:\u0026#34;Resource\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;memory\u0026#34;,\u0026#34;targetType\u0026#34;:\u0026#34;Utilization\u0026#34;,\u0026#34;targetRange\u0026#34;:{\u0026#34;low\u0026#34;:\u0026#34;40\u0026#34;,\u0026#34;high\u0026#34;:\u0026#34;55\u0026#34;}}]\u0026#39; extendedhpa.option: \u0026#39;{\u0026#34;downscaleWindow\u0026#34;:\u0026#34;1m\u0026#34;,\u0026#34;upscaleWindow\u0026#34;:\u0026#34;1m\u0026#34;}\u0026#39; name: hpa-example namespace: default spec: scaleTargetRef: kind: Deployment name: nginx apiVersion: apps/v1 minReplicas: 1 maxReplicas: 100 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 - type: Resource resource: name: memory targetAverageUtilization: 50 上面的声明指定HPA策略为：CPU在40%-55%之间不做扩缩容，内存在40%-55%之间不扩缩容，超出阈值时，根据计算公式desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]，扩容POD或者缩容POD。\n更新该HPA策略。\n1 ➜ kubectl apply -f hpa-demo.yaml 模拟内存占用触发HPA策略。内存的模拟有几种方式：\n1）常用的方法为通过mount命名将RAMFS文件系统挂载到一个目录，这个文件系统是内存型文件系统，通过dd命令往挂载目录里写大文件就会增加内存使用率。如：\n1 2 3 mkdir z mount -t ramfs ramfs z/ dd if=/dev/zero of=z/file bs=1M count=128 #使用 dd 在该目录下创建一个 128M 的文件 2）通过stress 工具和 lookbusy 工具实现精准、可控和易用的控制。\n如：\n1 2 3 4 5 6 7 8 9 10 11 stress --cpu 2 # 产生 2 个工作进程对 CPU 施加压力，也就是将会占用两个 CPU 核心 stress --vm 1 --vm-bytes 128M --vm-hang 0 # 产生 1 个工作进程，占用 128MB 内存并保持 stress --io 1 # 产生 1 个工作进程对 IO 施加压力 lookbusy -c 50 # 占用所有 CPU 核心各 50% lookbusy -c 50 -n 2 # 占用两个 CPU 核心各 50% lookbusy -c 50-80 -r curve # 占用所有 CPU 核心在 50%-80% 左右浮动 lookbusy -c 0 -m 128MB -M 1000 # 每 1000 毫秒，循环释放并分配 128MB 内存 lookbusy -c 0 -d 1GB -b 1MB -D 10 # 每 10 毫秒，循环进行 1MB 磁盘写入，临时文件不超过 1GB #以上命令的参数均可结合使用，同时对系统多个维度施加压力。 这两种方法都可以单纯模拟内存增加，但是方法1需要在容器内部mount volume，需修改docker启动方式时加上--privileged，方法2又需要在容器镜像中安装stress和lookbusy工具，比较麻烦不单独演示。\n3.5 暴露服务并循环访问服务触发HPA 给pod加上一个service，在集群内部通过service作为前端接入分流到POD上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx-svc spec: ports: - port: 8080 targetPort: 80 protocol: TCP name: http - port: 443 protocol: TCP name: https selector: app: hpa-nginx #这里与上面的pod的labels要一致 创建并查询服务：\n1 2 3 4 5 6 7 8 9 10 ➜ kubectl apply -f service.yaml service/nginx-svc created ➜ kubectl get po -l app=hpa-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-58c7b6bf67-fs6nn 1/1 Running 0 13m 12.11.0.121 10.247.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ➜ kubectl get svc -l app=hpa-nginx -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc ClusterIP 12.12.232.122 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 12h app=hpa-nginx 在集群节点上模拟循环访问该service触发HPA扩容。\n1 ➜ while true;do wget -q -O- http://12.12.232.122:8080; done 由于模拟过程中CPU利用率一直不会冲高很多，所以我们将HPA策略中的target阈值调整一下,改为最低20%，最高30%，期望值为25%，改完再观察HPA伸缩过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ➜ kubectl get hpa hpa-example --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/nginx 2%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 2%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, \u0026lt;unknown\u0026gt;/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 54%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 36%/25% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 36%/25% 1 100 1 49d hpa-example Deployment/nginx 0%/49%, 30%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 16%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 22%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 19%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 24%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 16%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 20%/25% 1 100 2 49d 可见一会儿pod副本从1变为2，如果停止循环访问服务，约1分钟后POD副本数将为1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 kubectl get hpa hpa-example --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example Deployment/nginx 2%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 2%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, \u0026lt;unknown\u0026gt;/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 54%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/50% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 36%/25% 1 100 1 49d hpa-example Deployment/nginx 1%/49%, 36%/25% 1 100 1 49d hpa-example Deployment/nginx 0%/49%, 30%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 16%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 22%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 19%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 24%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 16%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 20%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 18%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 23%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 22%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 28%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 26%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 24%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 2 49d hpa-example Deployment/nginx 1%/49%, 0%/25% 1 100 1 49d 最后一条log显示已经POD数已经将为1了。\n全文完。\n","description":"Kubernetes中弹性伸缩最主要的就是使用HPA（Horizontal Pod Autoscaling）和CA（Cluster AutoScaling）两种弹性伸缩策略，HPA负责工作负载弹性伸缩，也就是应用层面的弹性伸缩。","id":9,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"07-基于CPU和内存指标实现HPA弹性","uri":"https://laomeinote.com/posts/how-to-make-hpa-with-basic-metrics-in-k8s/"},{"content":"1、原理 通过K8s的亲和性和反亲和性设置，可以将POD在节点间调度。\n亲和性（affinity）：将多个POD部署在同一个节点上，容器间通信无需转发路由，减少网络消耗。 反亲和性（anti-affinity）：将多个POD部署在不同节点上，满足业务容灾冗余需求，比如节点1挂了，节点2的POD还能提供服务。不过，K8s本身也会在pod消亡时自动寻找节点部署业务。 亲和与反亲和有两种条件：\n必须满足： 即硬约束，关键字为requiredDuringSchedulingIgnoredDuringExecution。 尽量满足： 即软约束，关键字为preferredDuringSchedulingIgnoredDuringExecution。 除此之外，还有几个关键概念：\n● 拓扑域： 即topologyKey，拓扑域通过设置工作节点的标签，包含默认和自定义标签，用于指定调度时作用域。比如topologyKey: failure-domain.beta.kubernetes.io/zone指的拓扑域是AZ域，topologyKey: kubernetes.io/hostname指的拓扑域是主机。\n● 选择器： 对应于matchExpressions，可以添加多条选择器，多条选择器之间是一种“与”的关系，即需要满足全部选择器才能依据此条规则进行调度。\n● 标签名： 对应应用组件的标签，您可以使用默认标签app或者使用自定义标签。\n● 匹配关系： 即操作符，可以设置四种匹配关系（In, NotIn, Exists, DoesNotExist）。 In和NotIn操作符可以添加单个值或者多个value值（多值使用；进行划分）， Exists和DoesNotExist判断某个label是否存在，不需设置value值。\n2、方案 2.1、前提 确保 K8S 纳管节点分布在不同的可用区（至少 2 个可用区）。比如：我们需要4 台节点，则确保2台在 AZ1、另外2台在 AZ2。\n节点纳管后可查看其标签信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ➜ kubectl describe node Name: 10.x.154.106 Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=Si3.xlarge.2 beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=cn-southwest-1 failure-domain.beta.kubernetes.io/zone=cn-southwest-1a kubernetes.io/arch=amd64 kubernetes.io/hostname=10.x.154.106 kubernetes.io/os=linux node.kubernetes.io/baremetal=false node.kubernetes.io/container-engine=docker node.kubernetes.io/instance-type=Si3.xlarge.2 node.kubernetes.io/subnetid=0b809883-9c99-4b67-a6ca-5c1a948a5def os.architecture=amd64 os.name=EulerOS_2.0_SP9x86_64 os.version=4.18.0-147.5.1.6.h541.eulerosv2r9.x86_64 topology.kubernetes.io/region=cn-southwest-1 topology.kubernetes.io/zone=cn-southwest-1a 其中：\n节点所在区域： Key：failure-domain.beta.kubernetes.io/region Value：cn-southwest-1 节点所在可用区： Key：failure-domain.beta.kubernetes.io/zone Value：cn-southwest-1a，a表示az1，b表示az2，c表示az3 节点的主机名： Key：kubernetes.io/hostname Value：10.x.154.106 2.2、反亲和性设置 创建一个无状态组件，设置反亲和性，这里均采用软约束尽量满足。\n创建成功后可见7个实例能分在两个AZ的4个节点上。\n尝试将节点之间的调度用硬约束，AZ之间的约束用软约束，通过定义yaml来实现，定义一个pod-antiaffinity.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod-affinity-test spec: selector: matchLabels: app: nginx-pod-affinity-test replicas: 5 template: metadata: labels: app: nginx-pod-affinity-test spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - nginx-pod-affinity-test topologyKey: failure-domain.beta.kubernetes.io/zone weight: 50 requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-pod-affinity-test topologyKey: kubernetes.io/hostname containers: - name: nginx-pod-affinity-test image: xxxx.xxx.xxx.com:80/public/nginx:x86-1.20.1 imagePullPolicy: IfNotPresent 这里的实例个数为5，实际上集群的计算节点只有4个，AZ只有两个，apply观察一下结果：\n1 2 3 ➜ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-pod-affinity-test 4/5 5 4 33m 可见期望5个实例只起来了4个，查看一下POD实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ➜ kubectl get po -l app=nginx-pod-affinity-test NAME READY STATUS RESTARTS AGE nginx-pod-affinity-test-688d8f5fb5-44h4h 1/1 Running 0 35m nginx-pod-affinity-test-688d8f5fb5-9sddx 0/1 Pending 0 35m nginx-pod-affinity-test-688d8f5fb5-f2qxn 1/1 Running 0 35m nginx-pod-affinity-test-688d8f5fb5-nbzft 1/1 Running 0 35m nginx-pod-affinity-test-688d8f5fb5-zfdqb 1/1 Running 0 35m ➜ kubectl describe po nginx-pod-affinity-test-688d8f5fb5-9sddx Name: nginx-pod-affinity-test-688d8f5fb5-9sddx Namespace: default Priority: 0 Node: \u0026lt;none\u0026gt; Labels: app=nginx-pod-affinity-test pod-template-hash=688d8f5fb5 Annotations: kubernetes.io/psp: psp-global Status: Pending IP: IPs: \u0026lt;none\u0026gt; Controlled By: ReplicaSet/nginx-pod-affinity-test-688d8f5fb5 Containers: nginx-pod-affinity-test: Image: xxxx.xxx.xxx.com:80/public/nginx:x86-1.20.1 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Environment: VMPORT: [] VMIP: (v1:status.hostIP) Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-47g7m (ro) Conditions: Type Status PodScheduled False Volumes: default-token-47g7m: Type: Secret (a volume populated by a Secret) SecretName: default-token-47g7m Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 36m default-scheduler 0/4 nodes are available: 4 node(s) didn\u0026#39;t match pod affinity/anti-affinity, 4 node(s) didn\u0026#39;t match pod anti-affinity rules. Warning FailedScheduling 36m default-scheduler 0/4 nodes are available: 4 node(s) didn\u0026#39;t match pod affinity/anti-affinity, 4 node(s) didn\u0026#39;t match pod anti-affinity rules. Normal NotTriggerScaleUp 2s (x217 over 36m) cluster-autoscaler pod didn\u0026#39;t trigger scale-up (it wouldn\u0026#39;t fit if a new node is added): 提示第5个POD创建不出来，因为上面的策略定义的是POD硬约束在每个节点上分开，期望5个POD就需要5个节点，然而我们只有4个节点，所以第五个是创建不出来的。修改下策略为软约束：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod-affinity-test spec: selector: matchLabels: app: nginx-pod-affinity-test replicas: 5 template: metadata: labels: app: nginx-pod-affinity-test spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - nginx-pod-affinity-test topologyKey: failure-domain.beta.kubernetes.io/zone weight: 50 - podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - nginx-pod-affinity-test topologyKey: kubernetes.io/hostname weight: 50 containers: - name: nginx-pod-affinity-test image: xxxx.xxx.xxx.com:80/public/nginx:x86-1.20.1 imagePullPolicy: IfNotPresent 部署并查看POD实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ➜ kubectl apply -f pod-affinity/pod-affinity.yaml deployment.apps/nginx-pod-affinity-test created ➜ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-pod-affinity-test 5/5 5 5 5s ➜ kubectl get po -l app=nginx-pod-affinity-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod-affinity-test-8545c897c6-5ckjt 1/1 Running 0 93s 12.11.0.78 10.x.154.106 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-6svzj 1/1 Running 0 93s 12.11.0.122 10.x.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-hz2dp 1/1 Running 0 93s 12.11.0.57 10.x.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-ltc8w 1/1 Running 0 93s 12.11.0.104 10.x.154.147 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-q2m9h 1/1 Running 0 93s 12.11.0.58 10.x.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5个实例分在两个AZ的5个节点上了。修改实例个数为12个看看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ➜ kubectl edit deploy nginx-pod-affinity-test deployment.apps/nginx-pod-affinity-test edited ➜ kubectl get po -l app=nginx-pod-affinity-test -o wide kubectl get po -l app=nginx-pod-affinity-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod-affinity-test-8545c897c6-5ckjt 1/1 Running 0 6m26s 12.11.0.78 10.x.154.106 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-6svzj 1/1 Running 0 6m26s 12.11.0.122 10.x.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-bgt4c 1/1 Running 0 50s 12.11.0.84 10.x.154.106 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-f7hbt 1/1 Running 0 50s 12.11.0.128 10.x.154.147 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-hz2dp 1/1 Running 0 6m26s 12.11.0.57 10.x.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-j4tlk 1/1 Running 0 50s 12.11.0.61 10.x.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-l7fvd 1/1 Running 0 50s 12.11.0.125 10.x.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-ltc8w 1/1 Running 0 6m26s 12.11.0.104 10.x.154.147 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-t62h7 1/1 Running 0 50s 12.11.0.127 10.x.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-w4rws 1/1 Running 0 50s 12.11.0.126 10.x.154.161 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-wbgvc 1/1 Running 0 50s 12.11.0.60 10.x.154.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-pod-affinity-test-8545c897c6-wtvvx 1/1 Running 0 50s 12.11.0.102 10.x.154.147 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 四个节点上的POD分布如下：\nAZ 节点IP POD个数 AZ1 10.x.154.147 3 AZ1 10.x.154.106 2 AZ2 10.x.154.161 4 AZ2 10.x.154.39 3 全文完。\n","description":"基于云架构平台，基本都是多AZ保障高可用，那么如何设置POD在多AZ之间调度呢？","id":10,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"06-POD在多AZ之间高可用的实现","uri":"https://laomeinote.com/posts/how-to-make-pod-ha-in-multipule-az/"},{"content":"RBAC：即role-based access control，基于角色的访问控制。RBAC 鉴权机制使用 rbac.authorization.k8s.io API 组 来驱动鉴权，通过 Kubernetes API 动态配置策略。\nRBAC的几个关键概念 Kubernetes 所有资源对象都是模型化的 API 对象，允许执行 CRUD(Create、Read、Update、Delete) 操作，比如下面的这些资源：\nPods、ConfigMaps、Deployments、Nodes、Secrets、Namespaces 对于上面这些资源对象的可能存在的操作有：\ncreate、get、delete、list、update、edit、watch、exec、patch 在更上层，这些资源和 API Group 进行关联，比如 Pods 属于 Core API Group，而 Deployements 属于 apps API Group。\n除了这些资源和操作以外，还有几个关键概念：\nRule：规则，规则是一组属于不同 API Group 资源上的一组操作的集合 Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的 Rules 元素，二者的区别在于，在 Role 中，定义的规则只适用于单个命名空间，也就是和 namespace 关联的，而 ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外 Role 和 ClusterRole 在Kubernetes 中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、Deployment 这些对象类似，都是我们集群的资源对象，所以同样的可以使用 YAML 文件来描述，用 kubectl 工具来管理 Subject：主题，对应集群中尝试操作的对象，集群中定义了3种类型的主题资源： User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone 或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如 cluster-admin Service Account：服务帐号，通过 Kubernetes API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点 RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程（给某个用户绑定上操作的权限），二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。 RBAC就是分别定义好subject API对象、角色API对象、角色绑定API对象，然后再将subject与角色通过binding进行绑定。\nDemo1：只能访问某个 namespace 的普通用户 目标：创建一个 User Account，只能访问 kube-system 这个命名空间，对应的用户信息如下所示：\n1 2 username: laomei group: laomeigroup 1. 创建用户凭证 Kubernetes 没有 User Account 的 API 对象，不过利用管理员分配给的私钥就可以创建一个用户账号，可以参考官方文档中的方法，这里我们来使用 OpenSSL 证书来创建一个 User（或者也可以使用更简单的 cfssl工具来创建）。\n使用openssl给用户 laomei 创建一个私钥，命名成 laomei.key：\n1 2 3 4 5 ➜ openssl genrsa -out laomei.key 2048 Generating RSA private key, 2048 bit long modulus ...................+++ .........................+++ e is 65537 (0x10001) 使用刚刚创建的私钥创建一个证书签名请求文件：laomei.csr，要注意需要确保在-subj参数中指定用户名和组(CN表示用户名，O表示组)：\n1 ➜ openssl req -new -key laomei.key -out laomei.csr -subj \u0026#34;/CN=laomei /O=laomeigroup\u0026#34; 然后找到 Kubernetes 集群的 CA 证书，如果使用kubeadm 安装的集群，CA 相关证书位于 /etc/kubernetes/pki/ 目录下面，如果是二进制方式搭建的，在最开始搭建集群的时候就已经指定好了 CA 的目录。利用该目录下面的 ca.crt 和 ca.key两个文件来批准上面的证书请求，生成最终的证书文件laomei.crt，我们这里设置证书的有效期为 500 天：\n1 2 3 4 ➜ openssl x509 -req -in laomei.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out laomei.crt -days 500 Signature ok subject=/CN=laomei /O=laomeigroup Getting CA Private Key 查看当前文件夹下面是否生成了一个证书文件：\n1 2 3 4 ➜ ll -rw-r--r-- 1 root root 1001 Feb 8 16:25 laomei.crt -rw-r--r-- 1 root root 915 Feb 8 16:21 laomei.csr -rw-r--r-- 1 root root 1675 Feb 8 16:14 laomei.key 然后可以使用刚刚创建的证书文件和私钥文件在集群中创建新的凭证和上下文(Context):\n1 2 ➜ kubectl config set-credentials laomei --client-certificate=laomei.crt --client-key=laomei.key User \u0026#34;laomei\u0026#34; set. 这里可以看到一个用户 laomei 创建了，然后为这个用户设置新的 Context，这里指定特定的一个 namespace：\n1 2 ➜ kubectl config set-context laomei-context --cluster=kubernetes --namespace=kube-system --user=laomei Context \u0026#34;laomei-context\u0026#34; created. 到这里，用户 laomei 就已经创建成功了，现在使用当前的这个配置文件来操作 kubectl 命令的时候，应该会出现错误，因为还没有为该用户定义任何操作的权限：\n1 2 ➜ kubectl get pods --context=laomei-context Error from server (Forbidden): pods is forbidden: User \u0026#34;laomei \u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;kube-system\u0026#34; 2. 创建角色Role 用户创建完成后，接下来就需要给该用户添加操作权限，可定义一个 YAML 文件，创建一个允许用户操作 Deployment、Pod、ReplicaSets 的角色，如下定义：(laomei-role.yaml).\n1 2 3 4 5 6 7 8 9 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: laomei-role namespace: kube-system rules: - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] # 也可以使用[\u0026#39;*\u0026#39;] 其中apiGroups包含两个，一个core API Group（在YAML中用空字符可表示核心组），POD属于core API组，Deployment 和 ReplicaSet 都属于 apps 这个 API Group，所以 rules 下面的 apiGroups 就综合了这几个资源的 API Group：[\u0026quot;\u0026quot;, \u0026ldquo;apps\u0026rdquo;]。\nAPI资源对象包括我们希望限制的几个deployments,replicasets和pods。其中verbs 就是上面提到的可以对这些资源对象执行的操作，我们这里需要所有的操作方法，所以我们也可以使用[\u0026rsquo;*\u0026rsquo;]来代替。然后直接创建这个 Role：\n1 2 ➜ kubectl apply -f laomei-role.yaml role.rbac.authorization.k8s.io/laomei-role created 3. 用户与角色绑定Rolebinding 上面创建了用户laomei与角色laomei-role，但是二者并没有关联起来，使用laomei还是无法操作API资源对象，所以这里就需要创建一个角色绑定Rolebinding资源对象，将user与role绑定起来。例：在kube-system这个namespace下将上面的laomei与laomei-role进行绑定（laomei-rolebinding.yaml）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: laomei-rolebinding namespace: kube-system subjects: - kind: User name: laomei apiGroup: \u0026#34;\u0026#34; roleRef: kind: Role name: laomei-role apiGroup: rbac.authorization.k8s.io 其中subjects 字段就是User资源对象，这里对应上面的 User 帐号 laomei，roleRef字段将上面的subject与角色Role或者clusterrole进行绑定。使用kubectl 创建上面的资源对象：\n1 2 ➜ kubectl apply -f laomei-rolebinding.yaml rolebinding.rbac.authorization.k8s.io/laomei-rolebinding created 4. 测试 使用上下文laomei-context操作集群。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ➜ kubectl get pods --context=laomei-context NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-4dxst 1/1 Running 11 58d coredns-f9fd979d6-59d2f 1/1 Running 11 58d etcd-master 1/1 Running 11 58d kube-apiserver-master 1/1 Running 13 58d kube-controller-manager-master 1/1 Running 11 58d kube-flannel-ds-kcsd6 1/1 Running 9 58d kube-flannel-ds-s7r2g 1/1 Running 14 58d kube-flannel-ds-tdjr8 1/1 Running 6 58d kube-proxy-nllct 1/1 Running 7 58d kube-proxy-qx2kp 1/1 Running 4 58d kube-proxy-xjsnk 1/1 Running 11 58d kube-scheduler-master 1/1 Running 13 58d metrics-server-5458746495-pfv4d 1/1 Running 0 12d 这里使用kubectl 并没有指定 namespace，这是因为上面创建Context 的时候就绑定在了 kube-system 这个命名空间下面。\n1 ➜ kubectl get pods --context=laomei-context 等同于下面的命令：\n1 ➜ kubectl get pods --context=laomei-context -nkube-system 可以测试一下查询default命名空间。\n1 2 ➜ kubectl get pods --context=laomei-context -ndefault Error from server (Forbidden): pods is forbidden: User \u0026#34;laomei\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;default\u0026#34; 测试验证下service服务是否可以查询。\n1 2 ➜ kubectl get svc --context=laomei-context -nkube-system Error from server (Forbidden): services is forbidden: User \u0026#34;laomei\u0026#34; cannot list resource \u0026#34;services\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;kube-system\u0026#34; Demo2：只能访问某个 namespace 的 ServiceAccount 上面的demo1创建了一个只能访问某个命名空间下面的普通用户，前面也提到 subjects 下面还有一种类型的主题资源：ServiceAccount。现在通过demo2演示创建一个集群内部的用户只能操作 kube-system 这个命名空间下面的 pods 和 deployments。\n1. 创建SA对象 首先来创建一个 ServiceAccount 对象：\n1 ➜ kubectl create sa laomei-sa -n kube-system 当然也可以定义成 YAML 文件的形式来创建：\n1 2 3 4 5 apiVersion: v1 kind: ServiceAccount metadata: name: laomei-sa namespace: kube-system 2. 创建Role对象 然后新建一个 Role 对象：(laomei-sa-role.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: laomei-sa-role namespace: kube-system rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] 这里定义的角色操作Pod 的权限只有get,watch,list权限，没有create、delete、update ，稍后可重点测试。\n创建该 Role 对象：\n1 2 ➜ kubectl apply -f laomei-sa-role.yaml role.rbac.authorization.k8s.io/laomei-sa-role created 3. 用户与角色绑定Rolebinding 然后创建一个 RoleBinding 对象，将上面的 laomei-sa 和角色 laomei-sa-role 进行绑定：(laomei-sa-rolebinding.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: laomei-sa-rolebinding namespace: kube-system subjects: - kind: ServiceAccount name: laomei-sa namespace: kube-system roleRef: kind: Role name: laomei-sa-role apiGroup: rbac.authorization.k8s.io 添加这个资源对象：\n1 2 ➜ kubectl apply -f laomei-sa-rolebinding.yaml rolebinding.rbac.authorization.k8s.io/laomei-sa-rolebinding created 这里就将laomei-sa这个subject与laomei-sa-role完成了绑定，laomei-sa这个账号对Pod 有get,watch,list权限，对deployments有get, list, watch, create, update, patch, delete权限。\n4. 验证 这个ServiceAccount需要怎么验证呢？其实ServiceAccount 会生成一个 Secret 对象和它进行映射，这个 Secret 里面包含一个 token，使用该token登陆Dashboard，登陆到Dashboard中可验证权限是否符合预期。\n1 2 3 4 ➜ kubectl get secret -n kube-system |grep laomei-sa laomei-sa-token-695h2 kubernetes.io/service-account-token 3 2m3s ➜ kubectl get secret laomei-sa-token-695h2 -o jsonpath={.data.token} -n kube-system |base64 -d # 会生成一串很长的base64后的字符串 使用这里的 token 去 Dashboard 页面进行登录：\n可以看到上面的提示现在使用的这个 ServiceAccount 没有权限获取当前命名空间下面的资源对象，这是因为登录进来后默认跳转到 default 命名空间，如果切换到 kube-system 命名空间下面就可以了：\n这里看到可以访问 pod 列表了，但也会有一些其他提示：\nevents is forbidden: User \u0026quot;system:serviceaccount:kube-system:laomei-sa\u0026quot; cannot list events in the namespace \u0026quot;kube-system\u0026quot;，这是因为当前登录用只被授权了访问 pod 和 deployment 的权限。\n全文完。\n","description":"Kubernetes 所有资源对象都是模型化的 API 对象，允许执行CRUD(Create、Read、Update、Delete)操作，通过RBAC可以设置基于角色控制CRUD操作。","id":11,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"05-通过RBAC授权API对象的操作","uri":"https://laomeinote.com/posts/how-to-authority-users-with-rbac-in-k8s/"},{"content":"1. POD的生命周期 一个 Pod 的完整生命周期过程包含 Init Container、Pod Hook、健康检查 三个主要部分，如下图所示。\n其中Init Container初始化容器是用来做初始化的，比如提前给main container主容器准备卷、文件、检测环境等等。main容器只有等初始化容器运行结束后才会启动。\nPOD Hook是main容器的钩子，有post start和pre stop钩子，post start即是在main容器启动之后马上开始的动作；pre stop是在main容器结束之前的动作。一般事项零宕机就是通过设置pre stop钩子，在POD结束之前使容器优雅退出。 设置pre stop后，删除POD时，POD将会发SIGTERM 信号给POD中各个容器通知准备退出了，同时Kubernetes 将从 Endpoints 对象中删除该 Pod，所以该 Pod 将会从负载均衡器中排除，负载均衡器便不再转发流量到这个POD上，理论上可实现零宕机（应用退出快的情况下）。一般优雅停机方法：\n1 2 3 4 lifecycle: preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;，\u0026#34;-c\u0026#34;，\u0026#34;/pre-stop.sh\u0026#34;] 在/pre-stop.sh 脚本里添加应用的清理逻辑。\n串一串POD退出的流程：\n一、用户删除POD\n二、Pod 进入 Terminating 状态; 与此同时，k8s 会将 Pod 从对应的 service 上摘除;\n三、与此同时，针对有 pre stop hook 的容器，kubelet 会调用每个容器的 preStop hook，假如 preStop hook 的运行时间超出了 grace period（默认30s），kubelet 会发送 SIGTERM 并再等 2 秒;\n四、与此同时，针对没有 preStop hook 的容器，kubelet 发送 SIGTERM。\n五、grace period 超出之后，kubelet 发送 SIGKILL 干掉尚未退出的容器。\n总结：如果在POD停止前执行一条命令，通知网关或者注册中心对这个POD进行下线，那么注册中心就会标记POD已经下线，对其不进行流量转发，用户就不会有任何影响，这就是优雅停止，将滚动更新影响最小化。\n详细参考：https://imroc.cc/k8s/best-practice/graceful-shutdown/\n健康检查主要是使用了三个探针： liveness probe 和readiness probe。\n一、kubelet 通过使用 liveness probe 来确定应用程序是否正在运行，即是否还活着。如果程序一旦崩溃了，通过 liveness probe 探针， Kubernetes 立刻就知道这个程序已经终止了，然后就会重启这个程序。\n二、kubelet通过 使用 readiness probe 来确定容器是否已经就绪可以接收流量过来了，即容器是否准备好了，是否可以开始工作了。只有当 Pod 中的容器都处于就绪状态的时候 kubelet 才会认定该 Pod 处于就绪状态，因为一个 Pod 下面可能会有多个容器。当然 Pod 如果处于非就绪状态，Kubernetes就会将他从 Service 的 Endpoints 列表中移除出来，这样流量就不会被路由到这个 Pod 里面来了。\n2. POD初始化容器demo init-pod.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: Pod metadata: name: first-demo spec: volumes: - name: workdir emptyDir: {} initContainers: - name: install image: busybox command: - \u0026#34;/bin/sh\u0026#34; - \u0026#34;-c\u0026#34; - \u0026#34;echo hello,pod \u0026gt;\u0026gt; /home/message\u0026#34; volumeMounts: - name: workdir mountPath: /home containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html 在yaml中定义了一个全局volumes，可以供下面的两个容器使用，其中volume没有使用hostpath，而是使用的是empyDir，kubelet会在主机上自动创建一个目录，可被挂载到容器目录中，该目录的生命周期等于POD的生命周期。\n在初始化init容器中，拉取busybox镜像，执行一条echo语句打印到/home目录下的message文件中。并将主机上kubelet创建的目录挂载到容器的home目录下，volumeMounts的name需要与全局volumes的name一致。\n在主容器声明中，定义了一个容器，拉取nginx镜像，容器暴露端口为80，将kubelet创建的emptyDir的名为workdir的全局目录挂载到容器的**/usr/share/nginx/html**目录下。即是说在初始化容器中生成的home目录与主容器目录均都mount了主机的emptyDir目录。主容器只有在初始容器起来后才会被拉起。所以很多初始化动作可在这个初始化容器中来预置。\nApply测试一下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 ➜ kubectl apply -f init-pod.yaml pod/first-demo created ➜ kubectl describe pod first-demo Name: first-demo Namespace: default Priority: 0 Node: master/192.168.0.114 Start Time: Thu, 13 Jan 2022 08:29:25 +0800 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.244.0.22 IPs: IP: 10.244.0.22 Init Containers: install: Container ID: docker://a65b77a94ba8d822052f72fc8e673dbe1026b93a202b855d0dc1eb1019c4170d Image: busybox Image ID: docker-pullable://busybox@sha256:5acba83a746c7608ed544dc1533b87c737a0b0fb730301639a0179f9344b1678 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh -c echo hello,pod \u0026gt;\u0026gt; /home/message State: Terminated Reason: Completed Exit Code: 0 Started: Thu, 13 Jan 2022 08:29:50 +0800 Finished: Thu, 13 Jan 2022 08:29:50 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /home from workdir (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-g4ktr (ro) Containers: nginx: Container ID: docker://47a20ab3bcbaedf472e063c2ebbfd3e8efef929fbb9f83f4ea6698a1eda2f318 Image: nginx Image ID: docker-pullable://nginx@sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31 Port: 80/TCP Host Port: 0/TCP State: Running Started: Thu, 13 Jan 2022 08:30:15 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /usr/share/nginx/html from workdir (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-g4ktr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: workdir: Type: EmptyDir (a temporary directory that shares a pod\u0026#39;s lifetime) Medium: SizeLimit: \u0026lt;unset\u0026gt; default-token-g4ktr: Type: Secret (a volume populated by a Secret) SecretName: default-token-g4ktr Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m9s default-scheduler Successfully assigned default/first-demo to master Normal Pulling 4m8s kubelet Pulling image \u0026#34;busybox\u0026#34; Normal Pulled 3m44s kubelet Successfully pulled image \u0026#34;busybox\u0026#34; in 24.295845225s Normal Created 3m44s kubelet Created container install Normal Started 3m44s kubelet Started container install Normal Pulling 3m43s kubelet Pulling image \u0026#34;nginx\u0026#34; Normal Pulled 3m19s kubelet Successfully pulled image \u0026#34;nginx\u0026#34; in 24.107119504s Normal Created 3m19s kubelet Created container nginx Normal Started 3m19s kubelet Started container nginx 在部署pod之后，先拉取busybox镜像生成初始容器，初始容器的状态为Terminated后才会启动主容器，开始拉取nginx镜像启动主容器。\n3. POD优雅停机demo gracestop.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: v1 kind: Pod metadata: name: hook-demo2 spec: containers: - name: hook-demo2 image: nginx lifecycle: preStop: exec: command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;] # 优雅退出 --- apiVersion: v1 kind: Pod metadata: name: hook-demo3 spec: volumes: - name: message hostPath: path: /tmp containers: - name: hook-demo2 image: nginx ports: - containerPort: 80 volumeMounts: - name: message mountPath: /usr/share/ lifecycle: preStop: exec: command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo Hello from the preStop Handler \u0026gt; /usr/share/message\u0026#39;] 这里定义了两个POD，hook-demo2和hook-demo3，其中hook-demo2是利用 preStop 来进行优雅删除，另外一个是利用 preStop 来做一些信息记录。\n1 2 3 4 5 6 ➜ kubectl apply -f gracestop.yaml pod/hook-demo2 created pod/hook-demo3 created ➜ kubectl get pods |grep hook hook-demo2 1/1 Running 0 2m14s hook-demo3 1/1 Running 0 2m14s 创建完成后，直接删除 hook-demo2 这个 Pod，在容器删除之前会执行 preStop 里面的优雅关闭命令。\n第二个 Pod 声明了一个 hostPath 类型的 Volume，在容器里面声明挂载到了这个 Volume，所以当删除 Pod时，在退出容器之前，在容器里面输出的信息也会同样的保存到宿主机的 /tmp 目录下面，我们可以查看 hook-demo3 这个 Pod 被调度的节点：\n➜ kubectl describe pod hook-demo3\rName: hook-demo3\rNamespace: default\rPriority: 0\rNode: master/192.168.0.114\r...... 可以看到这个 Pod 被调度到了 master 这个节点上，可以先到该节点上查看 /tmp 目录没有message输出：\n➜ ls /tmp/ 现在删除 hook-demo3 这个 Pod，容器退出之前会执行 preStop 里面的命令，也就是会往 message 文件中输出一些信息：\n➜ kubectl delete pod hook-demo3\rpod \u0026#34;hook-demo3\u0026#34; deleted\r➜ ls /tmp/\rmessage\r➜ cat /tmp/message\rHello from the preStop Handler 4. POD探针demo demo: 用 exec 执行命令的方式来检测容器的存活，如下liveness-exec.yaml。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 这个例子中，容器启动时创建文件/tmp/healthy，然后休眠30秒，30秒后删除文件，修改10分钟。探针livenessProbe会执行exec命令cat /tmp/healthy，执行成功会返回值成功码0，如果返回值为非0，kubelet就会kill掉容器并重启它。\nperiodSeconds：表示让 kubelet 每隔5秒执行一次存活探针，也就是每5秒执行一次上面的cat /tmp/healthy命令。默认是10秒，最小1秒。 initialDelaySeconds：表示在第一次执行探针的时候要等待5秒，这样能够确保容器能够有足够的时间启动起来。如果第一次执行探针等待时间段，可能容器都还没起来，存活探针始终都是失败的，容器就会一直重启下去。 测试一下：\n我们来创建下该 Pod，然后在 30 秒内，查看 Pod 的 Event：\n1 2 3 4 5 6 7 8 9 10 11 ➜ kubectl apply -f liveness-exec.yaml ➜ kubectl describe pod liveness-exec ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 51s default-scheduler Successfully assigned default/liveness-exec to node2 Normal Pulling 51s kubelet Pulling image \u0026#34;busybox\u0026#34; Normal Pulled 35s kubelet Successfully pulled image \u0026#34;busybox\u0026#34; in 16.548421804s Normal Created 34s kubelet Created container liveness Normal Started 34s kubelet Started container liveness 可以观察到容器一开始是正常启动的，在隔一会儿，比如 40s 后，再查看下 Pod 的 Event，在最下面有一条信息显示 liveness probe 失败了，容器将要重启。然后可以查看到 Pod 的 RESTARTS 值加 1 了：\n➜ kubectl describe pod liveness-exec\r---- ------ ---- ---- -------\rNormal Scheduled 2m9s default-scheduler Successfully assigned default/liveness-exec to node2\rNormal Pulled 113s kubelet Successfully pulled image \u0026#34;busybox\u0026#34; in 16.548421804s\rWarning Unhealthy 68s (x3 over 78s) kubelet Liveness probe failed: cat: can\u0026#39;t open \u0026#39;/tmp/healthy\u0026#39;: No such file or directory\rNormal Killing 68s kubelet Container liveness failed liveness probe, will be restarted\rNormal Pulling 38s (x2 over 2m9s) kubelet Pulling image \u0026#34;busybox\u0026#34;\rNormal Created 22s (x2 over 112s) kubelet Created container liveness\rNormal Started 22s (x2 over 112s) kubelet Started container liveness\rNormal Pulled 22s kubelet Successfully pulled image \u0026#34;busybox\u0026#34; in 16.089227559s\r➜ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rliveness-exec 1/1 Running 1 1m 全文完。\n","description":"本文介绍POD的生命周期与使用滚动更新避免应用宕机。","id":12,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"04-POD初始化与滚动更新","uri":"https://laomeinote.com/posts/how-to-initiate-and-update-pod/"},{"content":"本文介绍如何使用 Secret 从私有镜像仓库或代码仓库拉取镜像来创建 Pod。\n准备工作 Kubernetes集群 私有仓库 可登陆私有仓库的账号 登录Docker私有仓库 要想拉取私有镜像必须在镜像仓库上进行身份验证。\n1 docker login -u \u0026lt;username\u0026gt; \u0026lt;repo-url\u0026gt; 当出现提示时，输入账号密码。\n登录过程会创建或更新保存有授权令牌的 config.json 文件。 查看 Kubernetes 中如何解析这个文件。\n查看 config.json 文件：\n1 cat ~/.docker/config.json 输出结果包含类似于以下内容的部分：\n1 2 3 4 5 6 7 { \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;c3R...zE2\u0026#34; } } } 说明： 如果使用 Docker 凭证仓库，则不会看到 auth 条目，看到的将是以仓库名称作为值的 credsStore 条目。\n创建Secret 在集群中创建保存授权令牌的 Secret\nKubernetes 集群使用 docker-registry 类型的 Secret 来通过容器仓库的身份验证，进而提取私有映像。\n创建 Secret，命名为 myregcred：\n1 2 3 4 5 kubectl create secret docker-registry myregcred \\ --docker-server=\u0026lt;你的镜像仓库服务器\u0026gt; \\ --docker-username=\u0026lt;你的用户名\u0026gt; \\ --docker-password=\u0026lt;你的密码\u0026gt; \\ --docker-email=\u0026lt;你的邮箱地址\u0026gt; 参数说明：\n\u0026lt;your-registry-server\u0026gt; 是私有 Docker 仓库全限定域名（FQDN）。 DockerHub 使用 https://index.docker.io/v1/。 \u0026lt;your-name\u0026gt; 是 Docker 用户名，注意使用单引号引用。 \u0026lt;your-pword\u0026gt; 是 Docker 密码，注意使用单引号引用。 \u0026lt;your-email\u0026gt; 是Docker 邮箱，注意使用单引号引用。 这样就成功地将集群中的 Docker 凭证设置为名为 regcred 的 Secret。\n检查 Secret myregcred 要了解创建的 myregcred Secret 的内容，可以用 YAML 格式进行查看：\n1 kubectl get secret myregcred --output=yaml 输出和下面类似：\n1 2 3 4 5 6 7 8 9 apiVersion: v1 data: .dockerconfigjson: eyJodHRwczovL2luZGV4L ... J0QUl6RTIifX0= kind: Secret metadata: ... name: myregcred ... type: kubernetes.io/dockerconfigjson .dockerconfigjson 字段的值是 Docker 凭证的 base64 表示。\n要了解 dockerconfigjson 字段中的内容，请将 Secret 数据转换为可读格式：\n1 kubectl get secret myregcred --output=\u0026#34;jsonpath={.data.\\.dockerconfigjson}\u0026#34; | base64 --decode 输出和下面类似：\n1 {\u0026#34;auths\u0026#34;:{\u0026#34;yourprivateregistry.com\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;janedoe\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;xxxxxxxxxxx\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;jdoe@example.com\u0026#34;,\u0026#34;auth\u0026#34;:\u0026#34;c3R...zE2\u0026#34;}}} 要了解 auth 字段中的内容，请将 base64 编码过的数据转换为可读格式：\n1 echo \u0026#34;c3R...zE2\u0026#34; | base64 --decode 输出结果中，用户名和密码用 : 链接，类似下面这样：\njanedoe:xxxxxxxxxxx 注意，Secret 数据包含与本地 ~/.docker/config.json 文件类似的授权令牌。\n这样你就已经成功地将 Docker 凭证设置为集群中的名为 myregcred 的 Secret。\n创建一个使用Secret 的 Pod 下面是一个 Pod 配置清单示例，该示例中 Pod 需要访问你的 Docker 凭证 regcred：\npods/private-reg-pod.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: private-reg spec: containers: - name: private-reg-container image: \u0026lt;your-private-image\u0026gt; imagePullSecrets: - name: myregcred 在my-private-reg-pod.yaml 文件中，使用私有仓库的镜像路径替换 \u0026lt;your-private-image\u0026gt;，例如：\njanedoe/jdoe-private:v1 要从私有仓库拉取镜像，Kubernetes 需要凭证。 配置文件中的 imagePullSecrets 字段表明 Kubernetes 应该通过名为 myregcred 的 Secret 获取凭证。\n创建使用了 Secret 的 Pod，并检查它是否正常运行：\n1 2 kubectl apply -f my-private-reg-pod.yaml kubectl get pod private-reg 全文完。\n","description":"本文介绍如何使用 Secret从私有镜像仓库或代码仓库拉取镜像来创建负载。","id":13,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"03-K8s从私有仓库中拉取镜像","uri":"https://laomeinote.com/posts/pull-images-from-private-repository/"},{"content":"通过YAML基于kubectl使用原生方式跟API-Server快速交互，创建对应的API对象。\nYAML基本语法 YAML的基本语法规则如下：\n大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 # 表示注释，从这个字符一直到行尾，都会被解析器忽略 在 Kubernetes 中，首先了解两种结构类型就行了：\nLists（列表） Maps（字典） Maps Map 就是字典，即是一个 key:value 的键值对，Maps 可以让我们更加方便的去书写配置信息，例如：\n1 2 3 --- apiVersion: v1 kind: Pod 其中第一行的---是分隔符，是可选的，在单一文件中，可用连续三个连字号---区分多个文件。这里我们可以看到，我们有两个键：kind 和 apiVersion，他们对应的值分别是：v1 和 Pod。\n再创建一个相对复杂一点的 YAML 文件，创建一个 KEY 对应的值不是字符串而是一个 Maps：\n1 2 3 4 5 6 7 --- apiVersion: v1 kind: Pod metadata: name: test-pod labels: app: web 上面的 YAML 文件，metadata 这个 KEY 对应的值就是一个 Maps ，它嵌套的 labels 这个 KEY 的值又是一个 Map，可根据情况进行多层嵌套。\n注意：在 YAML 文件中绝对不要使用 tab 键来进行缩进，而且缩进前后保持一致的规范，比如前面的name缩进两个空格，后面的app就不要缩进四个空格，最好都保持两个空格作为规范。\nLists Lists就是列表，也是数组，跟Python、Markdown中定义列表的方式是一样的，如下：\n1 2 3 4 args - Cat - Dog - Fish 当然，Lists 的子项也可以是 Maps，Maps 的子项也可以是 Lists 如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 --- apiVersion: v1 kind: Pod metadata: name: test labels: app: web spec: containers: - name: front-end image: nginx ports: - containerPort: 80 - name: back-end image: flaskapp ports: - containerPort: 5000 比如这个 YAML 文件，定义了一个叫 containers 的 List 对象，其中有两个列表子项，每个子项都由name、image、ports 组成，每个 ports 都有一个 key 为 containerPort 的 Map 组成。\n部署Nginx Deployment 基本使用 定义Nginx的YAML描述：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 # API版本 kind: Deployment # API对象类型 metadata: name: nginx-deploy labels: chapter: first-app spec: selector: matchLabels: #spec.selector.matchLabels的键值对与spec.template.metadata.labels的键值对需一致,用于选择pod app: nginx replicas: 3 # Pod 副本数量 template: # Pod 模板 metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:xxxx.xxx.xxxx.com:80/public/nginx:x86-1.20.1 ports: - containerPort: 80 #容器监听的端口80 定义的YAML中有三个重要内容：\nreplias：表示期望的 Pod 的副本数量，POD实际上是通过控制器ReplicaSet来控制的 selector：matchLabels，用来匹配要控制的 Pod 标签，需要和下面的 Pod 模板中的标签一致 template：Pod 模板，Pod的详细定义，相当于把一个 Pod 的描述以模板的形式嵌入到了 ReplicaSet 中来。 上面提到的POD、ReplicaSet、Deployment是什么关系呢？可以用下图来说明：\n如上图所示，Deployment控制Replicaset，Replicaset控制POD，三者是层层控制的，Deployment 通过管理 ReplicaSet 的数量和属性来实现水平扩展/收缩以及滚动更新两个功能的；ReplicaSet 通过replicas: 3来保证 Pod 的个数始终保存为3。\n使用Kubectl部署Deployment：\n1 2 # kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deploy created 查询部署结果：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE dddddddddd 1/1 1 1 13d hpa-example 1/1 1 1 6d2h nginx-deploy 0/3 3 0 35s saf 1/1 1 1 18d # kubectl get deploy -l app=nginx No resources found in default namespace. # kubectl get deploy -l chapter=first-app NAME READY UP-TO-DATE AVAILABLE AGE nginx-deploy 0/3 3 0 82s 如果不指定标签，则kubectl会将默认的default namespace下的所有负载均查询出来，可以通过标签选择器过滤刚才部署的负载，上面的例子中，故意选择标签-l app=nginx结果什么都没查到，实际上这是因为在上面的YAML定义中指定Deployment的标签键值对为chapter: first-app,POD的标签键值对才是app: nginx。\n注：键值对可自由定义，但是控制器的label选择器和POD模板中的label定义要前后一致。\n上面的例子中，通过Deployment定义了三个POD，通过POD的标签选择器过滤查询：\n1 2 3 4 5 # kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-67df4cc9c-dwrhd 0/1 InvalidImageName 0 6m59s nginx-deploy-67df4cc9c-nwfw4 0/1 InvalidImageName 0 6m59s nginx-deploy-67df4cc9c-v6v9g 0/1 InvalidImageName 0 6m59s 其中STATUS为InvalidImageName，表示POD没有起来，可通过describe命令查询第一个POD nginx-deploy-67df4cc9c-dwrhd的详细信息：\n1 2 3 4 5 6 7 8 9 10 11 12 # kubectl describe pod nginx-deploy-67df4cc9c-dwrhd #...省略 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 8m22s default-scheduler Successfully assigned default/nginx-deploy-67df4cc9c-dwrhd to 10.247.154.106 Normal SuccessfulMountVolume 8m22s kubelet Successfully mounted volumes for pod \u0026#34;nginx-deploy-67df4cc9c-dwrhd_default(9cf7030b- cafc-470c-ba51-ef12d798cb68)\u0026#34; Normal SandboxChanged 8m19s kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreate 6m48s (x11 over 8m19s) kubelet Error: InvalidImageName Warning InspectFailed 3m14s (x28 over 8m19s) kubelet Failed to apply default image tag \u0026#34;nginx:kweecr03.my-privaterepo.com:80/public/nginx:v1\u0026#34;: couldn\u0026#39;t parse image reference \u0026#34;nginx:kweecr03.my-privaterepo.com:80/nginx:x86-1.20.1\u0026#34;: invalid reference format #...省略 从Events中可见其根因为image仓库地址写错，修改一下重新apply部署即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deploy configured # kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-5bbc6bfb99-7rgtd 0/1 Pending 0 0s nginx-deploy-5bbc6bfb99-gwtwx 1/1 Running 0 2s nginx-deploy-5bbc6bfb99-wtx8l 1/1 Running 0 4s nginx-deploy-67df4cc9c-dwrhd 0/1 Terminating 0 12m nginx-deploy-67df4cc9c-nwfw4 0/1 InvalidImageName 0 12m nginx-deploy-67df4cc9c-v6v9g 0/1 Terminating 0 12m # kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-5bbc6bfb99-7rgtd 1/1 Running 0 22s nginx-deploy-5bbc6bfb99-gwtwx 1/1 Running 0 24s nginx-deploy-5bbc6bfb99-wtx8l 1/1 Running 0 26s 重新部署的过程中，ReplicaSet控制器会重新部署三个POD，并将之前状态不正确的POD删除。查看一下ReplicaSet控制器：\n1 2 3 4 # kubectl get rs -l app=nginx NAME DESIRED CURRENT READY AGE nginx-deploy-5bbc6bfb99 3 3 3 4m12s nginx-deploy-67df4cc9c 0 0 0 16m 其中nginx-deploy-67df4cc9c的Age是16分钟，是之前部署出错时Deployment生成的ReplicaSet控制器。nginx-deploy-5bbc6bfb99是目前的控制器，控制3个POD。查看一下这个ReplicaSet详细信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # kubectl describe rs nginx-deploy-5bbc6bfb99 Name: nginx-deploy-5bbc6bfb99 Namespace: default Selector: app=nginx,pod-template-hash=5bbc6bfb99 Labels: app=nginx pod-template-hash=5bbc6bfb99 Annotations: deployment.kubernetes.io/desired-replicas: 3 deployment.kubernetes.io/max-replicas: 4 deployment.kubernetes.io/revision: 2 Controlled By: Deployment/nginx-deploy Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=nginx pod-template-hash=5bbc6bfb99 Containers: nginx: Image: kweecr03.xxxxxxxxxx.com:80/public/nginx:x86-1.20.1 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 6m44s replicaset-controller Created pod: nginx-deploy-5bbc6bfb99-wtx8l Normal SuccessfulCreate 6m42s replicaset-controller Created pod: nginx-deploy-5bbc6bfb99-gwtwx Normal SuccessfulCreate 6m40s replicaset-controller Created pod: nginx-deploy-5bbc6bfb99-7rgtd Events记录了创建三个POD事件。Containers、Labels、Annotations等信息也一并打印出来了。\n水平伸缩 水平伸缩Horizontal POD Autoscaling，即是通过控制器增加/删除POD。常见的几种方案：\n使用命令 kubectl scale 来实现。 修改YAML文件中的对象定义，然后重新执行kubectl apply来实现。 基于k8s插件实现的控制器来获取相应指标，实现自动HPA。 第三种方案稍微比较复杂，本次实践前两种。\n使用kubectl scale进行HPA 1 2 3 4 5 6 7 8 9 10 11 12 # kubectl scale deployment nginx-deploy --replicas=4 deployment.apps/nginx-deploy scaled # kubectl get rs -l app=nginx NAME DESIRED CURRENT READY AGE nginx-deploy-5bbc6bfb99 4 4 4 24m nginx-deploy-67df4cc9c 0 0 0 37m # kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-5bbc6bfb99-7rgtd 1/1 Running 0 24m nginx-deploy-5bbc6bfb99-8cqdv 1/1 Running 0 28s nginx-deploy-5bbc6bfb99-gwtwx 1/1 Running 0 24m nginx-deploy-5bbc6bfb99-wtx8l 1/1 Running 0 24m 其中POD - nginx-deploy-5bbc6bfb99-8cqdv即是ReplicaSet控制器弹出来的。同样，可以指定replicas=2实现POD的缩容。\n1 2 3 4 5 6 7 8 9 10 # kubectl scale deployment nginx-deploy --replicas=2 deployment.apps/nginx-deploy scaled [root@kweekshcgp-2ywd5 nginx_demo]# kubectl get rs -l app=nginx NAME DESIRED CURRENT READY AGE nginx-deploy-5bbc6bfb99 2 2 2 26m nginx-deploy-67df4cc9c 0 0 0 38mk [root@kweekshcgp-2ywd5 nginx_demo]# kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-5bbc6bfb99-gwtwx 1/1 Running 0 26m nginx-deploy-5bbc6bfb99-wtx8l 1/1 Running 0 26m 修改YAML文件进行HPA 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 # API版本 kind: Deployment # API对象类型 metadata: name: nginx-deploy labels: chapter: first-app spec: selector: matchLabels: #spec.selector.matchLabels的键值对与spec.template.metadata.labels的键值对需一致,用于选择pod app: nginx replicas: 5 # Pod 副本数量 template: # Pod 模板 metadata: labels: app: nginx spec: containers: - name: nginx image: xxxx.xxx.xxxx.com:80/public/nginx:x86-1.20.1 ports: - containerPort: 80 #容器监听的端口80 将replcas的值修改为5，然后手动apply。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deploy configured # kubectl describe deploy nginx-deploy #...省略其他log Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 43m deployment-controller Scaled up replica set nginx-deploy-67df4cc9c to 3 Normal ScalingReplicaSet 31m deployment-controller Scaled up replica set nginx-deploy-5bbc6bfb99 to 1 Normal ScalingReplicaSet 31m deployment-controller Scaled down replica set nginx-deploy-67df4cc9c to 2 Normal ScalingReplicaSet 31m deployment-controller Scaled up replica set nginx-deploy-5bbc6bfb99 to 2 Normal ScalingReplicaSet 31m deployment-controller Scaled down replica set nginx-deploy-67df4cc9c to 1 Normal ScalingReplicaSet 31m deployment-controller Scaled up replica set nginx-deploy-5bbc6bfb99 to 3 Normal ScalingReplicaSet 31m deployment-controller Scaled down replica set nginx-deploy-67df4cc9c to 0 Normal ScalingReplicaSet 7m2s deployment-controller Scaled up replica set nginx-deploy-5bbc6bfb99 to 4 Normal ScalingReplicaSet 4m56s deployment-controller Scaled down replica set nginx-deploy-5bbc6bfb99 to 2 Normal ScalingReplicaSet 42s deployment-controller Scaled up replica set nginx-deploy-5bbc6bfb99 to 5 从打印的信息中可见，42秒前deployment-controller进行了一次伸缩的动作，将ReplicaSet对象nginx-deploy-5bbc6bfb99控制的POD伸缩到5了。\n负载的滚动升级 Deployment怎么做到滚动升级，其实就是通过ReplicaSet对象来实现的，实际上在前文中，我们写错了image地址，导致生成的ReplicaSet创建的POD不生效，后面更新了YAML文件，重新生成了ReplicaSet，并创建了新的POD，这个过程跟负载滚动升级是一致的。\n修改一下YAML文件，修改镜像版本，或者使用新的仓库地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 # API版本 kind: Deployment # API对象类型 metadata: name: nginx-deploy labels: chapter: first-app spec: selector: matchLabels: #spec.selector.matchLabels的键值对与spec.template.metadata.labels的键值对需一致,用于选择pod app: nginx replicas: 3 # Pod 副本数量 minReadySeconds: 5 strategy: type: RollingUpdate # 指定更新策略：RollingUpdate和Recreate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: # Pod 模板 metadata: labels: app: nginx spec: containers: - name: nginx image: xxxxxxxxxxxxxx.com:80/public/nginx:x86-1.20.2 ports: - containerPort: 80 #容器监听的端口80 实际上除了更改了镜像之外，还指定了更新策略：\n1 2 3 4 5 6 minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 minReadySeconds：表示 Kubernetes 在等待设置的时间后才进行升级，如果没有设置该值，Kubernetes 会假设该容器启动起来后就提供服务了，如果没有设置该值，在某些极端情况下可能会造成服务不正常运行，默认值就是0。 type=RollingUpdate：表示设置更新策略为滚动更新，可以设置为Recreate和RollingUpdate两个值，Recreate表示全部重新创建，默认值就是RollingUpdate，表示创建一个POD再删除一个POD来进行滚动升级。 maxSurge：表示升级过程中最多可以比原先设置多出的 Pod 数量，例如：maxSurage=1，replicas=5，就表示Kubernetes 会先启动一个新的 Pod，然后才删掉一个旧的 Pod，整个升级过程中最多会有5+1个 Pod。 maxUnavaible：表示升级过程中最多有多少个 Pod 处于无法提供服务的状态，当maxSurge不为0时，该值也不能为0，例如：maxUnavaible=1，则表示 Kubernetes 整个升级过程中最多会有1个 Pod 处于无法服务的状态。 使用kubectl apply进行滚动升级Deployment对象。\n1 2 # kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deploy configured 当然也可在升级过程中暂停，使用pause命令。\n1 2 # kubectl rollout pause deployment/nginx-deploy deployment.apps/nginx-deploy paused 查看Deployment详细信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # kubectl describe deploy nginx-deploy Name: nginx-deploy Namespace: default CreationTimestamp: Wed, 05 Jan 2022 14:49:34 +0800 Labels: chapter=first-app Annotations: deployment.kubernetes.io/revision: 6 Selector: app=nginx Replicas: 3 desired | 2 updated | 4 total | 4 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 5 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: #........省略 OldReplicaSets: nginx-deploy-5bbc6bfb99 (2/2 replicas created) NewReplicaSet: nginx-deploy-5b7b9ccb95 (5/5 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 26m deployment-controller Scaled up replica set nginx-deploy-85ff79dd56 to 4 Normal ScalingReplicaSet 3m44s deployment-controller Scaled down replica set nginx-deploy-85ff79dd56 to 3 Normal ScalingReplicaSet 3m44s deployment-controller Scaled up replica set nginx-deploy-5b7b9ccb95 to 1 Normal ScalingReplicaSet 3m44s deployment-controller Scaled down replica set nginx-deploy-85ff79dd56 to 2 Normal ScalingReplicaSet 3m44s deployment-controller Scaled up replica set nginx-deploy-5b7b9ccb95 to 2 在上面的例子中，伸缩前用 kubectl scale 命令将 Pod 副本调整到了 4，但更新的时候在YAML中又声明副本数为 3 了，所以 Deployment 控制器首先是将之前控制的 nginx-deploy-85ff79dd56 这个 RS 资源对象进行缩容操作，然后滚动更新开始了，可以发现 Deployment 为一个新的 nginx-deploy-5b7b9ccb95 RS 资源对象首先新建了一个新的 Pod，然后将之前的 RS 对象缩容到 2 了，再然后新的 RS 对象扩容到 2，后面由于我们暂停滚动升级了，所以没有后续的事件了。即滚动升级的过程是：\n启动一个新的 Pod。\n删除一个旧的 Pod。\n然后再启动一个新的 Pod，直到所有的POD的都变成新的POD。\n而且因为设置的策略maxSurge=1，所以在升级过程中是允许比原先设置多出1个 Pod ，即4个POD，两个新的 Pod，两个旧的 Pod，如下图示意：\n使用kubectl rollout resume来恢复滚动更新：\n1 2 3 4 5 # kubectl rollout resume deployment/nginx-deploy deployment.apps/nginx-deploy resumed # kubectl rollout status deployment/nginx-deploy Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 2 of 3 updated replicas are available... deployment \u0026#34;nginx-deploy\u0026#34; successfully rolled out 观察POD信息查看资源状态：\n1 2 3 4 5 # kubectl get pod -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deploy-75bcbf6c9b-lfdlf 1/1 Running 0 23m nginx-deploy-75bcbf6c9b-tnbpm 1/1 Running 0 20m nginx-deploy-75bcbf6c9b-tx6gw 1/1 Running 0 23m 几个有用的小tips 查看负载、pod、服务等对象的IP、端口信息时可加上-o wide参数，如：\n1 2 3 # kubectl get pod nginx-deploy-75bcbf6c9b-lfdlf -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deploy-75bcbf6c9b-lfdlf 1/1 Running 0 24m 12.11.0.58 10.247.154.37 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 想查询系统中部署好的负载、服务的YAML定义，可加上-o yaml参数\n1 # kubectl get deploy nginx-deploy -o yaml 可以通过rollout历史记录回滚对应版本。\n1 2 3 4 5 6 7 8 # kubectl rollout history deployment nginx-deploy #查询revision版本 deployment.apps/nginx-deploy REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; # kubectl rollout history deployment nginx-deploy --revision=1 #查看revision=1的版本信息 # kubectl rollout undo deployment nginx-deploy #回滚到升级前的前一个版本 # kubectl rollout undo deployment nginx-deploy --to-revision=1 #回滚到revision=1的版本 全文完。\n","description":"通过YAML声明K8s API对象，使用kubectl部署与升级。","id":14,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"02-使用YAML部署与升级Nginx","uri":"https://laomeinote.com/posts/deploy-and-update-nginx-with-yaml/"},{"content":"1.环境准备 在华为云上申请3台ECS虚机服务器，临时测试使用，建议按需付费申请。选择VPC为192.168.0.0/16网段，将在这三个节点部署v1.23.1最新版本Kubernetes集群。为何要选三个节点？是因为准备实验ceph作为后端分布式存储，至少需要三个节点起，且需要一个裸盘作为ceph存储数据盘，所以在华为云上给每个ECS配置一个额外的100G空EVS块存储，具体信息如下表；选择flannel作为底层网络插件。\n角色 OS 节点name 存储 IP docker version kubelet version kubeadm version kubectl version network master Centos7.9 master 40G+100G(数据盘) 192.168.0.11 Docker 20.10.8 V1.23.1 V1.23.1 V1.23.1 flannel master Centos7.9 node1 40G+100G(数据盘) 192.168.0.23 Docker 20.10.8 V1.23.1 V1.23.1 V1.23.1 flannel master Centos7.9 node2 40G+100G(数据盘) 192.168.0.51 Docker 20.10.8 V1.23.1 V1.23.1 V1.23.1 flannel 表1：环境信息，可左右滑动查看全部信息。\n注: Kubernetes从v.1.20版本起默认移除 docker 的依赖，如果宿主机上安装了 docker 和 containerd，将优先使用 docker 作为容器运行引擎，如果宿主机上未安装 docker 只安装了 containerd，将使用 containerd 作为容器运行引擎。为减少学习成本，这里选择安装docker。\n2.配置安全组 无论华为云、腾讯云还是阿里云、AWS、Azure在配置生成VM虚机都会默认选择安全组，对虚机的网络进行简单防护，即通过安全组限制哪些端口放开，哪些端口可访问，K8s在安装过程中一些组件是通过Service、POD提供网络服务的，是需要开启对应的端口的，否则服务会异常无法部署成功。\n当然如果是直接采用物理服务器，或者VMware虚拟机则不需要考虑。默认需要开启的网络端口如下：\n2.1 入站规则 master 节点端口检查：\nProtocol Direction Port Range Purpose TCP Inbound 6443 Kube-apiserver TCP Inbound 2379-2380 Etcd API TCP Inbound 10250 Kubelet API TCP Inbound 10251 Kube-scheduler TCP Inbound 10252 Kube-controller-manager node1、node2 节点端口检查：\nProtocol Direction Port Range Purpose TCP Inbound 10250 Kubelet api TCP Inbound 30000-32767 NodePort Service 2.2 出站规则 协议规则 端口 来源 策略 ALL ALL 0.0.0.0/0 允许 3. 配置基础信息 给所有节点提前安装准备好基础软件。\n3.1修改主机信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # 分别在三个主机上设置主机名，重启生效 hostnamectl set-hostname master reboot hostnamectl set-hostname node1 reboot hostnamectl set-hostname node2 reboot # 三个主机上同步时间 systemctl restart chronyd #三个主机上配置hosts地址DNS解析地址 cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 192.168.0.11 master 192.168.0.23 node1 192.168.0.51 node2 EOF # 设置三台机子间无密码访问，在主节点生成密钥，拷贝至另外两台，则可在master直接登录node1和node2 # 如果想在node1和node2访问节点，也分别执行下面语句生成密钥并拷贝 ssh-keygen -t rsa ssh-copy-id root@node1 ssh-copy-id root@node2 # 关闭防火墙和iptables systemctl stop firewalld.service systemctl disable firewalld.service systemctl stop iptables.service systemctl disable iptables.service # 关闭SELinux setenforce 0 sed -i \u0026#39;s/^SELINUX=.*/SELINUX=disabled/\u0026#39; /etc/selinux/config # 关闭swap swapoff -a sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab # 配置内核参数： cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF 3.2 修改yum源 1 2 3 4 sudo mkdir /etc/yum.repos.d/bak \u0026amp;\u0026amp; mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak wget -O /etc/yum.repos.d/CentOS-Base.repo https://repo.huaweicloud.com/repository/conf/CentOS-7-reg.repo sudo yum clean all sudo yum makecache fast 3.3 安装基础软件 1 2 3 4 5 #安装自动补全软件与基础依赖包 sudo yum install -y bash-completion source /etc/profile.d/bash_completion.sh sudo yum remove docker docker-common docker-selinux docker-engine sudo yum install -y yum-utils device-mapper-persistent-data lvm2 4.安装docker 给所有节点安装Docker、配置镜像加速。\n4.1 安装Docker软件 1 2 3 4 5 6 7 8 9 10 11 #补充docker yum源 wget -O /etc/yum.repos.d/docker-ce.repo https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo sudo sed -i \u0026#39;s+download.docker.com+repo.huaweicloud.com/docker-ce+\u0026#39; /etc/yum.repos.d/docker-ce.repo sudo yum makecache fast #安装最新版本Docker 20.10.8，安装前可使用yum list docker-ce --showduplicates |sort -r查看yum源中的docker列表 sudo yum install -y docker-ce sudo systemctl enable docker sudo systemctl start docker sudo systemctl status docker docker --version 4.2 Docker镜像加速与设置cgroup 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #配置/etc/docker/daemon.json文件，对镜像加速，注意换成自己镜像地址 sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://e2660ea6dc2b4a16a3ae382f8d227beb.mirror.swr.myhuaweicloud.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF # 上面配置\u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;]，即是将docker使用systemd作为cgroupdriver，否则kubelet可能启动不正常 # 重启docker sudo systemctl daemon-reload sudo systemctl restart docker sudo systemctl status docker 5. 安装Kubernets集群 首先对Master节点安装Kubernetes，然后将Node1、Node2加入集群。\n5.1 安装Kubeadm 分别在三个节点上安装kubeadm，kubelet，kubectl工具。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # 所有节点上添加阿里的Kubernetes源 cat \u0026gt;\u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt;EOF [kubernetes] name=Kubernetes Repository baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF sudo yum clean all sudo yum makecache fast #分别查询1.23.1包是否在yum源里 yum list kubelet --showduplicates | sort -r yum list kubectl --showduplicates | sort -r yum list kubeadm --showduplicates | sort -r #安装kubeadm，会自动安装好kubectl，kubelet sudo yum install -y kubeadm sudo systemctl enable kubelet sudo systemctl start kubelet kubeadm version kubectl version kubelet --version #kubectl命令补全 cd echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile source .bash_profile # master查看所需的镜像 sudo kubeadm config images list ------------------------------------------------ #查询的需要如下镜像 k8s.gcr.io/kube-apiserver:v1.23.1 k8s.gcr.io/kube-controller-manager:v1.23.1 k8s.gcr.io/kube-scheduler:v1.23.1 k8s.gcr.io/kube-proxy:v1.23.1 k8s.gcr.io/pause:3.6 k8s.gcr.io/etcd:3.5.1-0 k8s.gcr.io/coredns/coredns:v1.8.6 ------------------------------------------------- # 由于kubeadm依赖国外的k8s.gcr.io的镜像，国内被墙所以这边的解决方案是下载国内的镜像重新打tag的方式 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.1 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.1 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.1 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.1 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 # 修改tag回k8s.gcr.io（重命名） docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.1 k8s.gcr.io/kube-apiserver:v1.23.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.1 k8s.gcr.io/kube-controller-manager:v1.23.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.1 k8s.gcr.io/kube-scheduler:v1.23.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.1 k8s.gcr.io/kube-proxy:v1.23.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 k8s.gcr.io/pause:3.6 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0 k8s.gcr.io/etcd:3.5.1-0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 k8s.gcr.io/coredns/coredns:v1.8.6 5.2 Master节点初始化 1 2 3 4 5 6 # master执行init初始化，指定pod网络为10.244.0.0/16，服务网络为10.1.0.0/16，这两个均为集群内部网络，API-server为master节点IP（从华为云上VPC中分配的） kubeadm init \\ --kubernetes-version=1.23.1 \\ --apiserver-advertise-address=192.168.0.11 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 初始化安装过程打印如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 [init] Using Kubernetes version: v1.23.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.lo cal master] and IPs [10.1.0.1 192.168.0.46] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.0.46 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.0.46 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 5.502543 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.23\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster NOTE: The \u0026#34;kubelet-config-1.23\u0026#34; naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just \u0026#34;kubelet-config\u0026#34;. Kubeadm upgrade will handle this transition transparently. [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: b2n16t.n6filxh3vc6byr7c [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.46:6443 --token b2n16t.n6filxh3vc6byr7c \\ --discovery-token-ca-cert-hash sha256:f4d103707658df3fa7a8dc95a59719f362cd42edb40c8ebc5ae19d53655813d1 根据提示，将配置拷贝至.kube目录下\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 应用网络插件flannel\n1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 5.3 节点加入集群 分别在两个节点上执行kubectl join加入集群。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@node1 ~]# kubeadm join 192.168.0.46:6443 --token b2n16t.n6filxh3vc6byr7c \\ \u0026gt; --discovery-token-ca-cert-hash sha256:f4d103707658df3fa7a8dc95a59719f362cd42edb40c8ebc5ae19d53655813d1 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. [root@node1 ~]# 如果想在节点上执行kubectl命令，则需要将master节点配置拷贝至节点的$HOME/.kube目录下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #在节点上创建一个目录，即kubectl默认启动读取证书配置目录 [root@node1 ~]# mkdir -p $HOME/.kube #在master节点将配置复制到node1节点 [root@master ~]# scp .kube/config root@node1:/root/.kube/ #可在node节点查看节点，组件，pod等状态 [root@node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 43m v1.23.1 node1 Ready \u0026lt;none\u0026gt; 39m v1.23.1 node2 Ready \u0026lt;none\u0026gt; 39m v1.23.1 [root@node1 ~]# kubectl get ns NAME STATUS AGE default Active 43m kube-node-lease Active 43m kube-public Active 43m kube-system Active 43m [root@node1 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} [root@node1 ~]# kubectl get pods -nkube-system NAME READY STATUS RESTARTS AGE coredns-64897985d-bs6b9 1/1 Running 0 43m coredns-64897985d-s2kml 1/1 Running 0 43m etcd-master 1/1 Running 0 44m kube-apiserver-master 1/1 Running 0 44m kube-controller-manager-master 1/1 Running 0 44m kube-flannel-ds-8jpd4 1/1 Running 0 39m kube-flannel-ds-jlfzx 1/1 Running 0 39m kube-flannel-ds-jztwk 1/1 Running 0 41m kube-proxy-5lnr9 1/1 Running 0 39m kube-proxy-thghs 1/1 Running 0 43m kube-proxy-w7rhv 1/1 Running 0 39m kube-scheduler-master 1/1 Running 0 44m [root@node1 ~]# 5.4 安装ceph存储 我们通过rook来安装ceph存储，ceph要求至少三个节点，每个节点至少有一个裸盘。我们在申请ECS的时候加了一个100G的EVS块存储盘。\n1 2 3 4 5 6 7 8 9 10 [root@master ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part / vdb 253:16 0 100G 0 disk [root@master ~]# lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT vda └─vda1 ext4 b64c5c5d-9f6b-4754-9e1e-eaef91437f7a / vdb 为了方便，可先到github把整个rook项目下载下来。\n1 2 yum install -y git git clone https://github.com/rook/rook.git 确保系统内核支持rbd\n1 2 3 4 5 6 [root@master ~]# uname -r 3.10.0-1160.15.2.el7.x86_64 [root@master ~]# modprobe rbd [root@master ~]# lsmod |grep rbd rbd 102400 0 libceph 413696 1 rbd 因为我们只部署了3个节点，而ceph最低要求3个节点，pod默认不允许部署在master节点，为了ceph的pod能正常部署，我们提前将master节点的污点去掉，允许pod部署在master节点上。\n1 [root@master1 ~]# kubectl taint nodes --all node-role.kubernetes.io/master- 开始部署rook\n1 2 3 4 5 cd /root/rook/deploy/examples kubectl apply -f crds.yaml -f common.yaml kubectl apply -f operator.yaml #如果img下载不下来可提前pull到本地 kubectl apply -f cluster.yaml kubectl get pods -n rook-ceph -o wide 根据提示会有很多image拉取不到，到aliyun上逐渐获取，需要在各个节点上执行，记录的几个镜像：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/rook/ceph:master docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/csi-provisioner:v3.0.0 docker pull quay.io/cephcsi/cephcsi:v3.4.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/csi-attacher:v3.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/csi-snapshotter:v4.2.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/csi-resizer:v1.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.3.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/csi-provisioner:v3.0.0 k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/csi-attacher:v3.3.0 k8s.gcr.io/sig-storage/csi-attacher:v3.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/csi-snapshotter:v4.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/csi-resizer:v1.3.0 k8s.gcr.io/sig-storage/csi-resizer:v1.3.0 镜像问题解决后查看namespace下的pod状态：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [root@master examples]# kubectl get pods -nrook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-ct842 3/3 Running 0 25m csi-cephfsplugin-cvb7f 3/3 Running 0 25m csi-cephfsplugin-j5gbm 3/3 Running 0 25m csi-cephfsplugin-provisioner-5c8b6d6f4-hhvjq 6/6 Running 0 25m csi-cephfsplugin-provisioner-5c8b6d6f4-kr4n5 6/6 Running 0 25m csi-rbdplugin-fcbk9 3/3 Running 0 25m csi-rbdplugin-fpv8t 3/3 Running 0 25m csi-rbdplugin-provisioner-8564cfd44-jkqrq 6/6 Running 0 25m csi-rbdplugin-provisioner-8564cfd44-q8srg 6/6 Running 0 25m csi-rbdplugin-qtgvt 3/3 Running 0 25m rook-ceph-crashcollector-master-7bcf565ddc-4mvmk 1/1 Running 0 20m rook-ceph-crashcollector-node1-7bfc99f96d-2jw4w 1/1 Running 0 20m rook-ceph-crashcollector-node2-678f85bdf-qw2gq 1/1 Running 0 20m rook-ceph-mgr-a-574b6956fd-fzt5q 1/1 Running 0 20m rook-ceph-mon-a-668b48987f-g5zfw 1/1 Running 0 25m rook-ceph-mon-b-54996b7487-6qscc 1/1 Running 0 24m rook-ceph-mon-c-6cc5bd5c85-wsrn9 1/1 Running 0 22m rook-ceph-operator-75dd789779-8kq7z 1/1 Running 0 30m rook-ceph-osd-0-849c84cc87-bzpf9 1/1 Running 0 20m rook-ceph-osd-1-77cfc975bb-hbdnn 1/1 Running 0 20m rook-ceph-osd-2-5c7d59d74d-g67fz 1/1 Running 0 20m rook-ceph-osd-prepare-master-98nld 0/1 Completed 0 20m rook-ceph-osd-prepare-node1-nvqvg 0/1 Completed 0 20m rook-ceph-osd-prepare-node2-x6cnk 0/1 Completed 0 20m [root@master examples]# kubectl get service -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.1.101.105 \u0026lt;none\u0026gt; 8080/TCP,8081/TCP 26m csi-rbdplugin-metrics ClusterIP 10.1.238.71 \u0026lt;none\u0026gt; 8080/TCP,8081/TCP 26m rook-ceph-mgr ClusterIP 10.1.98.179 \u0026lt;none\u0026gt; 9283/TCP 21m rook-ceph-mgr-dashboard ClusterIP 10.1.251.161 \u0026lt;none\u0026gt; 8443/TCP 21m rook-ceph-mon-a ClusterIP 10.1.0.149 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 26m rook-ceph-mon-b ClusterIP 10.1.42.253 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 25m rook-ceph-mon-c ClusterIP 10.1.99.90 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 24m \u0026lt;1\u0026gt;. 三个节点rook-ceph-osd-prepare的正常状态为Completed\n\u0026lt;2\u0026gt;. 如果其中一个为Running或缺少rook-ceph-osd节点，注意检查异常节点的时间，防火墙，内存使用情况等。\n\u0026lt;3\u0026gt;. 部署Toolbox工具\n上面的dashboard是cluster IP集群内部访问，如果想在外部访问，可部署NodePort类型Dashboard，好在rook项目已经写好了，直接使用即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@master examples]# cd /root/rook/deploy/examples [root@master examples]# [root@master examples]# kubectl apply -f dashboard-external-https.yaml service/rook-ceph-mgr-dashboard-external-https created [root@master examples]# kubectl get service -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.1.101.105 \u0026lt;none\u0026gt; 8080/TCP,8081/TCP 31m csi-rbdplugin-metrics ClusterIP 10.1.238.71 \u0026lt;none\u0026gt; 8080/TCP,8081/TCP 31m rook-ceph-mgr ClusterIP 10.1.98.179 \u0026lt;none\u0026gt; 9283/TCP 26m rook-ceph-mgr-dashboard ClusterIP 10.1.251.161 \u0026lt;none\u0026gt; 8443/TCP 26m rook-ceph-mgr-dashboard-external-https NodePort 10.1.182.240 \u0026lt;none\u0026gt; 8443:30301/TCP 35s rook-ceph-mon-a ClusterIP 10.1.0.149 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 31m rook-ceph-mon-b ClusterIP 10.1.42.253 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 30m rook-ceph-mon-c ClusterIP 10.1.99.90 \u0026lt;none\u0026gt; 6789/TCP,3300/TCP 28m [root@master examples]# 已经多出一个30301端口的NodePort类型服务，随便拿一个node的IP访问：https://Node-EIP1:30301，输入用户名和密码即可。\n访问dashboard的用户名默认是admin，密码通过如下命令获取：\n1 kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\u0026#34;{[\u0026#39;data\u0026#39;][\u0026#39;password\u0026#39;]}\u0026#34; | base64 --decode \u0026amp;\u0026amp; echo 部署Ceph toolbox: 默认启动的Ceph集群，是开启Ceph认证的，这样你登陆Ceph组件所在的Pod里，是没法去获取集群状态，以及执行CLI命令，这时需要部署Ceph toolbox，命令如下：\n1 2 3 4 5 6 7 kubectl apply -f toolbox.yaml #查看是否正常 kubectl -n rook-ceph get pods -o wide | grep ceph-tools #然后可以登陆该pod后，执行Ceph CLI命令： kubectl -n rook-ceph exec -it rook-ceph-tools-76c7d559b6-8w7bk bash #查看集群状态 ceph status rook提供RBD服务，rook可以提供以下3类型的存储：\nBlock: Create block storage to be consumed by a pod Object: Create an object store that is accessible inside or outside the Kubernetes cluster Shared File System: Create a file system to be shared across multiple pods 在提供（Provisioning）块存储之前，需要先创建StorageClass和存储池。K8S需要这两类资源，才能和Rook交互，进而分配持久卷（PV）。\n在kubernetes集群里，要提供rbd块设备服务，需要有如下步骤：\n1）创建rbd-provisioner pod\n创建rbd对应的storageclass 创建pvc，使用rbd对应的storageclass 创建pod使用rbd pvc 通过rook创建Ceph Cluster之后，rook自身提供了rbd-provisioner服务，所以我们不需要再部署其provisioner。 创建pool和StorageClass\n查看storageclass.yaml的配置， vim storageclass.yaml，配置文件中包含了一个名为replicapool的存储池，名为rook-ceph-block的storageClass，运行yaml文件 1 2 3 4 [root@master ~]#cd rook/deploy/examples/csi/rbd [root@master rbd]# kubectl apply -f storageclass.yaml cephblockpool.ceph.rook.io/replicapool created storageclass.storage.k8s.io/rook-ceph-block created 2）查看创建的storageclass:\n1 2 3 [root@master rbd]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 2m36s 3）登录ceph dashboard查看创建的存储池：\n使用存储，以官方服务wordpress示例为例，创建一个经典的wordpress和mysql应用程序来使用Rook提供的块存储，这两个应用程序都将使用Rook提供的block volumes。\n查看yaml文件配置，主要看定义的pvc和挂载volume部分，以wordpress.yaml和mysql.yaml为例：\n1 2 3 4 5 6 7 8 9 10 11 12 [root@master ~]# cd rook/deploy/examples/ [root@master examples]# kubectl apply -f wordpress.yaml -f mysql.yaml service/wordpress created persistentvolumeclaim/wp-pv-claim created deployment.apps/wordpress created service/wordpress-mysql created persistentvolumeclaim/mysql-pv-claim created deployment.apps/wordpress-mysql created [root@master examples]# kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE wordpress 0/1 1 0 28s wordpress-mysql 0/1 1 0 28s 这2个应用都会创建一个块存储卷，并且挂载到各自的pod中，查看声明的pvc和pv：\n1 2 3 4 5 6 7 8 9 [root@master examples]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-cdfbbd11-a22e-4f72-96cd-064e228eb730 20Gi RWO rook-ceph-block 83s wp-pv-claim Bound pvc-b09ce46e-d00e-4b7d-8303-748bbb7d0944 20Gi RWO rook-ceph-block 83s [root@master examples]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b09ce46e-d00e-4b7d-8303-748bbb7d0944 20Gi RWO Delete Bound default/wp-pv-claim rook-ceph-block 86s pvc-cdfbbd11-a22e-4f72-96cd-064e228eb730 20Gi RWO Delete Bound default/mysql-pv-claim rook-ceph-block 86s [root@master examples]# 这里的pv会自动创建，当提交了包含 StorageClass 字段的 PVC 之后，Kubernetes 就会根据这个 StorageClass 创建出对应的 PV，这是用到的是Dynamic Provisioning机制来动态创建pv，PV 支持 Static 静态请求，和动态创建两种方式。\n登录ceph dashboard查看创建的images\n5.5 安装Dashboard可视化面板 从github获取dashboard源码\n1 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml 为了测试方便，我们将Service改成NodePort类型，注意 在YAML中下面的 Service 部分新增一个type=NodePort：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 type: NodePort selector: k8s-app: kubernetes-dashboard 默认没有字段type: NodePort，服务类型为cluster IP类型。\n然后直接部署新版本的dashboard即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@master ~]# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created [root@master ~]# kubectl get ns NAME STATUS AGE default Active 50m kube-node-lease Active 50m kube-public Active 50m kube-system Active 50m kubernetes-dashboard Active 11s rook-ceph Active 46m [root@master ~]# kubectl get svc -nkubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.1.213.171 \u0026lt;none\u0026gt; 8000/TCP 32s kubernetes-dashboard NodePort 10.1.221.14 \u0026lt;none\u0026gt; 443:31712/TCP 32s 其中NodePort为31712，随意组合一个Node节点IP即可访问。https://NodeIP:31712, 由于在华为云外网无法直接访问VPC内部IP地址，所以需要使用外部EIP访问，EIP会隐射到内部Node节点IP上去。\n这个时候需要使用Token或者Kubeconfig来登陆。\n1 2 3 kubectl create serviceaccount dashboard-admin -n kube-system kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 示例一下Token：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@master ~]# kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) Name: dashboard-admin-token-thf6q Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: d6ea3599-19c6-48a9-aa3b-2ec7ce265a24 Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImJQRzl4aF9wMFdRbWE2blp0b1JvN2dVNWhkRkdZVzRpMndLMnhJbks5S00ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdGhmNnEiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZDZlYTM1OTktMTljNi00OGE5LWFhM2ItMmVjN2NlMjY1YTI0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.PlaEmz10kVjQf1zxUSNfiGytP0Ha6hCLuk2fBFM08owjEaFcIWHdRVRsHL6RO0w0i81YG0Gh6x3zJffy_ojhi_M-bCaPSVubPFrZz-CYO7Uia4fYv1P8f5c6I2X1e_-K2DzCYUlJvI3nzZy-jrFMIz_W19k63rRbxeNrqkdBJpsheWmaT_g8fjAzjtCDEnYUGDDPTVOtEvuhaSC_yci42f7eqTtlR2_QK1Bg2Id0GIEtEXT3xBgaofWuyjJVEex1mc4LImsdzpVFMtmPum9vEoZzxq1EONhOWxaaFIaadstfM-id9vDNlvZ5O2szk5xVtdgryFi72ICX7x5EpPyOqw ca.crt: 1099 bytes namespace: 11 bytes 可以拿上面的token直接登陆，另也可以使用config文件也可以登录Dashboard。\n生成kubeconfig文件\n1 2 3 4 5 6 7 8 9 DASH_TOCKEN=$(kubectl get secret -n kube-system dashboard-admin-token-thf6q -o jsonpath={.data.token}|base64 -d) #其中的 dashboard-admin-token-thf6q为上面生成的token名 kubectl config set-cluster kubernetes --server=192.168.0.11:6443 --kubeconfig=/root/dashbord-admin.conf #其中server地址为API-server地址 kubectl config set-credentials dashboard-admin --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/dashbord-admin.conf kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/dashbord-admin.conf 生成的dashbord-admin.conf即可用于登录Dashboard。\n全文完。\n","description":"使用Kubeadm快速在华为云上搭建最新V1.23.1版本Kubenetes,使用Flannel作为网络插件,使用Rook搭建存储。","id":15,"section":"posts","tags":["K8s","Kubernetes","CloudNative"],"title":"01-以Kubeadm上部署K8s集群","uri":"https://laomeinote.com/posts/deploy-k8s-cluster/"},{"content":"Harbor是VMware公司开源的企业级的Docker Registry管理项目，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。使用Harbor可以部署企业自己的私有仓库，将一些核心镜像与软件与外部网络进行隔离与防护。\n1. 环境准备 本次在腾讯云ECS服务器上部署Harbor私有仓库，系统与相关信息如下。\n硬件信息：\n资源 容量 描述 CPU 2 核 4 CPU is preferred 内存 2 GB 8GB is preferred 磁盘 50 GB 160GB is preferred 网络 2 Mbps 4Mbps is preferred 软件信息：\n软件 版本 描述 Centos CentOS 7.6 64位 \\ Docker 19.03.1 官方安装指南 docker engine doc Docker Compose 1.22.0 官方安装指南 docker compose doc Openssl OpenSSL 1.0.2k-fips 为Harbor生成证书与密钥 注意：Harbor的所有服务组件都是在Docker中部署的，所以官方安装使用Docker-compose快速部署，因此需要安装Docker、Docker-compose。由于Harbor是基于Docker Registry V2版本，所以就要求Docker版本不小于1.10.0，Docker-compose版本不小于1.6.0。\n2. 安装Docker与Docker-Compose 2.1 安装Docker 1 2 3 # yum install -y yum-utils device-mapper-persistent-data lvm2 epel-release # yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # yum install docker-ce 2.2 启动Docker 1 2 3 # systemctl enable docker # systemctl start docker # docker --version 2.3 安装Docker-Compose 1 2 3 # curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose # chmod +x /usr/local/bin/docker-compose # docker-compose --version 2.4 配置证书与密钥 通过OpenSSL工具生成自签名的证书，后面将用于对请求进行校验。官方指南参考：Configuring Harbor with HTTPS Access\n首先找到OpenSSL工具配置文件openssl.cnf，对于Centos,目录在/etc/pki/tls/中，编辑openssl.cnf,在[v3_ca]下面添加：subjectAltName = IP:域名|IP地址。如果没有域名填写IP即可。\n1 2 [ v3_ca ] subjectAltName = IP:193.112.221.230 通过OpenSSL生成证书与密钥\n1 2 3 4 5 6 7 [root@VM_95_141_centos data]# cd /data/ssl [root@VM_95_141_centos ssl]# openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt [root@VM_95_141_centos ssl]# ls ca.crt ca.key [root@VM_95_141_centos ssl]# cp ca.crt /etc/pki/ca-trust/source/anchors/ [root@VM_95_141_centos ssl]# update-ca-trust enable [root@VM_95_141_centos ssl]# update-ca-trust extract 将生成的私有证书追加到系统的证书管理文件中。\n1 [root@VM_95_141_centos harbor]# cat ssl/ca.crt \u0026gt;\u0026gt; /etc/pki/tls/certs/ca-bundle.crt 重启docker, 该步骤一定不要省略，否则有可能加载私钥失败\n1 [root@VM_95_141_centos harbor]#systemctl restart docker 3. 安装Harbor 1 2 3 4 # wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.2.tgz # tar xzvf harbor-offline-installer-v1.8.2.tgz # cd harbor/ # ls 下载离线安装包，解压后可以看到里面包含了如下文件：\n1 2 [root@VM_95_141_centos harbor]# ls harbor.v1.8.2.tar.gz harbor.yml install.sh LICENSE prepare 其中harbor.v1.8.2.tar.gz是软件包，harbor.yml是配置文件，install.sh是安装脚本，prepare是准备配置Harbor环境脚本。需要修改harbor.yml如下字段进行配置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 hostname: 193.112.221.230 # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/ssl/ca.crt private_key: /data/ssl/ca.key 然后先后执行prepare与install.sh进行配置与安装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [root@VM_95_141_centos harbor]# ./prepare prepare base dir is set to /root/harbor Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml loaded secret from file: /secret/keys/secretkey Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir [root@VM_95_141_centos harbor]# ls -l total 564668 drwxr-xr-x 3 root root 4096 Aug 24 23:09 common -rw-r--r-- 1 root root 5377 Aug 24 23:09 docker-compose.yml -rw-r--r-- 1 root root 578167000 Aug 8 15:51 harbor.v1.8.2.tar.gz -rw-r--r-- 1 root root 4494 Aug 24 23:08 harbor.yml -rwxr-xr-x 1 root root 5088 Aug 8 15:51 install.sh -rw-r--r-- 1 root root 11347 Aug 8 15:51 LICENSE -rwxr-xr-x 1 root root 1654 Aug 8 15:51 prepare [root@VM_95_141_centos harbor]# vi docker-compose.yml [root@VM_95_141_centos harbor]# ./install.sh 一般都会安装成功。\n4. 操作Harbor Harbor安装成功后，如下图示。我们可以对齐进行启动、停止、修改、上传镜像、下载镜像等操作。\n4.1 查看Harbor 可以通过docker ps命令查询当前有哪些docker实例在运行。查询Harbor是否正常运行还可以查询docker-compose运行状态。需要进入Harbor所在目录，执行docker-compose ps。\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@VM_95_141_centos harbor]# docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------- harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-\u0026gt;10514/tcp harbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-\u0026gt;443/tcp, 0.0.0.0:80-\u0026gt;80/tcp redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) [root@VM_95_141_centos harbor]# 4.2 停止与重启Harbor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@VM_95_141_centos ~]# cd harbor [root@VM_95_141_centos harbor]# ls common docker-compose.yml harbor.v1.8.2.tar.gz harbor.yml install.sh LICENSE prepare [root@VM_95_141_centos harbor]# docker-compose stop Stopping nginx ... done Stopping harbor-portal ... done Stopping harbor-jobservice ... done Stopping harbor-core ... done Stopping registryctl ... done Stopping redis ... done Stopping registry ... done Stopping harbor-db ... done Stopping harbor-log ... done [root@VM_95_141_centos harbor]# docker-compose start Starting log ... done Starting registry ... done Starting registryctl ... done Starting postgresql ... done Starting core ... done Starting portal ... done Starting redis ... done Starting jobservice ... done Starting proxy ... done [root@VM_95_141_centos harbor]# 如果需要修改Harbor的配置，可以先停止Harbor实例，更新 harbor.yml文件，再执行 prepare 脚本重新构建配置，最终启动Harbor实例:\n1 2 3 4 [root@VM_95_141_centos ~]# docker-compose down -v [root@VM_95_141_centos ~]# vim harbor.yml [root@VM_95_141_centos ~]# prepare [root@VM_95_141_centos ~]# docker-compose up -d 清除Harbor实例的时候会保持镜像数据与数据库在系统中：\n1 [root@VM_95_141_centos ~]# docker-compose down -v 删除Harbor数据库与镜像数据 (如果需要重新安装):\n1 2 [root@VM_95_141_centos ~]# rm -r /data/database [root@VM_95_141_centos ~]# rm -r /data/registry 4.3 上传镜像到Harbor 查看当前镜像:docker images\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@VM_95_141_centos harbor]# docker images REPOSITORY TAG IMAGE ID C REATED SIZE goharbor/chartmuseum-photon v0.9.0-v1.8.2 e72f3e685a37 2 weeks ago 130MB goharbor/harbor-migrator v1.8.2 c11a64ae3a1e 2 weeks ago 361MB goharbor/redis-photon v1.8.2 18036ee471bc 2 weeks ago 107MB goharbor/clair-photon v2.0.8-v1.8.2 68de68a40e66 2 weeks ago 164MB goharbor/notary-server-photon v0.6.1-v1.8.2 90cf28ef3a84 2 weeks ago 135MB goharbor/notary-signer-photon v0.6.1-v1.8.2 e9b49ea8ed32 2 weeks ago 132MB goharbor/harbor-registryctl v1.8.2 ad798fd6e618 2 weeks ago 96.5MB goharbor/registry-photon v2.7.1-patch-2819-v1.8.2 081bfb3dc181 2 weeks ago 81.6MB goharbor/nginx-photon v1.8.2 1592a48daeac 2 weeks ago 36.2MB goharbor/harbor-log v1.8.2 42ad5ef672dd 2 weeks ago 81.8MB goharbor/harbor-jobservice v1.8.2 623ed0095966 2 weeks ago 119MB goharbor/harbor-core v1.8.2 03d6daab10c7 2 weeks ago 135MB goharbor/harbor-portal v1.8.2 41e264a7980b 2 weeks ago 43.2MB goharbor/harbor-db v1.8.2 927ecd68ee1f 2 weeks ago 144MB goharbor/prepare v1.8.2 b0d62cc7683d 2 weeks ago 145MB 从Docker hub上下载一个最新的Nginx镜像:docker pull nginx：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@VM_95_141_centos harbor]# docker pull nginx Using default tag: latest latest: Pulling from library/nginx 1ab2bdfe9778: Pull complete a17e64cfe253: Pull complete e1288088c7a8: Pull complete Digest: sha256:53ddb41e46de3d63376579acf46f9a41a8d7de33645db47a486de9769201fec9 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest [root@VM_95_141_centos harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest 5a3221f0137b 8 days ago 126MB goharbor/chartmuseum-photon v0.9.0-v1.8.2 e72f3e685a37 2 weeks ago 130MB goharbor/harbor-migrator v1.8.2 c11a64ae3a1e 2 weeks ago 361MB goharbor/redis-photon v1.8.2 18036ee471bc 2 weeks ago 107MB goharbor/clair-photon v2.0.8-v1.8.2 68de68a40e66 2 weeks ago 164MB goharbor/notary-server-photon v0.6.1-v1.8.2 90cf28ef3a84 2 weeks ago 135MB goharbor/notary-signer-photon v0.6.1-v1.8.2 e9b49ea8ed32 2 weeks ago 132MB goharbor/harbor-registryctl v1.8.2 ad798fd6e618 2 weeks ago 96.5MB goharbor/registry-photon v2.7.1-patch-2819-v1.8.2 081bfb3dc181 2 weeks ago 81.6MB goharbor/nginx-photon v1.8.2 1592a48daeac 2 weeks ago 36.2MB goharbor/harbor-log v1.8.2 42ad5ef672dd 2 weeks ago 81.8MB goharbor/harbor-jobservice v1.8.2 623ed0095966 2 weeks ago 119MB goharbor/harbor-core v1.8.2 03d6daab10c7 2 weeks ago 135MB goharbor/harbor-portal v1.8.2 41e264a7980b 2 weeks ago 43.2MB goharbor/harbor-db v1.8.2 927ecd68ee1f 2 weeks ago 144MB goharbor/prepare v1.8.2 b0d62cc7683d 2 weeks ago 145MB 给Nginx镜像打上新标签，push到Harbor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@VM_95_141_centos harbor]# docker tag nginx-mei:v1.0 193.112.221.230/library/nginx:latest [root@VM_95_141_centos harbor]# docker login 193.112.221.230 -uadmin -pHarbor12345 WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded [root@VM_95_141_centos harbor]# docker push 193.112.221.230/library/nginx:latest The push refers to repository [193.112.221.230/library/nginx] 12fdf55172df: Pushed 002a63507c1c: Pushed 1c95c77433e8: Pushed latest: digest: sha256:099019968725f0fc12c4b69b289a347ae74cc56da0f0ef56e8eb8e0134fc7911 size: 948 [root@VM_95_141_centos harbor]# 4.4 从Harbor中下载镜像 从Harbor中下载刚上传的Nginx镜像，可以先删除本地的Nginx镜像。然后再从Harbor中Pull下来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 [root@VM_95_141_centos harbor]# docker rmi 193.112.221.230/library/nginx:latest Untagged: 193.112.221.230/library/nginx:latest Untagged: 193.112.221.230/library/nginx@sha256:099019968725f0fc12c4b69b289a347ae74cc56da0f0ef56e8eb8e0134fc7911 [root@VM_95_141_centos harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE wordpress latest fc03dc56d371 3 days ago 502MB 193.112.221.230/mei_test/nginx v1.0 5a3221f0137b 9 days ago 126MB nginx-mei v1.0 5a3221f0137b 9 days ago 126MB nginx latest 5a3221f0137b 9 days ago 126MB mysql 5.6 732765f8c7d2 11 days ago 257MB goharbor/chartmuseum-photon v0.9.0-v1.8.2 e72f3e685a37 2 weeks ago 130MB goharbor/harbor-migrator v1.8.2 c11a64ae3a1e 2 weeks ago 361MB goharbor/redis-photon v1.8.2 18036ee471bc 2 weeks ago 107MB goharbor/clair-photon v2.0.8-v1.8.2 68de68a40e66 2 weeks ago 164MB goharbor/notary-server-photon v0.6.1-v1.8.2 90cf28ef3a84 2 weeks ago 135MB goharbor/notary-signer-photon v0.6.1-v1.8.2 e9b49ea8ed32 2 weeks ago 132MB goharbor/harbor-registryctl v1.8.2 ad798fd6e618 2 weeks ago 96.5MB goharbor/registry-photon v2.7.1-patch-2819-v1.8.2 081bfb3dc181 2 weeks ago 81.6MB goharbor/nginx-photon v1.8.2 1592a48daeac 2 weeks ago 36.2MB goharbor/harbor-log v1.8.2 42ad5ef672dd 2 weeks ago 81.8MB goharbor/harbor-jobservice v1.8.2 623ed0095966 2 weeks ago 119MB goharbor/harbor-core v1.8.2 03d6daab10c7 2 weeks ago 135MB goharbor/harbor-portal v1.8.2 41e264a7980b 2 weeks ago 43.2MB goharbor/harbor-db v1.8.2 927ecd68ee1f 2 weeks ago 144MB goharbor/prepare v1.8.2 b0d62cc7683d 2 weeks ago 145MB [root@VM_95_141_centos harbor]# docker pull 193.112.221.230/library/nginx:latest latest: Pulling from library/nginx Digest: sha256:099019968725f0fc12c4b69b289a347ae74cc56da0f0ef56e8eb8e0134fc7911 Status: Downloaded newer image for 193.112.221.230/library/nginx:latest 193.112.221.230/library/nginx:latest [root@VM_95_141_centos harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE wordpress latest fc03dc56d371 3 days ago 502MB 193.112.221.230/library/nginx latest 5a3221f0137b 9 days ago 126MB 193.112.221.230/mei_test/nginx v1.0 5a3221f0137b 9 days ago 126MB nginx-mei v1.0 5a3221f0137b 9 days ago 126MB nginx latest 5a3221f0137b 9 days ago 126MB mysql 5.6 732765f8c7d2 11 days ago 257MB goharbor/chartmuseum-photon v0.9.0-v1.8.2 e72f3e685a37 2 weeks ago 130MB goharbor/harbor-migrator v1.8.2 c11a64ae3a1e 2 weeks ago 361MB goharbor/redis-photon v1.8.2 18036ee471bc 2 weeks ago 107MB goharbor/clair-photon v2.0.8-v1.8.2 68de68a40e66 2 weeks ago 164MB goharbor/notary-server-photon v0.6.1-v1.8.2 90cf28ef3a84 2 weeks ago 135MB goharbor/notary-signer-photon v0.6.1-v1.8.2 e9b49ea8ed32 2 weeks ago 132MB goharbor/harbor-registryctl v1.8.2 ad798fd6e618 2 weeks ago 96.5MB goharbor/registry-photon v2.7.1-patch-2819-v1.8.2 081bfb3dc181 2 weeks ago 81.6MB goharbor/nginx-photon v1.8.2 1592a48daeac 2 weeks ago 36.2MB goharbor/harbor-log v1.8.2 42ad5ef672dd 2 weeks ago 81.8MB goharbor/harbor-jobservice v1.8.2 623ed0095966 2 weeks ago 119MB goharbor/harbor-core v1.8.2 03d6daab10c7 2 weeks ago 135MB goharbor/harbor-portal v1.8.2 41e264a7980b 2 weeks ago 43.2MB goharbor/harbor-db v1.8.2 927ecd68ee1f 2 weeks ago 144MB goharbor/prepare v1.8.2 b0d62cc7683d 2 weeks ago 145MB [root@VM_95_141_centos harbor]# 全文完。\n","description":"使用Harbor部署企业私有仓库，将一些核心镜像与软件与外部网络进行隔离与防护。","id":16,"section":"posts","tags":["Docker","Harbor","K8s"],"title":"07-Centos上部署Harbor私有仓库","uri":"https://laomeinote.com/posts/deploy-harbor-in-centos/"},{"content":"Docker Compose是什么？它是Docker的编排开源组件，后被Docker收购，是当年社区内火爆程度仅次于Docker的开源项目，Docker的三架马车之一，主要用来编排docker容器。本文将简述Docker compose的使用。\n1.Docker Compose组件 Compose 组件是Docker官方基于Python开发的Docker编排的组件。之前我们启动多个容器的时候，需要为每个容器编写dockerfile，分别使用docker run或者docker exec启动它们。通过Compose咱们就可以一次性在定义好多个容器，一次性启动，实现编排功能。它的两个重点内容：\ndocker-compose.yml： Compose的配置文件 docker-compose： Compose的命令行工具 1.1 Docker Compose安装与卸载 推荐直接下载二进制可执行文件。\n1 2 3 4 5 6 7 8 9 10 11 #从github下载1.25.4版本compose $ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m) \u0026#34; -o /usr/local/bin/docker-compose #如果国内下载速度慢，可到下面网址下载 $ sudo curl -L \u0026#34;https://get.daocloud.io/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m) \u0026#34; -o /usr/local/bin/docker-compose # 给 docker-compose 添加可执行权限 $ sudo chmod +x /usr/local/bin/docker-compose # 卸载 docker-compose $ sudo rm /usr/local/bin/docker-compose 1.2 使用Compose编排flask实例 这个实例中，我们使用使用创建一个flask demo应用，使用dockerfile编写flask应用镜像，使用Compose基于dockerfile创建的flask镜像拉取一个flask容器和其依赖的redis容器。\n来看下这个测试目录结构：\n1 2 3 4 5 6 7 8 9 10 11 [root@VM-95-141-centos flask-demo]# pwd /root/flask-demo [root@VM-95-141-centos flask-demo]# tree . |-- app.py |-- docker-compose.yml |-- dockerfile └── requirements.txt 0 directories, 4 files 在root/flask-demo下新建了4个文件，分别是app.py-flask源程序，requirements.txt-flask程序安装的依赖程序，dockerfile-构建flask镜像的dockerfile文件，docker-compose.yml-编排flask和redis容器的compose文件。\napp.py - flask源程序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import time import redis from flask import Flask app = Flask(__name__) cache = redis.Redis(host=\u0026#39;redis\u0026#39;, port=6379) def get_hit_count(): retries = 5 while True: try: return cache.incr(\u0026#39;hits\u0026#39;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5) @app.route(\u0026#39;/\u0026#39;) def hello(): count = get_hit_count() return \u0026#39;Hello World! I have been seen {} times.\\n\u0026#39;.format(count) requirements.txt-flask程序安装的依赖程序\n1 2 flask redis dockerfile-构建flask镜像的dockerfile文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #设置base镜像 FROM python:3.7-alpine #设置容器的工作目录 WORKDIR /code #设置环境变量 ENV FLASK_APP app.py ENV FLASK_RUN_HOST 0.0.0.0 #将当期目录下的python依赖文件copy至容器文件系统中 COPY requirements.txt requirements.txt COPY . . #设置镜像源，安装gcc,musl-dev和linux-headers RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk add --no-cache gcc musl-dev linux-headers\\ \u0026amp;\u0026amp; pip install -r requirements.txt CMD [\u0026#34;flask\u0026#34;, \u0026#34;run\u0026#34;] 从Python 3.7的alpine映像开始构建flask映像 将容器的工作目录设置为/code 设置flask命令使用的环境变量 复制requirements.txt并 将.项目中的当前目录复制到.映像中的工作目录/code 修改源以便快速下载安装gcc， musl-dev，linux-headers与Python依赖项 将容器的默认命令设置为flask run。 docker-compose.yml-编排flask和redis容器的compose文件\n1 2 3 4 5 6 7 8 version: \u0026#39;3\u0026#39; services: web: #构建web容器 build: . #基于当前目录下的dockfile构建镜像 ports: - \u0026#34;5000:5000\u0026#34; #隐射主机与容器端口，相当于容器的docker run -p redis: #构建redis容器 image: \u0026#34;redis:latest\u0026#34; #引用官网的redis镜像 在这里，通过docker-compose.yml文件，编排了两个容器，其中第一个web容器通过build命令对dockerfile文件进行构建生成flask镜像后启动容器；第二个redis容器通过官方redis镜像构建。\n通过docker-compose up启动两个容器即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 [root@VM-95-141-centos flask-demo]# docker-compose up Building web Step 1/8 : FROM python:3.7-alpine ---\u0026gt; f0c1a69798c7 Step 2/8 : WORKDIR /code ---\u0026gt; Using cache ---\u0026gt; d75b21d5a312 Step 3/8 : ENV FLASK_APP app.py ---\u0026gt; Using cache ---\u0026gt; 2a8e613c3372 Step 4/8 : ENV FLASK_RUN_HOST 0.0.0.0 ---\u0026gt; Using cache ---\u0026gt; e3af59840e84 Step 5/8 : COPY requirements.txt requirements.txt ---\u0026gt; Using cache ---\u0026gt; a6b832e8d8a3 Step 6/8 : COPY . . ---\u0026gt; c6bf499fd10c Removing intermediate container d3b237395dea Step 7/8 : RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; apk add --no-cache gcc musl-dev linux-headers \u0026amp;\u0026amp; pip install -r requirements.txt ---\u0026gt; Running in 0e6c2354c9d2 fetch https://mirrors.ustc.edu.cn/alpine/v3.14/main/x86_64/APKINDEX.tar.gz fetch https://mirrors.ustc.edu.cn/alpine/v3.14/community/x86_64/APKINDEX.tar.gz (1/13) Installing libgcc (10.3.1_git20210424-r2) (2/13) Installing libstdc++ (10.3.1_git20210424-r2) (3/13) Installing binutils (2.35.2-r2) (4/13) Installing libgomp (10.3.1_git20210424-r2) (5/13) Installing libatomic (10.3.1_git20210424-r2) (6/13) Installing libgphobos (10.3.1_git20210424-r2) (7/13) Installing gmp (6.2.1-r0) (8/13) Installing isl22 (0.22-r0) (9/13) Installing mpfr4 (4.1.0-r0) (10/13) Installing mpc1 (1.2.1-r0) (11/13) Installing gcc (10.3.1_git20210424-r2) (12/13) Installing linux-headers (5.10.41-r0) (13/13) Installing musl-dev (1.2.2-r3) Executing busybox-1.33.1-r6.trigger OK: 140 MiB in 48 packages Collecting flask Downloading Flask-2.0.2-py3-none-any.whl (95 kB) Collecting redis Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB) Collecting Jinja2\u0026gt;=3.0 Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB) Collecting click\u0026gt;=7.1.2 Downloading click-8.0.3-py3-none-any.whl (97 kB) Collecting itsdangerous\u0026gt;=2.0 Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB) Collecting Werkzeug\u0026gt;=2.0 Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB) Collecting importlib-metadata Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB) Collecting MarkupSafe\u0026gt;=2.0 Downloading MarkupSafe-2.0.1-cp37-cp37m-musllinux_1_1_x86_64.whl (30 kB) Collecting zipp\u0026gt;=0.5 Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB) Collecting typing-extensions\u0026gt;=3.6.4 Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB) Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, Werkzeug, Jinja2, itsdangerous, click, redis, flask Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 Werkzeug-2.0.2 click-8.0.3 flask-2.0.2 importlib-metadata-4.8.2 itsdangerous-2.0.1 redis-3.5.3 typing-extensions-3.10.0.2 zipp-3.6.0 WARNING: Running pip as the \u0026#39;root\u0026#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available. You should consider upgrading via the \u0026#39;/usr/local/bin/python -m pip install --upgrade pip\u0026#39; command. ---\u0026gt; 5aa794729c32 Removing intermediate container 0e6c2354c9d2 Step 8/8 : CMD flask run ---\u0026gt; Running in 77721b2fb1c6 ---\u0026gt; 71de094f6563 Removing intermediate container 77721b2fb1c6 Successfully built 71de094f6563 WARNING: Image for service web was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`. Creating flask-demo_redis_1 ... done Creating flask-demo_web_1 ... done Attaching to flask-demo_redis_1, flask-demo_web_1 redis_1 | 1:C 14 Nov 2021 14:02:30.930 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis_1 | 1:C 14 Nov 2021 14:02:30.930 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started redis_1 | 1:C 14 Nov 2021 14:02:30.930 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis_1 | 1:M 14 Nov 2021 14:02:30.930 * monotonic clock: POSIX clock_gettime redis_1 | 1:M 14 Nov 2021 14:02:30.931 * Running mode=standalone, port=6379. redis_1 | 1:M 14 Nov 2021 14:02:30.931 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis_1 | 1:M 14 Nov 2021 14:02:30.931 # Server initialized redis_1 | 1:M 14 Nov 2021 14:02:30.931 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and then reboot or run the command \u0026#39;sysctl vm.overcommit_memory=1\u0026#39; for this to take effect. redis_1 | 1:M 14 Nov 2021 14:02:30.938 * Ready to accept connections web_1 | * Serving Flask app \u0026#39;app.py\u0026#39; (lazy loading) web_1 | * Environment: production web_1 | WARNING: This is a development server. Do not use it in a production deployment. web_1 | Use a production WSGI server instead. web_1 | * Debug mode: off web_1 | * Running on all addresses. web_1 | WARNING: This is a development server. Do not use it in a production deployment. web_1 | * Running on http://172.19.0.3:5000/ (Press CTRL+C to quit) 测试一下：\n1 2 3 4 5 6 7 [root@VM-95-141-centos flask-demo]# curl 172.19.0.3:5000 Hello World! I have been seen 1 times. [root@VM-95-141-centos flask-demo]# curl 172.19.0.3:5000 Hello World! I have been seen 2 times. [root@VM-95-141-centos flask-demo]# curl 172.19.0.3:5000 Hello World! I have been seen 3 times. [root@VM-95-141-centos flask-demo]# 全文完。\n","description":"Docker Compose是Docker的编排开源组件，Docker的三架马车之一，主要用来编排docker容器。本文将简述Docker compose的使用。","id":17,"section":"posts","tags":["Docker","Docker-Compose"],"title":"06-容器编排之Docker-Compose","uri":"https://laomeinote.com/posts/docker-compose/"},{"content":"容器网络比较复杂，目前主流的网络方案是以VXLAN为主的Overlay方案，关于VXLAN的理解，可以参考这篇文章《什么是VXLAN》。\n这篇文章里，我们将容器网络分为单节点上的容器网络通信和跨节点网络通信两部分内容。\n1.单Host容器网络 在单个host主机上Docker的网络模型有四种：None网络、Host网络、Bridge网络和User-defined网络。\nNone网络：单机版容器，没有网络，适用场景为无需联网的业务，比如生成随机码业务，用法为指定参数--network=none。\nHost网络：连接到Host主机上，完全与主机共享网络，用法为指定参数--network=host，由于直接共享主机网络无需转发，所以性能比桥接网络性能要好，但是要考虑到不能与主机端口冲突。\nBridge网络：桥接网络，Docker的默认网络模型，通过桥转发网络流量，无需指定参数默认使用桥接网络。\nDocker默认会生成一个docker0的网桥，可看成一个交换机。\n1 2 3 [root@ecs_lm_test ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024250df460d no 在主机上查看linux bridge桥信息，可见默认有一个docker0的桥，在没有容器启动的时候这个桥（理解成一个主机上的软交换机）上interfaces为空。启动一个busybox容器，不指定网络则默认使用网桥网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [root@ecs_lm_test ~]# docker run -it busybox:v1.0 #启动一个busybox容器 / # ip a #在容器中查看IP地址 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 21: eth0@if22: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever / # [root@ecs_lm_test ~]# brctl show #在主机上查看linux网桥 bridge name bridge id STP enabled interfaces docker0 8000.024250df460d no vetha2b8732 [root@ecs_lm_test ~]# ifconfig docker0 #在主机上查看linux网桥配置IP docker0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 0.0.0.0 inet6 fe80::42:50ff:fedf:460d prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:42:50:df:46:0d txqueuelen 0 (Ethernet) RX packets 57 bytes 3884 (3.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 656 (656.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@ecs_lm_test ~]# 此时，网桥（Host主机上的一个软交换机）上生成一个接口vetha2b8732，网桥的地址为172.17.0.1/16，容器内生成一个接口 eth0@if22，地址为 172.17.0.2/16。其详细拓扑类似于下图示。\ndocker0网桥是在Docker安装完成后已经生成的，其默认地址为172.17.0.1/16。启动bubybox容器且默认使用桥接网络时，docker会给这个容器生成一个端口eth0，如上图中的eth0@if22,容器默认从172.17.0.0/16网段里分配一个IP给该端口，如本示例中分配的172.17.0.2/16;同时生成一个veth，如上图中的vetha2b8732。veth是一个键值对，连接网桥与容器端口。容器的网络流量均将通过这个网桥转给Host主机。\nUser-defined网络：用户自定义网络，也是桥接网络的一种，通过桥转发。用法分两步：1）创建一个桥接网络network，如my_net；2）指定网络为自定义的桥接网络--network=my_net。如下示例：\n1 2 $sudo docker network create --driver bridge my_net #创建自定义桥接网络my_net $sudo docker run -it --network=my_net --name=busybox_2 busybox:v1.0 #创建一个容器busybox_2, 指定网络为my_net 创建完网络以后可以查看一下自定义网络信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 [root@ecs_lm_test ~]# brctl show bridge name bridge id STP enabled interfaces br-af0983622432 8000.024250455fc5 no veth9c4e5f0 docker0 8000.024250df460d no vetha2b8732 [root@ecs_lm_test ~]# docker network ls NETWORK ID NAME DRIVER SCOPE dc6b5efa55b1 bridge bridge local bd6f8b541421 host host local 84dbdab42e7e my_net bridge local 8477f439c570 none null local [root@ecs_lm_test ~]# docker network inspect my_net [ { \u0026#34;Name\u0026#34;: \u0026#34;my_net\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;84dbdab42e7e1b57d304fa9f30cc0c2b1257af4a249d838ffd50516a4c28ee50\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2021-11-17T19:09:01.483500585+08:00\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] [root@ecs_lm_test ~]# ifconfig br-af0983622432 br-af0983622432: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 0.0.0.0 ether 02:42:50:45:5f:c5 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@ecs_lm_test ~]# docker exec -it busybox_2 /bin/sh / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 26: eth0@if27: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever / # 可见创建了一个名为br-af0983622432的、network名为my_net的桥接网络，其子网IP为172.18.0.0/16，网关IP为172.18.0.1/16，网关在网桥自身上。且Docker为容器busybox_2分配了一个接口eth0@if27对应IP为172.18.0.2/16;键值对为veth9c4e5f0，连接容器与网桥，其网络架构如下图示。\n上面的示例中自定义网桥没有指定网段，默认使用172.18.0.0/16网段，也可指定特定网段如172.19.0.0/16.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 [root@ecs_lm_test ~]# docker network create --driver bridge --subnet 172.19.0.0/16 --gateway 172.19.0.1 my_net2 3182a1c4c328ae30c6a98cbb1148924585f6975531bc3a56718d9fcb8250f3b9 [root@ecs_lm_test ~]# docker network ls #可见生成了一个my_net2的网络 NETWORK ID NAME DRIVER SCOPE dc6b5efa55b1 bridge bridge local bd6f8b541421 host host local af0983622432 my_net bridge local 3182a1c4c328 my_net2 bridge local 8477f439c570 none null local [root@ecs_lm_test ~]# docker run -it --name=busybox_3 --network=my_net2 busybox:v1.0 #启动一个容器，指定网络为my_net2 / # ip a #在容器中查看其IP地址 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 29: eth0@if30: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff inet 172.19.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe13:2/64 scope link valid_lft forever preferred_lft forever / # [root@ecs_lm_test ~]# brctl show #在主机上查看可见生成了一个网桥名为br-3182a1c4c328，键值对veth9d6f750 bridge name bridge id STP enabled interfaces br-3182a1c4c328 8000.02421bd7d910 no veth9d6f750 br-af0983622432 8000.024250455fc5 no docker0 8000.024250df460d no vetha2b8732 [root@ecs_lm_test ~]# docker network inspect my_net2 #在主机上查看my_net2网络信息 [ { \u0026#34;Name\u0026#34;: \u0026#34;my_net2\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;3182a1c4c328ae30c6a98cbb1148924585f6975531bc3a56718d9fcb8250f3b9\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2021-11-18T09:38:28.214281939+08:00\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.19.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.19.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;6d854882025fa02dd7e79b7a3258fd21e6ad38d9888fc414fcd79306b7920a44\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;busybox_3\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;a40fd9865bbcbbb72cc676c407f9b2ed5c496d7b1d56ccbb0dcbf1a9ed0c8a4a\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:13:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.19.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] 可见在主机上创建了一个my_net2的自定义网络，网桥名为my_net2，指定容器以该网桥启动后，Docker生成了一个键值对veth9d6f750，整个网络的地址段为172.19.0.0/16，整体网络架构如下图示。\n显然这三个容器不在同一个网桥上是互相隔离，无法互通的，如果需要互通怎么办，比如讲容器2与容器3打通网络？可为容器busybox_2再加一个my_net2的网卡即可，可使用命令docker network connect [network] [container name/id] 将容器2连接到网桥2上来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@ecs_lm_test ~]# docker network connect my_net2 busybox_2 [root@ecs_lm_test ~]# docker exec -it busybox_2 /bin/sh / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 33: eth0@if34: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever 35: eth1@if36: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:13:00:03 brd ff:ff:ff:ff:ff:ff inet 172.19.0.3/16 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe13:3/64 scope link valid_lft forever preferred_lft forever [root@ecs_lm_test ~]# brctl show bridge name bridge id STP enabled interfaces br-3182a1c4c328 8000.02421bd7d910 no veth6f2fed5 veth9d6f750 br-81a7cff59d14 8000.0242576559e7 no vethecf3979 docker0 8000.024250df460d no vetha2b8732 这里可见已经新生产一个veth键值对，容器端的端口为eth1@if36，地址为172.19.0.3/16。此时的网络架构如下：\n从容器busybox_2中往容器busybox_3做ping测：\n2.容器通信 容器可通过IP,DNS,Joined容器三种方式进行通信。\nIP通信：IP只要能够互通即可，如上面busybox_2与busybox_3之间使用IP进行通信。\nDNS：在使用了user-defined的桥接网络的容器，可以直接使用容器名进行通信。\n1 2 3 4 5 [root@ecs_lm_test ~]# docker exec -it busybox_2 sh / # ping busybox_3 PING busybox_3 (172.19.0.2): 56 data bytes 64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.068 ms 64 bytes from 172.19.0.2: seq=1 ttl=64 time=0.097 ms 需要注意的是，这里只适用于user-defined的桥接网络，默认桥接网络不行。\nJoined容器：即多个容器共享一个网络栈，共享MAC，IP等。可使用--network=container:[已有容器name/ID]来指定新容器，如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@ecs_lm_test ~]#docker run -it --name=busybox_4 --network=container:busybox_3 busybox:v1.0 / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 29: eth0@if30: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff inet 172.19.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe13:2/64 scope link valid_lft forever preferred_lft forever / # 可见容器busybox_4与容器busybox_3的网络是完全一致的。\n3.容器与外部网络互通 这里分两部分来看，首先是容器访问外部网络，其次是外部网络访问容器。\n容器访问外部网络：容器网络与外部互通以后，容器可以访问外部，这里网络类型其实只有两种：\nHost类型：容器直接通过Host转发流量。 Bridge类型：无论是Docker默认网络类型还是自定义类型，均是桥接网络，其访问外部网络一般分为四个步骤： 1）容器发起外部访问 2）网桥（如默认的docker0网络和自定义的bridge网络）转发给主机host 3）主机host通过NAT转换源地址为host本机地址 4）host主机发起外部访问 外部网络访问容器：外部网络访问容器指的是在IP可达之后，容器提供对外服务，外部通过IP网络访问该服务，如容器提供一个Nginx服务，外部通过IP访问到该服务。在对外服务中，需要将服务端口暴露隐射至主机端口，主要包含以下几个步骤：\n1）容器中提供对外服务，对外暴露服务端口xxx，如8090 2）启动容器时将主机端口8091绑定到容器端口8090，docker run -p 8091:8090 [其他参数] 3）docker-proxy进程会监听主机端口8091 4）当外部访问到主机的8091端口时，主机转发至容器8090端口 5）容器对外服务端口接收到流量后服务程序开始处理 4.跨Host网络 Docker跨Host有包括原生的overlay和macvlan方案和多种第三方方案，如下图所示。\n其中原生方案中用的较多的是overlay方案，其基本原理是在本端隧道起点VTEP 1将二层网络数据封装在三层UDP数据中通过隧道技术传输，在对端隧道终点VTEP 2上解封装，通过三层路由来打通二层网络，与GRE隧道类似，形成大二层网络。\n原生MACVLAN方案本质就是传统三层网络互通方案，性能比overlay要好，但是局限于二层网络VLAN的规模。\n除Docker原生网络方案以外，还有三个常见的第三方网络方案flannel、weave和calico。这里重点记录一下前两种flannel和weave方案。\n4.1 flannel方案 flannel是CoreOS公司开发的跨主机通信网络解决方案，它会为每个host分配一个subnet，容器从这个subnet中获取IP地址，这个IP地址在各个host主机组成的集群中是全局唯一的，其框架如下：\n每个节点上有一个叫flanneld的agent，负责为每个主机分配和管理子网； 全局的网络配置存储etcd负责存储主机容器子网的映射关系； 数据包在主机之间发展是由backend来实现的，常见的backend有VXLAN和host-gw两种模式。 4.1.1 flannel数据转发VXLAN模式 关于VXLAN的本质用一句话概括，就是将三层网络数据封装在虚拟二层网络中，通过二层互通实现三层网络不同网段IP互通，同时突破了同一个VLAN只有2049个子网的限制，VXLAN使用VNI（24 bit)来标识VXLAN网络，故而有(2^24)个子网。\n下面转自水立方在掘金博文：《Flannel的两种模式解析（VXLAN、host-gw)》\nVXLAN模式是Flannel默认和推荐的模式，使用VXLAN模式时，它会为每个节点分配一个24位子网 。flanneld会在宿主机host上创建一个 VTEP 设备（flannel.1）和一个网桥cni0。（flannel.1）就是VXLAN隧道的起/始点，VNI=1，实现对VXLAN报文的解封装。\n来看看跨节点Node1和Node2之间的容器互通式如何通信的？\n发送端Node1：在Node1的PodA（假设含一个容器）中发起对Node2的PodB（假设含一个容器）的ping测 ping 10.244.1.21 ，ICMP 报文经过 cni0 网桥后交由 flannel.1 设备处理。 flannel.1 设备是一个VXLAN类型的设备，负责VXLAN封包解包。 因此，在发送端，flannel.1 将原始L2报文封装成VXLAN UDP报文，然后从 eth0 发送。 接收端：Node2收到UDP报文，发现是一个VXLAN类型报文，交由 flannel.1 进行解包。根据解包后得到的原始报文中的目的IP，将原始报文经由 cni0 网桥发送给PodB。 哪些IP要交由 flannel.1 处理 flanneld 从 etcd 中可以获取所有节点的子网情况，以此为依据为各节点配置路由，将属于非本节点的子网IP都路由到 flannel.1 处理，本节点的子网路由到 cni0 网桥处理。\n1 2 3 4 5 [root@Node1 ~]# ip r ... 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 # Node1子网为10.224.0.0/24， 本机PodIP都交由cni0处理 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink # Node2子网为10.224.1.0/24，Node2的PodID都交由flannel.1处理 ... 如果节点信息有变化， flanneld 也会从etcd中同步更新路由信息。\nflannel.1的封包过程 VXLAN的封包是将二层以太网帧封装到四层UDP报文中的过程。\n原始L2帧\n要生成原始的L2帧， flannel.1 需要得知：\n内层源/目的IP地址\n内层源/目的MAC地址\n内层的源/目的IP地址是已知的，即为PodA/PodB的PodIP，在图例中，分别为10.224.0.20和10.224.1.20。 内层源/目的MAC地址要结合路由表和ARP表来获取。根据路由表①得知：\n1)下一跳地址是10.224.1.0，关联ARP表②，得到下一跳的MAC地址，也就是目的MAC地址：Node2_flannel.1_MAC； 2)报文要从 flannel.1 虚拟网卡发出，因此源MAC地址为 flannel.1 的MAC地址。 要注意的是，这里ARP表的表项②并不是通过ARP学习得到的，而是 flanneld 预先为每个节点设置好的，由 flanneld负责维护，没有过期时间。\n1 2 3 # 查看ARP表 [root@Node1 ~]# ip n | grep flannel.1 10.244.1.0 dev flannel.1 lladdr ba:74:f9:db:69:c1 PERMANENT # PERMANENT 表示永不过期 有了上面的信息， flannel.1 就可以构造出内层的2层以太网帧：\n外层VXLAN UDP报文\n要将原始L2帧封装成VXLAN UDP报文， flannel.1 还需要填充源/目的IP地址。我们知道VTEP是VXLAN隧道的起点或终点。因此，目的IP地址即为对端VTEP的IP地址，通过FDB表获取。在FDB表③中，dst字段表示的即为VXLAN隧道目的端点（对端VTEP）的IP地址，也就是VXLAN DUP报文的目的IP地址。FDB表也是由 flanneld 在每个节点上预设并负责维护的。\nFDB表（Forwarding database）用于保存二层设备中MAC地址和端口的关联关系，就像交换机中的MAC地址表一样。在二层设备转发二层以太网帧时，根据FDB表项来找到对应的端口。例如cni0网桥上连接了很多veth pair网卡，当网桥要将以太网帧转发给Pod时，FDB表根据Pod网卡的MAC地址查询FDB表，就能找到其对应的veth网卡，从而实现联通。\n可以使用 bridge fdb show 查看FDB表：\n1 2 [root@Node1 ~]# bridge fdb show | grep flannel.1 ba:74:f9:db:69:c1 dev flannel.1 dst 192.168.50.3 self permanent 源IP地址信息来自于 flannel.1 网卡设置本身，根据 local 192.168.50.2 可以得知源IP地址为192.168.50.2。\n1 2 3 4 5 6 7 8 [root@Node1 ~]# ip -d a show flannel.1 6: flannel.1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 32:02:78:2f:02:cb brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 1 local 192.168.50.2 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 noudpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.244.0.0/32 brd 10.244.0.0 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::3002:78ff:fe2f:2cb/64 scope link valid_lft forever preferred_lft forever 至此， flannel.1 已经得到了所有完成VXLAN封包所需的信息，最终通过 eth0 发送一个VXLAN UDP报文：\n即：把原始L2数据包封装在了UDP报文中，外层IP可二层直通，二层互通以后再VTEP端点进行VXLAN解封装后进行交互。\nFlannel的VXLAN模式通过静态配置路由表，ARP表和FDB表的信息，结合VXLAN虚拟网卡 flannel.1 ，实现了所有容器同属一个大二层网络的VXLAN网络模型。\n4.1.2 Flannel数据转发之host-gw模式 在上述的VXLAN的示例中，Node1和Node2其实是同一物理宿主机中的两台使用桥接模式的VM虚机，也就是说它们在一个二层网络中。在二层网络互通的情况下，直接配置节点的三层路由即可互通，不需要使用VXLAN隧道。要使用host-gw模式，在Kubernets中需要修改 ConfigMap kube-flannel-cfg ，将 Backend.Type 从vxlan改为host-gw，然后重启所有kube-flannel Pod即可：\n1 2 3 4 5 6 7 8 9 ... net-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;host-gw\u0026#34; // \u0026lt;- 改成host-gw } } ... 如果是docker直接使用Flannel，也需要修改类似的配置，不过是在文件flannel-config.json中直接修改，然后更新etcd数据库。\nhost-gw模式下的通信过程如下图所示：\n在host-gw模式下，由于不涉及VXLAN的封包解包，不再需要flannel.1虚机网卡。 flanneld 负责为各节点设置路由 ，将对应节点Pod子网的下一跳地址指向对应的节点的IP，如图中路由表①所示。\n1 2 3 4 5 [root@Node1 ~]# ip r ... 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 10.244.1.0/24 via 192.168.50.3 dev eth0 # Node2子网的下一跳地址指向Node2的public ip。 ... 由于没有封包解包带来的消耗，host-gw是性能最好的。不过一般在云环境下，都不支持使用host-gw的模式，在私有化部署的场景下，可以考虑。\n4.2 weave方案 weave创建的虚拟网络可以将部署在多个机器的容器连接起来。对容器来说，weave就像一个巨大的以太网交换机，所有容器都会接入这个交换机，容器可以直接通信，无需NAT和端口映射。\n默认配置下，weave使用一个大subnet（10.32.0.0/12），所有主机的容器都从这个地址空间中分配IP，因为同属一个subnet，weave网络中的所有容器可以直接通信。\n参考《每天5分钟玩转docker技术》的weave网络章节，先做个实验跑起来。\n安装完weave后在 host1中运行容器 busybox_weave：\n1 2 3 $sudo eval $(weave env) $sudo docker run --name busybox_weave -it -d busybox:v1.0 执行 eval $(weave env) 的作用是将后续的 docker 命令发给 weave proxy 处理，在此基础上的容器使用的网络模型就是weave。如果要恢复之前的环境，可执行 eval $(weave env --restore)。查看一下当前容器网络环境：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [root@ecs_lm_test ~]# docker exec -it busybox_weave ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 45: eth0@if46: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever 47: ethwe@if48: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether 1a:17:64:b9:ed:cb brd ff:ff:ff:ff:ff:ff inet 10.32.0.1/12 brd 10.47.255.255 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::1817:64ff:feb9:edcb/64 scope link valid_lft forever preferred_lft forever [root@ecs_lm_test ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024250df460d no vetha2b8732 weave 8000.5e0a88e58f54 no vethwe-bridge vethwepl6144 [root@ecs_lm_test ~]# ip -d link |grep datapath 37: datapath: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1376 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 42: vethwe-datapath@vethwe-bridge: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1376 qdisc noqueue master datapath state UP mode DEFAULT group default 43: vethwe-bridge@vethwe-datapath: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 44: vxlan-6784: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 65535 qdisc noqueue master datapath state UNKNOWN mode DEFAULT group default qlen 1000 [root@ecs_lm_test ~]# ip -d link |grep vxlan 44: vxlan-6784: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 65535 qdisc noqueue master datapath state UNKNOWN mode DEFAULT group default qlen 1000 vxlan id 0 srcport 0 0 dstport 6784 nolearning ageing 300 udpcsum noudp6zerocsumtx udp6zerocsumrx external 可见容器生成了，eth0@if46和ethwe@if48，且除了默认docker0网桥以外，多出了weave网桥。其网络模型如下截图示：\nweave 网络包含两个虚拟交换机：Linux bridge weave 和 Open vSwitch datapath，veth pair vethwe-bridge 和 vethwe-datapath 将二者连接在一起。\nweave 和 datapath 分工不同，weave 负责将容器接入 weave 网络，datapath 负责在主机间 VxLAN 隧道中并收发数据。\n① vethwe-bridge 与 vethwe-datapath 是 veth pair，连接weave网络与datapath网络。\n② vethwe-datapath 的父设备（master）是 datapath。\n③ datapath 是一个 openvswitch。\n④ vxlan-6784 是 vxlan interface，其 master 也是 datapath，weave 主机间是通过 VxLAN 通信的，就是VTEP起始端点，通过主机eth0转发流量。\n再新增一个容器busybox_weave2：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [root@ecs_lm_test tools]# docker run --name busybox_weave2 -it -d busybox:v1.0 b11b0d74a6190823edab79196cdb40179ecf20c71c66c727848a7960633d1f18 [root@ecs_lm_test tools]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b11b0d74a619 busybox:v1.0 \u0026#34;/w/w sh\u0026#34; 5 seconds ago Up 4 seconds busybox_weave2 55281c9e3e78 busybox:v1.0 \u0026#34;/w/w sh\u0026#34; 9 hours ago Up 9 hours busybox_weave 1c2e129594ca weaveworks/weave:latest \u0026#34;/home/weave/weave...\u0026#34; 42 hours ago Up 42 hours weave b65a38bf9fe2 weaveworks/weaveexec:latest \u0026#34;data-only\u0026#34; 42 hours ago Created weavevolumes-latest 3213246d832c weaveworks/weavedb:latest \u0026#34;data-only\u0026#34; 42 hours ago Created weavedb [root@ecs_lm_test tools]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024250df460d no veth265cf07 vetha2b8732 weave 8000.5e0a88e58f54 no vethwe-bridge vethwepl12236 vethwepl6144 [root@ecs_lm_test tools]# docker exec -it busybox_weave2 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 49: eth0@if50: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:04 brd ff:ff:ff:ff:ff:ff inet 172.17.0.4/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:4/64 scope link valid_lft forever preferred_lft forever 51: ethwe@if52: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether ba:6c:86:1b:e8:81 brd ff:ff:ff:ff:ff:ff inet 10.32.0.2/12 brd 10.47.255.255 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::b86c:86ff:fe1b:e881/64 scope link valid_lft forever preferred_lft forever [root@ecs_lm_test tools]# 可见docker0网桥上新生成了接口veth265cf07,weave网桥上新生成了vethwepl12236，对应组br网架构如下图：\n从容器bubybox_weave往容器bubybox_weave2和Host主机端口eth0做ping测试试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 [root@ecs_lm_test tools]# docker exec -it 55281c9e3e78 sh #进入容器bubybox_weave / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 45: eth0@if46: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever 47: ethwe@if48: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether 1a:17:64:b9:ed:cb brd ff:ff:ff:ff:ff:ff inet 10.32.0.1/12 brd 10.47.255.255 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::1817:64ff:feb9:edcb/64 scope link valid_lft forever preferred_lft forever / # ping 10.32.0.2 #ping测容器2的ethwe端口 PING 10.32.0.2 (10.32.0.2): 56 data bytes 64 bytes from 10.32.0.2: seq=0 ttl=64 time=0.232 ms 64 bytes from 10.32.0.2: seq=1 ttl=64 time=0.102 ms ^C --- 10.32.0.2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.102/0.167/0.232 ms / # ping 172.17.0.4 #ping测容器2的eth0端口 PING 172.17.0.4 (172.17.0.4): 56 data bytes 64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.132 ms 64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.077 ms ^C --- 172.17.0.4 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss / # ping 192.168.54.150 #ping测主机的eth0端口 PING 192.168.54.150 (192.168.54.150): 56 data bytes 64 bytes from 192.168.54.150: seq=0 ttl=64 time=0.116 ms 64 bytes from 192.168.54.150: seq=1 ttl=64 time=0.070 ms ^C --- 192.168.54.150 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.070/0.093/0.116 ms / # 从容器1分别去对容器2和主机端口做ping测，结果都能ping通，因为weave和datapath已经将这部分网络打通。\n上面的用例是在同一个主机上的多个容器，如果是跨Host主机之间的容器，weave是如何处理的呢？\n多Host主机之间通过Weave通信\nweave在多主机之间通过vxlan传输数据，VTEP起始点分别由主机的datapath这个openVSwith生成，对应vxlan-xxx端口，通过一个实例理解一下。\n加入weave网络的主机必须是同一个二层互通的网络，上面的主机是在云上建立的，第一个host主机VPC子网是192.168.32.0/19，再新建一个主机加入到相同子网中。两个主机的IP地址分别为192.168.54.150/19和192.168.54.21/19。\nhost2必须指定host1的IP 192.168.54.150，这样host1和host2才能加入到同一个weave网络。\n1 2 3 4 5 6 7 8 9 [root@ecs_lm_test2 myimages]# eval $(weave env) [root@ecs_lm_test2 myimages]# weave launch 195.168.54.150 cannot locate running docker daemon Warning: unable to detect proxy TLS configuration. To enable TLS, launch the proxy with \u0026#39;weave launch\u0026#39; and supply TLS options. To suppress this warning, supply the \u0026#39;--no-detect-tls\u0026#39; option. d7643e59716e4a6f861f7e89a9819f7d76640474604dbd087e62b8f413758618 [root@ecs_lm_test2 myimages]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.02426891e1ee no weave 8000.625ddc22c535 no vethwe-bridge 可见主机已经生成一个weave网桥，和一个接口vethwe-bridge。\n创建一个容器加入weave网络。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 [root@ecs_lm_test2 ~]# docker run --name busybox_weave_host2_1 -it -d busybox:v1.0 1355472f761a5a50400522602c29eb1e60e81d5923800f200bf1fd4d7651722e [root@ecs_lm_test2 ~]# [root@ecs_lm_test2 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1355472f761a busybox:v1.0 \u0026#34;sh\u0026#34; 25 seconds ago Up 24 seconds busybox_weave_host2_1 d7643e59716e weaveworks/weave:latest \u0026#34;/home/weave/weave...\u0026#34; 14 minutes ago Up 14 minutes weave c0799cfead90 weaveworks/weaveexec:latest \u0026#34;data-only\u0026#34; 14 minutes ago Created weavevolumes-latest a6ce55a36584 weaveworks/weavedb:latest \u0026#34;data-only\u0026#34; 14 minutes ago Created weavedb [root@ecs_lm_test2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.02426891e1ee no veth9b93012 weave 8000.625ddc22c535 no vethwe-bridge vethwepl23496 [root@ecs_lm_test2 ~]# docker exec -it busybox_weave_host2_1 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 24: eth0@if25: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever 26: ethwe@if27: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether c2:62:6f:e0:5f:c3 brd ff:ff:ff:ff:ff:ff inet 10.44.0.0/12 brd 10.47.255.255 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::c062:6fff:fee0:5fc3/64 scope link valid_lft forever preferred_lft forever [root@ecs_lm_test2 ~]# ip a |grep eth0 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 192.168.54.21/19 brd 192.168.63.255 scope global noprefixroute dynamic eth0 [root@ecs_lm_test2 ~]# [root@ecs_lm_test2 ~]# docker exec -it busybox_weave_host2_1 ping 10.32.0.1 PING 10.32.0.1 (10.32.0.1): 56 data bytes 64 bytes from 10.32.0.1: seq=0 ttl=64 time=1.611 ms 64 bytes from 10.32.0.1: seq=1 ttl=64 time=0.669 ms ^C --- 10.32.0.1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.669/1.140/1.611 ms [root@ecs_lm_test2 ~]# docker exec -it busybox_weave_host2_1 ping 10.32.0.2 PING 10.32.0.2 (10.32.0.2): 56 data bytes 64 bytes from 10.32.0.2: seq=0 ttl=64 time=1.496 ms ^C --- 10.32.0.2 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 1.496/1.496/1.496 ms [root@ecs_lm_test2 ~]# 主机2容器busybox_weave_host2_1分配到的地址为10.44.0.0/12，跟主机1的容器地址10.32.0.1/12同属于同一个子网段10.32.0.1/12-10.47.255.255/12,通过host1与host2直接的vxlan隧道，三个容器逻辑上在同一个大二层网络中，所以能够互通，对应网络架构图如下。\nWeave网络隔离\n从上面的几个例子中可见加入到同一个weave网络的所有容器默认可以互通，如果想要进行网络隔离呢？只需在启动容器时通过命令-e WEAVE_CIDR=net:[ip-subnet]指定非默认subnet即可，这样的话容器不会从默认的10.32.0.1/12中分配地址，如10.32.2.0/24子网不跟10.32.0.1/12一个网段，给容器指定子subnet：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [root@ecs_lm_test2 ~]# docker run -e WEAVE_CIDR=net:10.32.2.0/24 -it -d busybox:v1.0 6b939744f25afa2d128e2714c764e2314fbf0907e2e04b7e84c9fb63ae480170 [root@ecs_lm_test2 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6b939744f25a busybox:v1.0 \u0026#34;/w/w sh\u0026#34; 5 seconds ago Up 4 seconds brave_dijkstra 91c9295c24b4 busybox:v1.0 \u0026#34;/w/w sh\u0026#34; 18 minutes ago Up 18 minutes busybox_weave_host2_1 7c619bb95ff8 weaveworks/weave:latest \u0026#34;/home/weave/weave...\u0026#34; 19 minutes ago Up 19 minutes weave c0799cfead90 weaveworks/weaveexec:latest \u0026#34;data-only\u0026#34; 47 minutes ago Created weavevolumes-latest a6ce55a36584 weaveworks/weavedb:latest \u0026#34;data-only\u0026#34; 47 minutes ago Created weavedb [root@ecs_lm_test2 ~]# docker exec -it 6b939744f25a sh / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 28: eth0@if29: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever 30: ethwe@if31: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether ea:f5:cb:cb:41:47 brd ff:ff:ff:ff:ff:ff inet 10.32.2.128/24 brd 10.32.2.255 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::e8f5:cbff:fecb:4147/64 scope link valid_lft forever preferred_lft forever / # ip r default via 172.17.0.1 dev eth0 10.32.2.0/24 dev ethwe scope link src 10.32.2.128 172.17.0.0/16 dev eth0 scope link src 172.17.0.3 224.0.0.0/4 dev ethwe scope link / # ping 10.32.0.1 PING 10.32.0.1 (10.32.0.1): 56 data bytes ^C --- 10.32.0.1 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss 参考文档 《每天5分钟玩转Docker容器技术》\n《十分钟漫谈容器网络方案01-Flannel》\n《理解flannel：概览》\n《Flannel的两种模式解析（VXLAN、host-gw)》\n全文完。\n","description":"目前主流的容器网络方案是以VXLAN为主的Overlay方案，本文简单描述单Host下与多跨Host场景的容器网络架构与通信方式。","id":18,"section":"posts","tags":["Docker","VXLAN"],"title":"05-Docker网络","uri":"https://laomeinote.com/posts/docker-network/"},{"content":"Docker提供两种数据存储资源，一种是基于storage driver提供的镜像层和容器层，一种是基于data volume提供的持久化存储。\n图1 Docker存储框架\n图1 描述了Docker存储的整体框架，它主要支持两种类型存储，一种是storage driver管理的镜像层与容器层，对现有容器数据的读写；一种是data volume持久化存储，提供bind volume和docker managed volume存储。\nStorage driver 我们知道容器镜像是按照多个层级来分层叠加的，如下图2示意，容器包含最上层的读写层与下面多个只读镜像层，当需要修改容器数据的时候，会将只读层数据COPY至容器层进行修改，修改后数据保存在容器层，镜像层不变，即使用了linux的Copy on write特性。\n图2 存储：镜像层与容器层-图片出自网络\n这种数据存储的生命周期当然会随着容器的销毁而结束，如果想在容器销毁后依然保存数据，该怎么办呢，答案是持久化存储-data volume。\nData volume Data volume提供两种类型存储。一种是bind volume，另一种是docker managed volume。前者简单理解就是将host主机目录共享给容器，容器注销了主机还在，所以数据当然不会丢失（除非主机故障或者数据丢失）。后者也是挂载主机目录到容器，不过是隐形挂载。举几个示例简单说明。\n使用-v [host path]:/[container path]进行bind volume显示挂载\n在/root/test目录下新建了一个test.txt文档 1 2 3 4 $sudo pwd /root/test $sudo ls test.txt 使用显示挂载将其隐射给容器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $sudo docker run -it -d -v /root/test:/usr/home/test alpine:x86-3.11.5 adc809b503883f6cd91f30343a721c12a915e7cdb79ed1197d677bc705c23160 $sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES adc809b50388 alpine:x86-3.11.5 \u0026#34;/bin/sh\u0026#34; 14 seconds ago Up 14 seconds flamboyant_shirley $sudo docker exec -it adc809b50388 /bin/sh / # / # ls -l /usr/home/ total 4 drwxr-xr-x 2 root root 4096 Nov 17 01:13 test / # ls -l /usr/home/test/ total 0 -rw-r--r-- 1 root root 0 Nov 17 01:13 test.txt / # cat /usr/home/test/test.txt / # 上面示例中，首先基于alpine镜像启动了一个容器，通过显示挂载-v /root/test:/usr/home/test将主机的/root/test目录挂载到容器的/usr/home/test目录下，如果镜像中默认没有该目录，则会新建一个这样的目录；docker创建了一个随机名为flamboyant_shirley, ID为adc809b50388的容器。进入容器后查看目录确实已经存在，把容器关闭销毁后文件依然会在主机中。\n当然还可以将具体文件通过显示挂载给容器使用，比如通过指定参数-v：-v /root/test/test.txt:/usr/home/test/test.txt。\n使用-v /[container path]进行docker managed volume隐式挂载\n1 2 3 4 $sudo docker run -it --name myalpine -v /usr/local/lmtest alpine:x86-3.11.5 / # / # ls /usr/local/lmtest/ / # 这里基于alpine镜像启动了一个名为myalpine的容器，隐式挂载并没有指定具体主机目录，容器挂载了一个匿名卷/usr/local/lmtest，初始镜像里并没有这个目录，docker会为其创建一个该目录。使用inspect看看这个匿名卷。\n1 $sudo docker inspect myalpine 也可基于容器ID来查看容器情况，会输出很多内容，重点查看存储相关内容如下：\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;5778513ecf9bf04a44722082c8f5c91ee9f0e30922c17d71529009149c0dcc63\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/5778513ecf9bf04a44722082c8f5c91ee9f0e30922c17d71529009149c0dcc63/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/local/lmtest\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } 其中Source部分就是Host主机所在目录，Destination部分是容器挂载匿名卷，尝试从Host主机上的挂载目录写一个文件，然后查看容器中的变化。\n在主机上添加文件\n1 2 3 [root@ecs_lm_test ~]# cd /var/lib/docker/volumes/5778513ecf9bf04a44722082c8f5c91ee9f0e30922c17d71529009149c0dcc63/_data [root@ecs_lm_test _data]# ls [root@ecs_lm_test _data]# echo \u0026#34;hello,lm\u0026#34; \u0026gt; test.txt 在主机共享目录里新增一个test.txt文件，内容为hello,lm。\n在容器中查看文件\n1 2 3 / # cat /usr/local/lmtest/test.txt hello,lm / # 在容器中可见与主机是保持一致的。\n销毁容器后查看主机目录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@ecs_lm_test _data]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 236406585e9a alpine:x86-3.11.5 \u0026#34;/bin/sh\u0026#34; 16 minutes ago Exited (0) 16 seconds ago myalpine [root@ecs_lm_test _data]# [root@ecs_lm_test _data]# [root@ecs_lm_test _data]# docker rm myalpine myalpine [root@ecs_lm_test _data]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES [root@ecs_lm_test_data]# cd /var/lib/docker/volumes/5778513ecf9bf04a44722082c8f5c91ee9f0e30922c17d71529009149c0dcc63/_data [root@ecs_lm_test_data]# [root@ecs_lm_test_data]# [root@ecs_lm_test_data]# ls test.txt [root@ecs_lm_test_data]# 可见容器在销毁后文件还是存在于主机目录下。\n全文完。\n","description":"Docker提供两种数据存储资源，一种是基于storage driver提供的镜像层和容器层，一种是基于data volume提供的持久化存储，本文简要描述这两种存储资源。","id":19,"section":"posts","tags":["Docker","存储"],"title":"04-Docker数据存储","uri":"https://laomeinote.com/posts/docker-data-storage/"},{"content":"Dockerfile是用来干什么的？答案是制作镜像。那么了解Dockerfile之前，首先需要理解几个简单概念。\nDockerfile基础概念 制作镜像命令： docker build build context: 镜像编译上下文，其实就是docker客户端所在的目录。Docker基本架构与原理提到Docker 是一个典型的 C/S 架构的应用，分为 Docker 客户端（即平时敲的 docker 命令）和 Docker 服务端（dockerd 守护进程），Docker 客户端通过 REST API 和服务端进行交互，docker 客户端每发送一条指令，底层都会转化成 REST API 调用的形式发送给服务端，服务端处理客户端发送的请求并给出响应。 一个典型的使用docker build的构建镜像的流程大概分为如下三步:\n1）执行 docker build -t \u0026lt;imageName:imageTag\u0026gt; . ；注意命令中有一个点号**(.)**在linux中表示当前目录，就是这次build的上下文build context； 2）Docker 客户端会将构建命令后面指定的路径(.)下的所有文件打包成一个 tar 包，发送给 Docker 服务端; 3）Docker 服务端收到客户端发送的 tar 包进行解压，根据 Dockerfile 里面的指令进行镜像的分层构建。 以一个简单的例子说明理解build context，首先创建一个简单的 demo 工程，工程结构如下：\n1 2 3 4 5 6 7 helloworld-app ├── Dockerfile └── docker ├── app-1.0.jar ├── hello-world.txt └── html └── index.html Dockerfile 内容：\n1 2 3 FROM busybox COPY hello-world.txt . COPY html/index.html . 测试一下进入 helloworld-app 目录进行镜像构建，以 docker 目录为build context上下文构建一个镜像：\n1 $ docker build -f Dockerfile -t hello-app:1.0 docker 这里使用了docker build构建了一个镜像hello-app, tag是1.0，上下文是后面的参数docker，即helloworld-app目录下的docker目录，docker build执行的内容是Dockerfile定义的三个指令：FROM，COPY，COPY。当然这个镜像只是从busybox里复制了两个文件，实际意义不大。下面来看看详细的Dockerfile指令。\nDockerfile的基本指令 关于这些基本指令先来看一则网上的示意图，将大部分指令形象化描述出来了。\n图 1 Dockerfile指令 - 图片来自网络\nFROM指令：指定base镜像，Dockerfile中第一条指令必须是FROM指令，其格式如下：\n1 2 FROM image FROM image:tag MAINTAINER指令：指明作者信息，格式为:\n1 MAINTAINER user_name user_email WORKDIR指令：为后续的RUN、CMD、ENTRYPOINT 、ADD和COPY指令指定工作目录，这个目录必须是在容器中提前创建好的。docker build 构建镜像过程中的，每一个 RUN 命令都是新建的一层，只有通过 WORKDIR 创建的目录才会一直存在。格式如下：\n1 WORKDIR /path/to/workdir 可以使用多个 WORKDIR 指令，后续命令中的参数如果是相对路径，则基于之前命令指定路径，如：\n1 2 3 4 5 6 WORKDIR /a WORKDIR b WORKDIR c RUN pwd #RUN的结果是/a/b/c 这里有多个WORKDIR指令，第一个路径/a是绝对路径，第二个和第三个是b和c是相对路径，需要组合为/a/b/c为最终路径。\nCOPY指令：从上下文目录中复制文件或者目录到容器里指定路径，上下文目录就是指上面提到的build context。格式为:\n1 COPY src desc #若desc不存在则自动创建； ADD指令：ADD 指令和 COPY 的使用格类似（同样需求下，官方推荐使用 COPY），功能也类似，不同之处在于源文件格式为tar,gzip,bzip2,xz时，会自动复制并解压到目标路径。\n1 ADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; #src是上下文build context的相对路径，也可以是一个url ENV指令：设置环境变量，后续在Dockerfile的指令中，可以使用这个环境变量，并在容器运行时保持。格式如下：\n1 2 ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;... 以下示例设置 NODE_VERSION = 7.2.0 ， 后续在Dockerfile的指令中可以通过 $NODE_VERSION 引用：\n1 2 3 4 ENV NODE_VERSION 7.2.0 RUN curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; \\ \u0026amp;\u0026amp; curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\u0026#34; EXPOSE指令：指定容器中的进程会监听某个端口，Docker可以将该端口暴露出来。在运行时使用随机端口映射时，也就是docker run -P 时，会自动随机映射 EXPOSE 的端口。EXPOSE格式如下：\n1 EXPOSE \u0026lt;端口1\u0026gt; [\u0026lt;端口2\u0026gt;...] VOLUME指令：定义匿名数据卷，在HOST宿主机的目录/var/lib/docker/volumes下自动生成一个随机目录。我们知道不应该在容器存储层内进行数据写入操作，所有写操作都应该使用卷，那么匿名卷是什么意思？即\n在进行数据卷挂载的时候不指定HOST宿主机的数据卷目录，使用docker run -v命令后直接跟上容器内数据卷所在的路径。 或者在Dockerfile中定义了匿名卷以后，用户即使没有使用docker run -v指定卷也会在容器内指定一个目录，挂载在HOST宿主机的目录/var/lib/docker/volumes下面，可以通过docker inspect (这是容器id)查看容器挂载的卷的具体目录。 VOLUME指令格式如下：\n1 2 VOLUME [\u0026#34;\u0026lt;路径1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;路径2\u0026gt;\u0026#34;...] VOLUME \u0026lt;路径\u0026gt; 在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点。\n参考文章1： Dockerfile Volume指令与docker -v的区别\n参考文章2：Docker 匿名卷（匿名挂载）和命名卷（具名挂载)\nRUN指令：每条 RUN 指令将在当前的镜像基础上执行指定的命令，并创建一个新的镜像层，一层新的数据到镜像中。格式如下：\n1 2 RUN command ##格式1，调用shell终端通过/bin/sh来执行 RUN [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;.....] ##格式2，使用docker exec命令执行，推荐这种方式 CMD指令：为启动的容器指定默认要运行的程序，就是startup启动程序，程序运行结束时容器生命周期也就结束了。CMD指令指定的程序会被 docker run 命令行参数中指定要运行的程序所覆盖。每个Dockerfile只能有一个CMD指令，如果有多个则只有最后一个被启动执行。其格式为：\n1 2 CMD command param1 param2 ##格式1，调用shell终端通过/bin/sh来执行 CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] ##格式2，使用docker exec命令执行，推荐这种方式 ENTRYPOINT指令：跟CMD指令类似，也是容器startup启动程序，并且不会被 docker run 提供的参数覆盖，每个Dockerfile只能有一个ENTRYPOINT，当指定多个时，只有最后一个生效。格式如下：\n1 2 ENTRYPOINT command param1 param2 ##格式1，调用shell终端通过/bin/sh来执行 ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] ##格式2，使用docker exec命令执行，推荐这种方式 上面看起来RUN、CMD、ENTRYPOINT都一样，它们有什么区别呢？直接抛结论：\nRUN命令执行命令，并创建新的镜像层，通常用于安装软件包 CMD命令设置的是容器startup后执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换，如ubuntu, busybox, debian镜像在其dockerfile最后一句中都会指定启动命令为/bin/bash ENTRYPOINT配置容器startup时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） 列了这么多，举个栗子实操一下基于Dockerfile写一个Nginx镜像并启动一个nginx容器吧。\n基于Dockerfile定制Nginx镜像 设置工作目录\n1 2 $ sudo mkdir -p /root/lab $ sudo cd /root/lab 在工作目录下新建一个Dockerfile文件Dockerfile_nginx\n1 $ sudo vim Dockerfile_nginx 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 设置Base镜像为dockerhub最新nginx镜像 FROM nginx # 设置容器的工作目录 WORKDIR \u0026#34;/tmp\u0026#34; # 表示对外期望暴露的端口 EXPOSE 80 # 设置容器的匿名卷 /usr/local/tmp VOLUME /usr/local/tmp # 启动nginx，指定参数 -g `daemon off`将nginx设置为前台运行 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 使用nginx -g daemon off启动nginx容器的原因：\n背景：Docker 容器启动时，默认会把容器内部第一个进程，也就是pid=1的程序，作为docker容器是否正在运行的依据，如果 docker 容器pid=1的进程挂了，那么docker容器便会直接退出。 原因：nginx默认是以后台模式启动的，Docker未执行自定义的CMD之前，nginx的pid是1，执行到CMD之后，nginx就在后台运行，bash或sh脚本的pid变成了1。所以一旦执行完自定义CMD，nginx容器也就退出了。为了保持nginx的容器不退出，应该关闭nginx后台运行 编译Dockerfile生成一个镜像：\n1 $sudo docker build -f Dockerfile_nginx -t lm/nginx:v1.0 . 参数说明：\n-f：输入Dockerfile文件 -t：给镜像打TAG .：指定build context上下文为当前目录 开始启动容器：\n1 2 $sudo docker run -it -d -p 80:80 lm/nginx:v1.0 0f6ec4c8ac83edde6fbfc6288905af29f7ed2312a08d850fab53c299e7840fda 参数说明：\nit: 给容器分配一个shell终端进行交互，以tty进行； -d: 后台守护这个容器进程 -p: 第一个80是主机端口，第二个80是容器端口，将主机端口隐射至容器端口 启动后访问主机IP，可直接通过curl命令访问，也可通过浏览器访问，由于http协议默认端口为80所以无需加端口号访问，如下图示。\n上面是通过nginx镜像直接生成的，一般应用中nginx大多是当成中间件的，base镜像选择的一般是操作系统。\n导出导入Docker镜像 在内网中因为基于安全考虑，正式仓库中的镜像比较少，如果在内网从因为无法获取软件包而制作镜像失败，一个可行的解决方案是在外网中制作好镜像，将镜像打包后拷贝至内网中，再导入镜像，最终PUSH上传到自己的私有仓库。\n导出镜像：\n1 $sudo docker save [镜像:tag] \u0026gt; [镜像包名] 导入镜像：\n1 $sudo docker load \u0026lt; [镜像包名] 此举可以做一些曲线救国之事，比如我们在内网无法获取的一些软件安装包，可以在外网打包下载下来再加载进来，然后push到自己的私有仓库。后续需要使用的时候，就直接拉取即可。\n全文完。\n","description":"本文简单总结Dockerfile的基本规则，并通过样例演示如何使用dockerfile制作镜像。","id":20,"section":"posts","tags":["Docker","Dockefile"],"title":"03-Dockerfile基础","uri":"https://laomeinote.com/posts/dockerfile-basics/"},{"content":"1.安装Docker 国内直接访问Docker官网速度较慢，不过国内各大厂商都提供加速服务，我们到华为云镜像站加速。\n1.1 Centos安装Docker 安装过docker，需要先删掉，之后再安装依赖: 1 2 sudo yum remove docker docker-common docker-selinux docker-engine sudo yum install -y yum-utils device-mapper-persistent-data lvm2 下载docker repo文件，并更换软件仓库地址为华为云地址 1 2 wget -O /etc/yum.repos.d/docker-ce.repo https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo sudo sed -i \u0026#39;s+download.docker.com+repo.huaweicloud.com/docker-ce+\u0026#39; /etc/yum.repos.d/docker-ce.repo 更新yum源索引，并安装docker, CE为社区版本 1 2 sudo yum makecache fast sudo yum install docker-ce 启动Docker。 1 2 3 sudo systemctl enable docker #设置Docker服务宿主机重启后自动启动 sudo systemctl start docker #开始运行docker服务 sudo systemctl status docker #查看docker状态 检查一下docker安装结果 1 2 3 sudo docker info sudo docker -v sudo docker ps -a 1.2 配置Docker加速 华为云提供免费SWR仓库，可登陆后，拷贝加速地址（https://e2660ea6dc2b4a16a3ae382f8d227beb.mirror.swr.myhuaweicloud.com）。\n在宿主机的/etc/docker目录下添加配置文件daemon.json\n1 2 3 4 5 6 mkdir -p /etc/docker cat \u0026gt;\u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://e2660ea6dc2b4a16a3ae382f8d227beb.mirror.swr.myhuaweicloud.com/\u0026#34;] } EOF 重启服务生效：\n1 2 sudo systemctl daemon-reload sudo systemctl restart docker 当然，如果是在内网环境，使用的是自建私有仓库，可能使用的HTTP协议的仓库your-private-repo.com，则修改对应配置为：\n1 2 3 4 5 6 7 8 mkdir -p /etc/docker cat \u0026gt;\u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;your-private-repo.com:80\u0026#34; ] } EOF 1.3 获取镜像 安装完成后可拉取对应镜像，如果不指定仓库地址，则动docker-hub下载，或者指定地址下载镜像。\n1 2 3 docker pull nginx:alpine-perl #拉取镜像 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 #从aliyun下载pause镜像 docker images #查看镜像 2.启动一个ubuntu容器 直接启动容器一般有两个途径：\n新建一个容器后启动docker run。\n已有一个容器实例执行该容器docker exec。\n新建启动一个容器\n在获取到镜像以后，使用docker run基于镜像新建启动一个容器，拉取一个ubuntu镜像做下实验.\n1 2 3 4 $sudo docker pull ubuntu:latest #拉取ubuntu镜像 $sudo docker images #查询镜像 $sudo docker run -it -v /test:/tmp ubuntu:latest #以这个镜像为基础，启动一个容器 root@79258329dbf4:/# 可以看到命令行已经切入到ubuntu这个容器中了，容器与host主机是共内核的，上层文件系统是独立的，通过Cgroup与Namespace来隔离容器之间的进程与资源。\n参数说明：\n-i: 就是 –interactive 的缩写，表示以交互模式运行容器，通常与 -t 同时使用； -t: 也可以使用 –tty 来进行引用，为容器重新分配一个伪输入终端，通常与 -i 同时使用； -v: 指定主机的目录到容器目录下，实现主机与容器目录共享，-v /test:/tmp表示将主机上的/test目录挂到容器的/tmp下； 有几个特别注意事项：\n所有镜像的格式一定是**[REPOSITORY]:[tag]**。 当docker run命令不加任何启动命令的时候，默认执行容器的ENTRYPOINT或CMD指令所指定的命令，该命令是在镜像制作的时候指定的，以建议在制作镜像的时候，为服务性镜像提供默认的启动命令。上面的docker run没有添加启动命令是因为在ubuntu中已经默认以bash命令启动了。 正常启动应以命令启动如，docker run -it ubuntu:latest /bin/bash 执行一个已有容器\n如果已有一些容器在运行，希望更改该容器或从中获取某些内容，那么使用docker exec直接进入，其格式为docker exec [OPTIONS] CONTAINER COMMAND [ARG...]。如：\n1 sudo docker exec -it ubuntu_bash bash #ubuntu_bash为docker name也可以是docker ID 其中的**[OPTIONS]**包括常见的：\n-it: 分配一个终端交互 -w: 如，-w /tmp 说明要在哪个工作目录中运行命令 其中的**CONTAINER **可以是docker name也可以是docker ID，COMMAND就是容器中命令如上面示例中的bash。\n3.启动一个tomcat容器 3.1 拉取tomcat镜像 可使用login命令指定镜像仓库地址，输入账号登陆。\n1 2 3 $ sudo docker login -u my-private-repo.com:80 #输入密码可登陆私有镜像仓地址 $ sudo docker pull my-private-repo.com/public/tomcat:x86-9.0.31 #拉取镜像到本地 $ sudo docker tag my-private-repo.com/public/tomcat:x86-9.0.31 mytomcat:1.0 #给镜像打tag 打tag就像给文件重新命名一样，所有docker镜像必须要有tag说明，tag一般为版本号。\n3.2 运行tomcat容器 1 $ docker run -d -p 8080:8080 --name tomcat01 mytomcat:1.0 参数说明：\n-d: daemon - 以守护进程在后台运行 -p: port - 前者是外部访问端口，后者是容器内映射端口 --name: tomcat01是容器名称，也可以不用指定名字 最后一个参数是引用的具体镜像 3.3 进入已经运行的容器 1 2 3 4 [root@kweekshcgp-m19vr ~]# docker ps -a |grep tomcat f1cf7db136a9 mytomcat:v1.0 \u0026#34;catalina.sh run\u0026#34; 26 minutes ago Up 26 minutes 0.0.0.0:8080-\u0026gt;8080/tcp tomcat01 [root@kweekshcgp-m19vr ~]# docker exec -it f1cf7db136a9 /bin/bash 执行docker exec -it containerID /bin/bash即可进入容器中，其中：\n-i: 表示以交互模式运行容器，通常与 -t 同时使用； -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； 但在浏览器输入Host IP:8080后发现无法tomcat故障，到容器中查看tomcat目录结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 root@f1cf7db136a9:/usr/local/tomcat# ls -al total 172 drwxr-xr-x 1 root root 4096 Feb 11 2020 . drwxr-xr-x 1 root root 4096 Feb 7 2020 .. -rw-r--r-- 1 root root 18982 Feb 5 2020 BUILDING.txt -rw-r--r-- 1 root root 5409 Feb 5 2020 CONTRIBUTING.md -rw-r--r-- 1 root root 57092 Feb 5 2020 LICENSE -rw-r--r-- 1 root root 2333 Feb 5 2020 NOTICE -rw-r--r-- 1 root root 3255 Feb 5 2020 README.md -rw-r--r-- 1 root root 6898 Feb 5 2020 RELEASE-NOTES -rw-r--r-- 1 root root 16262 Feb 5 2020 RUNNING.txt drwxr-xr-x 2 root root 4096 Feb 11 2020 bin drwxr-xr-x 1 root root 4096 Nov 4 11:25 conf drwxr-xr-x 2 root root 4096 Feb 11 2020 include drwxr-xr-x 2 root root 4096 Feb 11 2020 lib drwxrwxrwx 1 root root 4096 Nov 4 11:25 logs drwxr-xr-x 3 root root 4096 Feb 11 2020 native-jni-lib drwxrwxrwx 2 root root 4096 Feb 11 2020 temp drwxr-xr-x 1 root root 4096 Nov 4 11:30 webapps drwxr-xr-x 7 root root 4096 Feb 5 2020 webapps.dist drwxrwxrwx 1 root root 4096 Nov 4 11:30 work root@f1cf7db136a9:/usr/local/tomcat# 其中，webapps目录为空，需要将文件从webapps.dist中拷贝至webapps目录。再次访问时便可成功。\n也可以在主机上查看tomcat所占资源情况。\n1 2 3 4 [root@kweekshcgp-m19vr ~]# docker stats CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS f1cf7db136a9 tomcat01 143.2MiB / 7.27GiB 1.92% 14.4kB/213kB 0B / 0B 39 如果想限制该容器的资源，怎么办？\n1 [root@kweekshcgp-m19vr ~]# docker run -d -p 8081:8080 --name tomcat02 -m 100M mytomcat:1.0 这是重新拉起一个tomcat02的容器，通过参数-m 100M来限制内存大小\n3.4 打包成新镜像 可以将允许修改了配置的容器重新打包，并上传到仓库，在其他机器上无需再做配置，直接拉取即可随时运行。\n1 $sudo docker commit -a=\u0026#34;author\u0026#34; -m=\u0026#34;comments\u0026#34; 容器ID imagename:tag -a: 作者信息 -m: 备注信息 容器ID: 将需要打包的容器ID imagename:tag: 镜像命名与打tag标签 1 2 3 4 5 6 [root@kweekshcgp-m19vr ~]# docker commit -a=\u0026#34;meixuhong\u0026#34; -m=\u0026#34;add webapps app\u0026#34; f1cf7db136a9 mytomcat:2.0 sha256:6d9f5f6c07cab801745da0b56583733be1eca9fb285a4caad5c2d8d1fc3c88e3 [root@kweekshcgp-m19vr ~]# docker images|grep tomcat mytomcat 2.0 6d9f5f6c07ca 57 seconds ago 652MB mytomcat v1.0 71445a67f6b6 21 months ago 647MB my-privare-repo.com:80/public/tomcat 9.0.31 71445a67f6b6 21 months ago 647MB 3.5 导出导入镜像 内网无法跟互联网互通，如果在内网无法获取到想要的镜像，可现在互联网将镜像导出并下载至本地，再将其导入到内网环境。\n外网导出镜像：\n1 $ docker save mytomcat:v.10 -o mytomcat.tar 内网导入镜像：\n1 $docker load \u0026lt; mytomcat.tar 3.6 重启、退出与删除容器 退出容器直接使用exit即可从交互命令中退出，删除容器前需要先停止，使用stop或者rm\n1 2 3 4 5 6 7 root@f1cf7db136a9:/usr/local/tomcat#exit #退出容器 [root@kweekshcgp-m19vr ~]# docker stop 容器ID #停止容器 [root@kweekshcgp-m19vr ~]# docker start 容器ID #再次启动容器 [root@kweekshcgp-m19vr ~]# docker rm -f 容器ID #停止容器 [root@kweekshcgp-m19vr ~]# docker rmi 镜像名称 #删除镜像 [root@kweekshcgp-m19vr ~]# docker container prune #删除所有停止的容器 文章首发公众号：，欢迎关注，不定期更新。\n全文完。\n","description":"本文简单总结如何安装、启动和启用一个Docker容器。","id":21,"section":"posts","tags":["Docker"],"title":"02-安装并启动Docker容器","uri":"https://laomeinote.com/posts/start-a-docker-container/"},{"content":"虽然Kubernetes从1.20版本中不再使用Docker作为容器引擎，但docker还是被广泛使用的，实际上新的Kubernetes容器引擎containerd的使用方式与docker大致相同。先继续了解下Docker基础架构与原理吧。\n先来简单描述一下容器与虚拟化的技术差别：虚拟化是将物理（硬件）资源如CPU、内存、存储、网络进行抽象转换为虚拟资源的技术，而容器技术其实是基于OS（操作系统软件）层面的虚拟化。理解一下这个公式：\n容器 = cgroup + namespace + rootfs + 容器引擎\n1. Cgroup 基于Linux Cgroup来限制和隔离一组进程对系统资源的使用。\n2. Namespace 基于Namespace来隔离资源。\n3. rootfs 文件系统，每个Linux启动时都会挂载一个可读写的根文件系统，不同的是容器在启动后挂载的是一个空的只读的文件系统，在此基础上再挂载一个读写的文件系统，如下图示。\n4. 容器引擎 Docker实际是一个C/S架构程序，如下图示容器引擎主要有三部分组成：\nClient：docker CLI，就是命令行客户端 REST API：客户端与服务端以API进行通信 Server：是一个docker daemon守护进程，用于创建个管理docker对象，如容器镜像、容器、网络、存储卷。 正常情况下，docker server以守护进程运行在主机上，client发出命令，通过REST API传递给server，server接收到指令开开始执行相应的动作，如启动容器，挂载卷等。容器引擎管理着容器实例container，镜像image，存储挂载卷data volumes和网络network，当然镜像image跟仓库registry又是强相关的，即我们是通过仓库来保存镜像的，通过pull/push来下拉和推送保存镜像。\n以官网的一张示意图说明这几者之间的关系。\n图中的①②③④，就是上文讲到的docker引擎内容，①是client客户端，执行docker命令，②是通过Rest API在③docker daemon守护进程server进行通信，执行最终操作，基于⑤镜像来创建/运行④容器。⑥是镜像仓库Registry，可通过Pull/Push来获取与推送镜像。\n","description":"容器=cgroup + namespace + rootfs + 容器引擎，其中docker就是其中一个容器引擎。","id":22,"section":"posts","tags":["Docker"],"title":"01-Docker基本架构与原理","uri":"https://laomeinote.com/posts/docker-architecture/"},{"content":"awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。\nawk基本使用方法 1 awk [-F field-separator] \u0026#39;commands\u0026#39; input-file(s) 参数描述：\n[-F域分隔符]是可选的，awk以行为单位，逐行读取数据，通过field-separator将这行分割，分割后的行分别为对应的域；如果不设置该参数默认以\u0026quot;空白键\u0026quot; 或 \u0026ldquo;[tab]键\u0026quot;为分隔符； commands 是awk真正执行的命令; input-file(s) 是待处理的文件。 需要注意的是，整个脚本命令是用单引号（\u0026rsquo;\u0026rsquo;）括起，而其中的执行命令部分需要用大括号（{}）括起来。\n举个栗子：\n读取系统前5条登陆记录：\n1 2 3 4 5 6 7 8 # last -n 5 root pts/0 10.45.52.12 Tue Jan 4 20:59 still logged in root pts/0 10.38.94.203 Tue Jan 4 16:34 - 18:17 (01:43) root pts/1 10.38.94.203 Tue Jan 4 15:00 - 16:29 (01:28) root pts/1 10.38.94.203 Tue Jan 4 14:44 - 15:00 (00:16) root pts/0 10.37.80.224 Tue Jan 4 11:06 - 15:53 (04:46) wtmp begins Mon Apr 27 22:28:21 2020 使用awk获取用户名\n1 2 3 4 5 6 7 8 # last -n 5 |awk \u0026#39;{print $1}\u0026#39; root root root root root wtmp 这个例子中awk工作流程是这样的：读入有\u0026rsquo;\\n\u0026rsquo;换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是\u0026quot;空白键\u0026rdquo; 或 \u0026ldquo;[tab]键\u0026rdquo;,所以$1表示登录用户，$3表示登录用户ip，以此类推。\n再举个栗子显性化:分隔符：\n1 2 3 4 5 6 7 8 # cat test.txt I had a lovely: time: on: our little picnic. asdh:sdhsf:dskgjfg 1h:sdjfhf:djhf # cat test.txt |awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; I had a lovely asdh 1h 这个例子中，awk工作流程是这样的：读入有\u0026rsquo;\\n\u0026rsquo;换行符分割的一条记录，然后将记录按指定的域分隔符:划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是\u0026quot;空白键\u0026quot; 或 \u0026ldquo;[tab]键\u0026rdquo;。\n上面我们见识了awk的基本使用方法，其强大之处在于脚本命令 'commands'，其实更强大的还没展示出来，awk的 'commands'由 2 部分组成，分别为匹配规则和执行命令，如下所示：\n1 \u0026#39;匹配规则{执行命令}\u0026#39; 即awk的使用方式为：\n1 awk \u0026#39;匹配规则{执行命令}\u0026#39; input-file(s) 举个栗子：test.txt共有5行，其中第2/4/6行为空行。\n1 2 3 4 5 6 7 8 9 10 11 # cat test.txt I had a lovely: time: on: our little picnic. asdh:sdhsf:dskgjfg 1h:sdjfhf:djhf # awk \u0026#39;/^$/{print \u0026#34;Blank line\u0026#34;}\u0026#39; test.txt Blank line Blank line Blank line /^$/ 是一个正则表达式，功能是匹配文本中的空白行，同时可以看到，执行命令使用的是 print 命令，此命令经常会使用，它的作用很简单，就是将指定的文本进行输出。因此，整个命令的功能是，如果 test.txt 有 N 个空白行，那么执行此命令会输出 N 个 Blank line。\nawk使用数据字段变量 awk 的主要特性之一是其处理文本文件中数据的能力，它会自动给一行中的每个数据元素分配一个变量。\n默认情况下，awk 会将如下变量分配给它在文本行中发现的数据字段：\n$0 代表整个文本行； $1 代表文本行中的第 1 个数据字段； $2 代表文本行中的第 2 个数据字段； $n 代表文本行中的第 n 个数据字段。 在 awk 中，默认的字段分隔符是任意的空白字符（例如空格或制表符）。 在文本行中，每个数据字段都是通过字段分隔符划分的。awk 在读取一行文本时，会用预定义的字段分隔符划分每个数据字段。\n所以在下面的例子中，awk 程序读取文本文件，只显示第 1 个数据字段的值：\n1 2 3 4 5 6 7 8 9 # cat data.txt One line of test text. Two lines of test text. Three lines of test text. # awk \u0026#39;{print $1}\u0026#39; data.txt One Two Three # awk使用多个命令 awk 可以将多条命令组合成一个正常的脚本，只需在命令之间加上分号即可。举个栗子：\n1 2 # echo \u0026#34;Hello this is for shell test\u0026#34; | awk \u0026#39;{$5=\u0026#34;awk\u0026#34;; print $0}\u0026#39; My name is Christine 第一条命令$5=\u0026ldquo;awk\u0026rdquo;，会给字段变量$4赋值，第二条命令打印整个字段。\nawk 脚本使用方法 一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被 单引号 中，其基本结构为：\n1 awk \u0026#39;BEGIN{ print \u0026#34;start\u0026#34; } pattern{ commands } END{ print \u0026#34;end\u0026#34; }\u0026#39; file awk的工作原理：\n第一步：执行BEGIN{ commands }语句块中的语句； 第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ commands }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。其中commands也可使用分号隔开引用多个命令。 第三步：当读至输入流末尾时，执行END{ commands }语句块。 BEGIN语句块 在awk开始从输入流中读取行 之前 被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。\nEND语句块 在awk从输入流中读取完所有的行 之后 即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。\npattern语句块 中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行{ print }，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。\n举个栗子：\n1 2 3 4 5 # echo -e \u0026#34;A line 1\\nA line 2\u0026#34; | awk \u0026#39;BEGIN{ print \u0026#34;Start\u0026#34; } { print } END{ print \u0026#34;End\u0026#34; }\u0026#39; Start A line 1 A line 2 End 当使用不带参数的print时，它就打印当前行，当print的参数是以逗号进行分隔时，打印时则以空格作为定界符。\nawk内置变量 awk有许多内置变量用来设置环境信息，这些变量可以被改变，下面给出了最常用的一些变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ARGC 命令行参数个数 ARGV 命令行参数排列 ENVIRON 支持队列中系统环境变量的使用 FILENAME awk浏览的文件名 FNR 浏览文件的记录数 FS 设置输入域分隔符，等价于命令行 -F选项 NF 浏览记录的域的个数 NR 已读的记录数 OFS 输出域分隔符 ORS 输出记录分隔符 RS 控制记录分隔符 $0 代表整个文本行； $1 代表文本行中的第 1 个数据字段； $2 代表文本行中的第 2 个数据字段； $n 代表文本行中的第 n 个数据字段。 举个栗子，统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # awk -F \u0026#39;:\u0026#39; \u0026#39;{print \u0026#34;filename:\u0026#34; FILENAME \u0026#34;,linenumber:\u0026#34; NR \u0026#34;,columns:\u0026#34; NF \u0026#34;,linecontent:\u0026#34;$0}\u0026#39; /etc/passwd filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bash filename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin filename:/etc/passwd,linenumber:3,columns:7,linecontent:daemon:x:2:2:daemon:/sbin:/sbin/nologin filename:/etc/passwd,linenumber:4,columns:7,linecontent:adm:x:3:4:adm:/var/adm:/sbin/nologin filename:/etc/passwd,linenumber:5,columns:7,linecontent:lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin filename:/etc/passwd,linenumber:6,columns:7,linecontent:sync:x:5:0:sync:/sbin:/bin/sync filename:/etc/passwd,linenumber:7,columns:7,linecontent:shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown filename:/etc/passwd,linenumber:8,columns:7,linecontent:halt:x:7:0:halt:/sbin:/sbin/halt filename:/etc/passwd,linenumber:9,columns:7,linecontent:mail:x:8:12:mail:/var/spool/mail:/sbin/nologin filename:/etc/passwd,linenumber:10,columns:7,linecontent:operator:x:11:0:operator:/root:/sbin/nologin filename:/etc/passwd,linenumber:11,columns:7,linecontent:games:x:12:100:games:/usr/games:/sbin/nologin filename:/etc/passwd,linenumber:12,columns:7,linecontent:ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin filename:/etc/passwd,linenumber:13,columns:7,linecontent:nobody:x:99:99:Nobody:/:/sbin/nologin filename:/etc/passwd,linenumber:14,columns:7,linecontent:systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin filename:/etc/passwd,linenumber:15,columns:7,linecontent:dbus:x:81:81:System message bus:/:/sbin/nologin filename:/etc/passwd,linenumber:16,columns:7,linecontent:polkitd:x:999:998:User for polkitd:/:/sbin/nologin filename:/etc/passwd,linenumber:17,columns:7,linecontent:postfix:x:89:89::/var/spool/postfix:/sbin/nologin filename:/etc/passwd,linenumber:18,columns:7,linecontent:sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin filename:/etc/passwd,linenumber:19,columns:7,linecontent:chrony:x:998:996::/var/lib/chrony:/sbin/nologin filename:/etc/passwd,linenumber:20,columns:7,linecontent:tcpdump:x:72:72::/:/sbin/nologin 使用printf替代print,可以让代码更加简洁，如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # awk -F \u0026#39;:\u0026#39; \u0026#39;{printf(\u0026#34;filename:%s,linenumber:%s,columns:%s,linecontent:%s\\n\u0026#34;,FILENAME,NR,NF,$0)}\u0026#39; /etc/passwd filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bash filename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin filename:/etc/passwd,linenumber:3,columns:7,linecontent:daemon:x:2:2:daemon:/sbin:/sbin/nologin filename:/etc/passwd,linenumber:4,columns:7,linecontent:adm:x:3:4:adm:/var/adm:/sbin/nologin filename:/etc/passwd,linenumber:5,columns:7,linecontent:lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin filename:/etc/passwd,linenumber:6,columns:7,linecontent:sync:x:5:0:sync:/sbin:/bin/sync filename:/etc/passwd,linenumber:7,columns:7,linecontent:shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown filename:/etc/passwd,linenumber:8,columns:7,linecontent:halt:x:7:0:halt:/sbin:/sbin/halt filename:/etc/passwd,linenumber:9,columns:7,linecontent:mail:x:8:12:mail:/var/spool/mail:/sbin/nologin filename:/etc/passwd,linenumber:10,columns:7,linecontent:operator:x:11:0:operator:/root:/sbin/nologin filename:/etc/passwd,linenumber:11,columns:7,linecontent:games:x:12:100:games:/usr/games:/sbin/nologin filename:/etc/passwd,linenumber:12,columns:7,linecontent:ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin filename:/etc/passwd,linenumber:13,columns:7,linecontent:nobody:x:99:99:Nobody:/:/sbin/nologin filename:/etc/passwd,linenumber:14,columns:7,linecontent:systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin filename:/etc/passwd,linenumber:15,columns:7,linecontent:dbus:x:81:81:System message bus:/:/sbin/nologin filename:/etc/passwd,linenumber:16,columns:7,linecontent:polkitd:x:999:998:User for polkitd:/:/sbin/nologin filename:/etc/passwd,linenumber:17,columns:7,linecontent:postfix:x:89:89::/var/spool/postfix:/sbin/nologin filename:/etc/passwd,linenumber:18,columns:7,linecontent:sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin filename:/etc/passwd,linenumber:19,columns:7,linecontent:chrony:x:998:996::/var/lib/chrony:/sbin/nologin filename:/etc/passwd,linenumber:20,columns:7,linecontent:tcpdump:x:72:72::/:/sbin/nologin 全文完。\n","description":"grep、sed、awk并称为linux文本处理器三剑客，本文主要总结awk的基础使用方法。","id":23,"section":"posts","tags":["Linux","Shell"],"title":"Linux Shell之awk","uri":"https://laomeinote.com/posts/how-to-use-awk-in-linux/"},{"content":"sed（stream editor）流编辑器也是linux中的一条命令，通过sed可在shell中实现非交互式修改文件内容。sed处理文本是按行处理，也就是读一行处理一行。\nsed命令基本格式是：\n**sed [选项] \u0026lsquo;编辑指令\u0026rsquo; 文件路径 **\n也可以通过管道|传输参数给sed处理 前置命令 | sed 选项 \u0026lsquo;编辑指令\u0026rsquo;\nsed选项常用参数 -n: 寂静模式，抑制来自sed命令执行过程中的冗余输出信息，比如只显示那些被改变的行；如果不加-n选项会全部输出文本满足条件的行再重复输出； -r: 如果使用扩展正则，则需要添加-r选项，默认不支持扩展正则，只支持标准正则 -i: 直接修改源文件；不加-i只会在屏幕临时输出不会修改源文件，一般测试过命令无误才会在脚本中使用-i选项 1 $ sed -n \u0026#39;2p\u0026#39;/etc/passwd 打印出第2行 sed的编辑指令 s 替换 命令 完整示例 说明 s/old/new/ sed \u0026rsquo;s/old/new/\u0026rsquo; reg.txt 删除reg.txt中每行的第一个old都替换成new s/old/new/2 sed \u0026rsquo;s/old/new/\u0026rsquo; reg.txt 删除reg.txt中每行的第二个old都替换成new s/old/new/g sed \u0026rsquo;s/old/new/g\u0026rsquo; reg.txt 删除reg.txt中每行的每一个old都替换成new s/old// sed \u0026rsquo;s/old//\u0026rsquo; reg.txt 删除reg.txt中每行的第一个old都替换成空也就是把old删除 s/old/\u0026amp;s/ sed \u0026rsquo;s/old/\u0026amp;s/\u0026rsquo; reg.txt 删除reg.txt中每行的第一个old都替换成olds \u0026amp;代表前面查找的字符串 4,7s/^/#/ sed \u0026lsquo;4,7s/^/#/\u0026rsquo; reg.txt 删除reg.txt中4-7行开头加上# 也就是批量添加注释 4,7s/^#an/an/ sed \u0026lsquo;4,7s/^#an/an/\u0026rsquo; reg.txt 删除reg.txt中4-7行以#an开头的行去掉# p 输出 命令 完整示例 说明 p sed -n \u0026lsquo;p\u0026rsquo; reg.txt 输出reg.txt的所有行 2p sed -n \u0026lsquo;2p\u0026rsquo; reg.txt 输出reg.txt的第二行 2,5p sed -n \u0026lsquo;2,5p\u0026rsquo; reg.txt 输出reg.txt的第二行到第五行 2,+5p sed -n \u0026lsquo;2,+5p\u0026rsquo; reg.txt 输出reg.txt的第二行和第二行以后的5行 1~2p sed -n \u0026lsquo;1~2p\u0026rsquo; reg.txt 输出第一行，每隔两行输出一行，也就是输出奇数行 2~2p sed -n \u0026lsquo;2~2p\u0026rsquo; reg.txt 输出第二行，每隔两行输出一行，也就是输出偶数数行 /正则/p sed -n \u0026lsquo;/[0-9]]/p\u0026rsquo; reg.txt 输出reg.txt以数字开头的行 $= sed -n \u0026lsquo;$=\u0026rsquo; reg.txt 输出reg.txt的行数 d 删除 命令 完整示例 说明 2d sed \u0026lsquo;2d\u0026rsquo; reg.txt 删除reg.txt的第二行 2,5d sed \u0026lsquo;2,5d\u0026rsquo; reg.txt 删除reg.txt的第二行到第五行 2,+5d sed \u0026lsquo;2,+5d\u0026rsquo; reg.txt 删除reg.txt的第二行和第二行以后的5行 $d sed \u0026lsquo;$d\u0026rsquo; reg.txt 删除reg.txt的最后一行 /正则/ sed \u0026lsquo;/[0-9]/d\u0026rsquo; reg.txt 删除以数字开始的行 ！ sed \u0026lsquo;/[0-9]/!d\u0026rsquo; reg.txt 删除不是以数字开头的行 ^$ sed \u0026lsquo;^$\u0026rsquo; reg.txt 删除reg.txt的空行 i/a/c插入 命令 完整示例 说明 行号 i sed \u0026lsquo;2i123\u0026rsquo; reg.txt 在reg.txt的第二行前面插入123 正则 i sed \u0026lsquo;/[0-9]/i123\u0026rsquo; reg.txt 在reg.txt中所有数字开头的行前面插入123 a 在行后插入 命令格式和i相同 c 替换该行 命令格式和i相同 sed高级应用 命令 完整示例 说明 行号r sed -i \u0026lsquo;2r 2.txt\u0026rsquo; 1.txt 在1.txt中第二行后面插入2.txt的内容 行号，行号r sed -i \u0026lsquo;2,5r 2.txt\u0026rsquo; 1.txt 在1.txt中第二行到第五行每行后面插入一遍2.txt的内容 /正则/r sed -i \u0026lsquo;/[ [0-9]/r 2.txt\u0026rsquo; 1.txt 在1.txt中以数字开头的行后每行面插入一遍2.txt的内容 w sed \u0026lsquo;2w 3.txt\u0026rsquo; 1.txt 在1.txt中第二行导出为3.txt的内容 w sed \u0026lsquo;2,5w 3.txt\u0026rsquo; 1.txt 在1.txt中第二行到第五行导出3.txt的内容 全文完。\n","description":"grep、sed、awk并称为linux文本处理器三剑客，本文主要总结sed的基础使用方法。","id":24,"section":"posts","tags":["Linux","Shell"],"title":"Linux Shell之sed","uri":"https://laomeinote.com/posts/how-to-use-sed-in-linux/"},{"content":"grep：global regular expressions print，全局正则表达式打印。主要用于在一个或者多个文件中，查找特定的单个字符、字符串、单词或句子，并打印出来。这个特定的字符(字符串)就是通过正则表达式(regular expressions print)来描述。\n正则表达式 基本通配符 通配符 功能 示例 ^ 以什么开头 的行：其实^（脱字号）符号与一行开头的空字符串匹配。 \u0026lsquo;^love\u0026rsquo;：匹配以字符love开头的行，即字符串love仅在出现在一行的开头时才匹配。 $ 以什么结尾的行：其实$（美元）符号与一行结尾的空字符串匹配。 \u0026rsquo;love$\u0026rsquo;：匹配以字符love结尾的行，即字符串love仅在出现在一行的末尾时才匹配。 . 匹配一个字符，一个任意字符的行 * 匹配0个或者多个字符的行 \u0026lsquo;c*\u0026rsquo;: 将匹配 0 个（即空白）或多个字符 c（c 为任一字符）的行；.*表示匹配任意字符串； [xyz] 匹配集合中的任意一个字符x,y,z的行 \u0026lsquo;[Ll]ove\u0026rsquo;: 匹配字符Love或者love的行 [^xyz] 匹配除方括号中字符外的所有字符的行 [^had]: 匹配除了had字符以外的所有字符、字符串、句子的行 [x-y] 匹配集合范围内的字符的行 \u0026lsquo;[A-Z]ove\u0026rsquo;: 匹配Aove ~ Zove的行 [^x-y] 匹配不在集合范围内的字符的行 [^A-Z]: 匹配不在A-Z范围内的任何字符的行，所有的小写字符 \\ 转义 需要注意的是，在基本正则表达式中，如通配符 *、+、{、|、( 和)等，已经失去了它们原本的含义，而若要恢复它们原本的含义，则要在之前添加反斜杠 \\，如\\*、\\+、\\{、\\|、\\( 和 \\)。\n上表中提到，^（脱字号）符号与一行开头的空字符串匹配，$（美元）符号与一行结尾的空字符串匹配。所以^$可用来匹配空行，即开头是空，中间没有字符，结尾是空。所以如果要将文本去掉空格行的话，可使用cat file |grep -v '^$'，其中参数-v表示翻转verse。\n量词 量词可指定要出现匹配项必须出现的项数。 下表显示了GNU grep支持的量词：\n量词 描述 * 将前一项匹配零次或多次。 ? 将前一项匹配零或一次。 + 匹配先前项一次或多次。 {n} 精确匹配先前项n {n,} 至少匹配n次。 {,m} 最多匹配前一项m次。 {n,m} 匹配前一项从n至m 次。 举几个栗子：\n正则表达式 12*4 与字符串 1234 不匹配，与 1224，12224，14 匹配：理解为字符2出现过0次或者多次； 正则表达式 (12)*4 表示与12匹配0次或者多次，再加上4，如124,12124,4； 正则表达式'o\\{2\\}'表示匹配两个o，即oo，使用\\对大括号进行转义。 正则表达式'o\\{2,4\\}'表示匹配两个o，到四个o，即oo,ooo,oooo，使用\\对大括号进行转义。 交替 交替是简单的or运算。 交替运算符|（竖线）使您可以指定不同的可能匹配项，这些匹配项可以是文字字符串或表达式集。 在所有正则表达式运算符中，此运算符的优先级最低。\n在下面的示例中，我们正在搜索[Nginx log]错误文件中所有出现的单词fatal，error和critical：\n1 grep \u0026#39;fatal\\|error\\|critical\u0026#39; /var/log/nginx/error.log grep基础用法 grep 命令的基本格式如下：\n1 [root@localhost ~]# grep [选项] 模式 文件名 这里的模式，要么是字符（串），要么是正则表达式。 常用的选项以及各自的含义如下表所示。 选项 含义 -c 仅列出文件中包含模式的行数。 -i 忽略模式中的字母大小写。 -l 列出带有匹配行的文件名。 -n 在每一行的最前面列出行号。 -v 列出没有匹配模式的行。 -w 把表达式当做一个完整的单字符来搜寻，忽略那些部分匹配的行。 注意，如果是搜索多个文件，grep 命令的搜索结果只显示文件中发现匹配模式的文件名；而如果搜索单个文件，grep 命令的结果将显示每一个包含匹配模式的行。\n比如有一份文件reg.txt：\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@localhost ~]# cat reg.txt I had a lovely time on our little picnic. Lovers were all around us. It is springtime. Oh love, how much I adore you. Do you know the extent of my love? Oh, by the way, I think I lost my gloves somewhere out in that field of clover. Did you see them? I can only hope love. is forever. I live for you. It\u0026#39;s hard to get back in the groove. ABDS JHB had what 【例 1】搜索此reg.txt文件，找出包含了字符串love，则执行命令如下：\n1 2 3 4 5 6 7 # cat reg.txt |grep -n love cat reg.txt |grep -n \u0026#39;love\u0026#39; 1:I had a lovely time on our little picnic. 3:love, how much I adore you. Do you know 4:the extent of my love? Oh, by the way, I think 5:I lost my gloves somewhere out in that field of 6:clover. Did you see them? I can only hope love. 使用参数-n将文件的行打印出来，可见在文件的1，3,4,5,6行分别都有字符love，但是第一行中的lovely也被筛选出来了，如果想要精确匹配这个字符love，则应该带上参数-w.\n1 2 3 4 # cat reg.txt |grep -n -w \u0026#39;love\u0026#39; 3:love, how much I adore you. Do you know 4:the extent of my love? Oh, by the way, I think 6:clover. Did you see them? I can only hope love. 在3,4,6行中匹配上了只有love字符，第一行的lovely和第5行的gloves被过滤掉了。\n如果只是想知道有多少个love字符串，则带上参数-c：\n1 2 # cat reg.txt |grep love -c 5 如果想知道有多少个精准的love字符串，则带上-w和-c:\n1 2 # cat reg.txt |grep -wc \u0026#39;love\u0026#39; 3 【例 2】搜索reg.txt文件，使用正则表达式找出以 love开头的数据行，执行命令如下：\n1 2 # cat reg.txt |grep -n \u0026#39;^love\u0026#39; 3:love, how much I adore you. Do you know 可见只匹配上了第三行。\ngrep 命令的功能非常强大，通过利用它的不同选项以及变化万千的正则表达式，可以获取任何我们所需要的信息。本节所介绍的 grep 命令，只介绍了它的一部分基础知识。也可到https://linuxtools-rst.readthedocs.io/zh_CN/latest/base/03_text_processing.html#id4查看对应文本处理章节。\n全文完。\n","description":"grep、sed、awk并称为linux文本处理器三剑客，本文主要总结grep基础使用方法。","id":25,"section":"posts","tags":["Linux","Shell"],"title":"Linux Shell之grep与正则","uri":"https://laomeinote.com/posts/how-to-use-grep-and-regular-expressions/"},{"content":"公司内网安全区无法通互联网，原本可以通过yum命令直接下载安装的软件包没法用了，那有没有一种快捷的方法把这些软件包与对应的依赖包一起打包下来导入到内网呢？本文将简述方案。\n1. 软件与依赖包打包 在可以联网的服务器上按需安装好软件包，然后将其打包。\n举个栗子，我需要安装docker和kubelet这些软件包，在可联网的服务器上安装：\n1 2 3 $sudo yum install -y yum-utils device-mapper-persistent-data lvm2 #安装依赖包 $sudo yum install -y docker-ce-20.10.8 docker-ce-cli-20.10.8 containerd.io #安装docker包 $sudo yum install -y kubelet-1.22.4 kubectl-1.22.4 kubeadm-1.22.4 #安装kubelet，kubectl，kubeadm包与对应的依赖包 通过repotrack将它们打包，如果没有这个安装包，可以先安装。\n1 2 $sudo yum install -y yum-utils #安装 $sudo yum install -y repotrack 然后通过repotrack [packages]进行打包，。\n1 repotrack yum-utils device-mapper-persistent-data lvm2 docker-ce-20.10.8 docker-ce-cli-20.10.8 containerd.io kubelet-1.22.4 kubectl-1.22.4 kubeadm-1.22.4 注意：kubeadm依赖kubectl,kubectl依赖kubelet，所以安装包的先后顺序不要弄错，否则版本依赖包会有问题。\n这时就会生成对应的软件依赖包了，可将这些软件包压缩下载。\n1 2 3 mkdir -p centos7.6_packages mv *.rpm centos7.6_packages tar cjvf centos7.6_packages.tar.bz2 centos7.6_packages 最后将centos7.6_packages.tar.bz2下载下来。\n如果发现版本依赖不匹配，也可以用yumdownloader下载指定包\nyum -y install yum-utils\nyumdownloader \u0026ndash;resolve \u0026ndash;destdir=/tmp docker-ce-cli-18.9.0\n参数说明：\n\u0026ndash;destdir：指定 rpm 包下载目录（不指定时，默认为当前目录） \u0026ndash;resolve：下载依赖的 rpm 包。 2.在内网服务器建立本地源 首先将上面的centos7.6_packages.tar.bz2下载到内网服务器，将所有rpm包解压至/mnt/packages目录下。\n1 2 3 tar xjvf centos7.6_packages.tar.bz2 /mnt/ mkdir -p /mnt/packages mv /mnt/*.rpm /mnt/packages 通过createrepo在内网服务器建立本地源，如果没有createrepo的话，先安装一个。\n1 yum install -y createrepo #假设内网可以安装这个createrepo软件 如果内网里，这个软件都安装不了的话，那就下载好rpm文件手动安装了。\n1 rpm -ivh createrepo.rpm 然后创建本地源目录，即有一堆rpm软件包的那个目录。\n1 createrepo /mnt/packages 然后添加配置文件。\n1 2 3 4 5 6 7 [root@master] # cat \u0026gt;\u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt; EOF [local] name=local.repo baseurl=file:///mnt/Packages enabled=1 gpgcheck=0 EOF [] 中括号中的是repository id，唯一，用来标识不同仓库 name 仓库名称，自定义 baseurl 仓库地址，设置为本地/mnt/Packages目录 enable 是否启用该仓库，默认为1表示启用 gpgcheck 是否验证从该仓库获得程序包的合法性，1为验证，0为不验证 添加完成后刷新源缓存。\n1 2 [root@master] #yum clean all [root@master] #yum makecache fast 然后就可以开心的安装已经在本地的源了。\n全文完。\n","description":"公司内网安全区无法通互联网，如何把外部软件与对应的依赖包一起打包下来导入到内网呢？本文将简述方案。","id":26,"section":"posts","tags":["Linux","Centos"],"title":"给Centos 7制作依赖包与本地仓库","uri":"https://laomeinote.com/posts/build-local-repository-in-centos/"},{"content":"在Shell脚本中经常看到数字0,1,2，到底是什么意思，又有什么用处呢？其实Shell脚本的数字分为两种，一种是文本描述，一种是命令参数，两者用法和场景不同。这篇文章简单归纳总结。\n1.文本描述0,1,2 在Linux和unix系统中，文件描述符0,1,2是系统预留的，它们的意义分别有如下对应关系：\n0 —— stdin（标准输入） 1 —— stdout （标准输出） 2 —— stderr （标准错误） 其中，shell脚本中经常用到的就是描述符1，和描述符2。举个栗子说明：\n1 2 3 4 5 6 7 [root@master test]# pwd #在/root/test目录下测试 /root/test [root@master test]# touch test.txt #新建一个test.txt文件，这里的test.txt文本就是stdin（标准输入） [root@master test]# ls #执行ls命令会输出一个结果，输出的text.txt就是stdout （标准输出） test.txt [root@master test]# ls nothing.txt #执行ls命令查看一个不存在的文件，下面输出的就是stderr （标准错误） ls: cannot access nothing.txt: No such file or directory 上面的例子中，\ntouch test.txt中的 test.txt就是0 — stdin（标准输入）， 执行ls命令会输出一个结果，如果不出错其输出结果就是1 — stdout （标准输出） 执行ls nothing.txt查询一个不存在的文件，肯定会抛出错误，下面输出的结果就是2 — stderr （标准错误） 可以将正确结果重定向到文件中如：\n1 2 3 4 5 6 7 [root@master test]# ls 1\u0026gt; stdout.txt [root@master test]# ls stdout.txt test.txt [root@master test]# cat stdout.txt stdout.txt test.txt [root@master test]# 将错误结果重定向到文件中如：\n1 2 3 4 [root@master test]# ls nothing.txt 2\u0026gt; stderr.txt [root@master test]# cat stderr.txt ls: cannot access nothing.txt: No such file or directory [root@master test]# 1.1 文本描述0,1,2的几种常见用法 在shell中经常是通过管道，重定向等使用这些文件描述符。\n1.2 重定向到/dev/null /dev/null是一个特殊的设备文件，这个文件接收到的任何数据都会被丢弃。因此，null这个设备通常也被成为位桶（bit bucket）或黑洞。\n即是将错误输出丢弃2\u0026gt; /dev/null的用法：\n1 $cat logcat.log 2\u0026gt; /dev/null |grep compiled 即打印logcat.log，将错误信息重定向到黑洞，标准输出作为grep的输入，过滤出包含compiled字段的信息。\n1.3 将标准输出与错误输出分开重定向 1 cmd 2\u0026gt;stderr.txt 1\u0026gt;stdout.txt 将cmd的结果中，正确的重定向到stdout.txt，错误的重定向到stderr.txt中。\n1.4 将标准输出和错误输出重定向到一个文件中 1 cmd\u0026gt; output.txt 2\u0026gt;\u0026amp;1 即是将cmd结果输出到output.txt，其中2\u0026gt;\u0026amp;1表示，将错误输出2-stderr叠加到1-stdout.txt中。\n2. 参数$0、$1、$2 除了文本描述以外，0,1,2与$符号在一起可作为shell参数。\n2.1 基本含义 参数中的0,1,2就表示shell中的参数位置，举个栗子：\n1 $ ./cmd.sh a b c 执行脚本cmd.sh，跟着的参数为a,b,c，其中:\n$0\n对应 ./cmd.sh 这个值。如果执行的是 ./myworkplace/cmd.sh， 则$0为 ./myworkplace/cmd.sh 这个值。\n$1\n会获取到 a，即 $1 对应传给脚本的第一个参数。\n$2\n会获取到 b，即 $2 对应传给脚本的第二个参数。\n$3\n会获取到 c，即 $3 对应传给脚本的第三个参数。$4、$5 等参数的含义依此类推。\n$#\n会获取到命令的参数个数，这里有a,b,c三个参数，所以其值为3，统计的参数不包括 $0。\n$@\n会获取到 \u0026ldquo;a\u0026rdquo; \u0026ldquo;b\u0026rdquo; \u0026ldquo;c\u0026rdquo;，也就是所有参数的列表，不包括 $0。\n$*\n也会获取到 \u0026ldquo;a\u0026rdquo; \u0026ldquo;b\u0026rdquo; \u0026ldquo;c\u0026rdquo;， 其值和 $@ 相同。但 \u0026quot;$*\u0026quot; 和 \u0026quot;$@\u0026quot; 有所不同。\n\u0026quot;$*\u0026quot; 把所有参数合并成一个字符串，而 \u0026quot;$@\u0026quot; 会得到一个字符串参数数组。\n$?\n可以获取到执行 ./cmd.sh a b c 命令后的返回值。\n在执行一个前台命令后，可以立即用 $? 获取到该命令的返回值。\n该命令可以是系统自身的命令，可以是 shell 脚本，也可以是自定义的 bash 函数。\n需要注意的是:\n1）当调用的是系统命令，返回的是系统命令返回值。\n2）当执行 shell 脚本时，$? 对应该脚本调用 exit 命令返回的值。如果没有主动调用 exit 命令，默认返回为 0。\n3）当执行自定义的 bash 函数时，$? 对应该函数调用 return 命令返回的值。如果没有主动调用 return 命令，默认返回为 0。\n另外，如果参数过多，超过9个，引用第10个参数的时候，不能使用$10，而应该使用${10}。\n2.2 基础使用 $与数字一起比较容易理解，剩下的几个稍微难理解，举个栗子测试$#、$@和$*。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash echo -------------- echo \u0026#34;#### demo:\u0026#34; $# echo -------------- for arg in \u0026#34;$@\u0026#34;; do echo \u0026#34;@@@@ demo:\u0026#34; $arg done echo -------------- for arg in \u0026#34;$*\u0026#34;; do echo \u0026#34;**** demo:\u0026#34; $arg done [root@node1 ~]# ./cmd.sh a b c -------------- #### demo: 3 -------------- @@@@ demo: a @@@@ demo: b @@@@ demo: c -------------- **** demo: a b c 测试时，执行./cmd.sh a b c，$#最终显示的是3表示脚本输入的参数个数，$@会逐个获取参数值，$*会将参数组合成字符串\u0026quot;a b c\u0026quot;.\n文章首发公众号-，欢迎关注，不定期更新。\n全文完。\n","description":"Shell脚本的数字分为两种，一种是文本描述，一种是命令参数，两者用法和场景不同。","id":27,"section":"posts","tags":["Linux","Shell"],"title":"Linux Shell中的数字0,1,2是什么意思","uri":"https://laomeinote.com/posts/the-numbers-in-linux-shell/"},{"content":"Linux 的 screen工具是终端复用器，可在一个终端窗口中运行多个终端会话，并可进行跨窗口和会话切换，而不必担心丢失连接和状态。我们可将其用于离线作业场景。\n想象一下这种场景：你需要在服务器上安装/编译一个大型软件，或者做系统备份、ftp 传输等，可能要花费1-2个小时，这期间不能关掉terminal窗口或者断开连接，否则这个任务就会被中断，一切半途而废了。但是又没法一直盯着屏幕，或者这时你需要关机出去处理别的事务，那这个任务岂不是要中断了？screen就可以完美解决这个问题，通过该软件同时连接多个命令行session会话，并在其间自由切换，只要不主动停止，会话里的任务不会中断。\n安装screen Centos下安装\n1 sudo yum install -y screen Ubuntu下安装\n1 apt-get install screen Screen用法 1 screen [-AmRvx -ls -wipe][-d \u0026lt;作业名称\u0026gt;][-h \u0026lt;行数\u0026gt;][-r \u0026lt;作业名称\u0026gt;][-s \u0026lt;shell\u0026gt;][-S \u0026lt;作业名称\u0026gt;] 参数说明：\n-A 将所有的视窗都调整为目前终端机的大小。 -d \u0026lt;作业名称\u0026gt; 将指定的screen作业离线。 -h \u0026lt;行数\u0026gt; 指定视窗的缓冲区行数。 -m 即使目前已在作业中的screen作业，仍强制建立新的screen作业。 -r \u0026lt;作业名称\u0026gt; 恢复离线的screen作业。 -R 先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。 -s 指定建立新视窗时，所要执行的shell。 -S \u0026lt;作业名称\u0026gt; 指定screen作业的名称。 -v 显示版本信息。 -x 恢复之前离线的screen作业。 -ls或\u0026ndash;list 显示目前所有的screen作业。 -wipe 检查目前所有的screen作业，并删除已经无法使用的screen作业。 Demo实例 创建 screen 终端\n1 2 # screen lnmp //创建 screen 终端，名称为lnmp # yum install -y xxx \u0026amp;\u0026amp; ./install.sh xxx.sh #在lnmp窗口执行任务 关闭终端，任务已经在远程的会话里执行，一天后想查看执行结果，恢复会话即可。\n1 2 # screen -ls //查看当前有哪些会话 # screen -r lnmp //恢复之前的lnmp会话 全文完。\n","description":"Linux 的 screen工具是终端复用器，可在一个终端窗口中运行多个终端会话，并可进行跨窗口和会话切换，而不必担心丢失连接和状态。","id":28,"section":"posts","tags":["Linux","Shell"],"title":"Linux 的 screen工具","uri":"https://laomeinote.com/posts/how-to-use-screen-in-linux/"},{"content":"Sample images from Pixabay\n","description":"cartoon gallery","id":29,"section":"gallery","tags":null,"title":"Cartoon","uri":"https://laomeinote.com/gallery/cartoon/"},{"content":"Sample images from Pixabay\n","description":"photo gallery","id":30,"section":"gallery","tags":null,"title":"Photo","uri":"https://laomeinote.com/gallery/photo/"},{"content":"这是Marco的个人站点，主要分享技术笔记、阅读心得、工具使用、资源共享等内容。\n[未完待续\u0026hellip;]\n","description":"这是Marco的个人站点，主要分享技术笔记、阅读心得、工具使用、资源共享等内容.","id":31,"section":"","tags":null,"title":"本站介绍","uri":"https://laomeinote.com/about/"}]